
Random seeds have been fixed as 42

D05-
['./data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy']

2021-0716-0814

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/8742050 (0.0%)]	Loss: 0.362629
Train Epoch: 1 [52224/8742050 (0.6%)]	Loss: 0.160396
Train Epoch: 1 [103424/8742050 (1.2%)]	Loss: 0.130689
Train Epoch: 1 [154624/8742050 (1.8%)]	Loss: 0.139665
Train Epoch: 1 [205824/8742050 (2.4%)]	Loss: 0.129057
Train Epoch: 1 [257024/8742050 (2.9%)]	Loss: 0.133686
Train Epoch: 1 [308224/8742050 (3.5%)]	Loss: 0.124570
Train Epoch: 1 [359424/8742050 (4.1%)]	Loss: 0.121193
Train Epoch: 1 [410624/8742050 (4.7%)]	Loss: 0.132704
Train Epoch: 1 [461824/8742050 (5.3%)]	Loss: 0.137600
Train Epoch: 1 [513024/8742050 (5.9%)]	Loss: 0.117818
Train Epoch: 1 [564224/8742050 (6.5%)]	Loss: 0.128450
Train Epoch: 1 [615424/8742050 (7.0%)]	Loss: 0.117154
Train Epoch: 1 [666624/8742050 (7.6%)]	Loss: 0.119478
Train Epoch: 1 [717824/8742050 (8.2%)]	Loss: 0.134609
Train Epoch: 1 [769024/8742050 (8.8%)]	Loss: 0.130889
Train Epoch: 1 [820224/8742050 (9.4%)]	Loss: 0.128373
Train Epoch: 1 [871424/8742050 (10.0%)]	Loss: 0.116827
Train Epoch: 1 [922624/8742050 (10.6%)]	Loss: 0.116052
Train Epoch: 1 [973824/8742050 (11.1%)]	Loss: 0.135475
Train Epoch: 1 [1025024/8742050 (11.7%)]	Loss: 0.111287
Train Epoch: 1 [1076224/8742050 (12.3%)]	Loss: 0.126296
Train Epoch: 1 [1127424/8742050 (12.9%)]	Loss: 0.122814
Train Epoch: 1 [1178624/8742050 (13.5%)]	Loss: 0.132042
Train Epoch: 1 [1229824/8742050 (14.1%)]	Loss: 0.112814
Train Epoch: 1 [1281024/8742050 (14.7%)]	Loss: 0.119823
Train Epoch: 1 [1332224/8742050 (15.2%)]	Loss: 0.133260
Train Epoch: 1 [1383424/8742050 (15.8%)]	Loss: 0.128044
Train Epoch: 1 [1434624/8742050 (16.4%)]	Loss: 0.113271
Train Epoch: 1 [1485824/8742050 (17.0%)]	Loss: 0.115790
Train Epoch: 1 [1537024/8742050 (17.6%)]	Loss: 0.118816
Train Epoch: 1 [1588224/8742050 (18.2%)]	Loss: 0.114057
Train Epoch: 1 [1639424/8742050 (18.8%)]	Loss: 0.125479
Train Epoch: 1 [1690624/8742050 (19.3%)]	Loss: 0.116844
Train Epoch: 1 [1741824/8742050 (19.9%)]	Loss: 0.117983
Train Epoch: 1 [1793024/8742050 (20.5%)]	Loss: 0.110445
Train Epoch: 1 [1844224/8742050 (21.1%)]	Loss: 0.116694
Train Epoch: 1 [1895424/8742050 (21.7%)]	Loss: 0.118328
Train Epoch: 1 [1946624/8742050 (22.3%)]	Loss: 0.115723
Train Epoch: 1 [1997824/8742050 (22.9%)]	Loss: 0.101060
Train Epoch: 1 [2049024/8742050 (23.4%)]	Loss: 0.117391
Train Epoch: 1 [2100224/8742050 (24.0%)]	Loss: 0.109629
Train Epoch: 1 [2151424/8742050 (24.6%)]	Loss: 0.128713
Train Epoch: 1 [2202624/8742050 (25.2%)]	Loss: 0.113812
Train Epoch: 1 [2253824/8742050 (25.8%)]	Loss: 0.121707
Train Epoch: 1 [2305024/8742050 (26.4%)]	Loss: 0.116497
Train Epoch: 1 [2356224/8742050 (27.0%)]	Loss: 0.114926
Train Epoch: 1 [2407424/8742050 (27.5%)]	Loss: 0.117030
Train Epoch: 1 [2458624/8742050 (28.1%)]	Loss: 0.138962
Train Epoch: 1 [2509824/8742050 (28.7%)]	Loss: 0.123246
Train Epoch: 1 [2561024/8742050 (29.3%)]	Loss: 0.118180
Train Epoch: 1 [2612224/8742050 (29.9%)]	Loss: 0.132565
Train Epoch: 1 [2663424/8742050 (30.5%)]	Loss: 0.114542
Train Epoch: 1 [2714624/8742050 (31.1%)]	Loss: 0.138364
Train Epoch: 1 [2765824/8742050 (31.6%)]	Loss: 0.127959
Train Epoch: 1 [2817024/8742050 (32.2%)]	Loss: 0.125056
Train Epoch: 1 [2868224/8742050 (32.8%)]	Loss: 0.114406
Train Epoch: 1 [2919424/8742050 (33.4%)]	Loss: 0.124049
Train Epoch: 1 [2970624/8742050 (34.0%)]	Loss: 0.108994
Train Epoch: 1 [3021824/8742050 (34.6%)]	Loss: 0.125164
Train Epoch: 1 [3073024/8742050 (35.2%)]	Loss: 0.112410
Train Epoch: 1 [3124224/8742050 (35.7%)]	Loss: 0.113421
Train Epoch: 1 [3175424/8742050 (36.3%)]	Loss: 0.134348
Train Epoch: 1 [3226624/8742050 (36.9%)]	Loss: 0.116926
Train Epoch: 1 [3277824/8742050 (37.5%)]	Loss: 0.110189
Train Epoch: 1 [3329024/8742050 (38.1%)]	Loss: 0.118922
Train Epoch: 1 [3380224/8742050 (38.7%)]	Loss: 0.114686
Train Epoch: 1 [3431424/8742050 (39.3%)]	Loss: 0.124865
Train Epoch: 1 [3482624/8742050 (39.8%)]	Loss: 0.112035
Train Epoch: 1 [3533824/8742050 (40.4%)]	Loss: 0.115265
Train Epoch: 1 [3585024/8742050 (41.0%)]	Loss: 0.118818
Train Epoch: 1 [3636224/8742050 (41.6%)]	Loss: 0.104945
Train Epoch: 1 [3687424/8742050 (42.2%)]	Loss: 0.112178
Train Epoch: 1 [3738624/8742050 (42.8%)]	Loss: 0.134362
Train Epoch: 1 [3789824/8742050 (43.4%)]	Loss: 0.112305
Train Epoch: 1 [3841024/8742050 (43.9%)]	Loss: 0.120938
Train Epoch: 1 [3892224/8742050 (44.5%)]	Loss: 0.118060
Train Epoch: 1 [3943424/8742050 (45.1%)]	Loss: 0.101137
Train Epoch: 1 [3994624/8742050 (45.7%)]	Loss: 0.119202
Train Epoch: 1 [4045824/8742050 (46.3%)]	Loss: 0.123971
Train Epoch: 1 [4097024/8742050 (46.9%)]	Loss: 0.133172
Train Epoch: 1 [4148224/8742050 (47.5%)]	Loss: 0.112620
Train Epoch: 1 [4199424/8742050 (48.0%)]	Loss: 0.106577
Train Epoch: 1 [4250624/8742050 (48.6%)]	Loss: 0.109177
Train Epoch: 1 [4301824/8742050 (49.2%)]	Loss: 0.109013
Train Epoch: 1 [4353024/8742050 (49.8%)]	Loss: 0.113249
Train Epoch: 1 [4404224/8742050 (50.4%)]	Loss: 0.103818
Train Epoch: 1 [4455424/8742050 (51.0%)]	Loss: 0.114664
Train Epoch: 1 [4506624/8742050 (51.6%)]	Loss: 0.118266
Train Epoch: 1 [4557824/8742050 (52.1%)]	Loss: 0.097167
Train Epoch: 1 [4609024/8742050 (52.7%)]	Loss: 0.118752
Train Epoch: 1 [4660224/8742050 (53.3%)]	Loss: 0.111525
Train Epoch: 1 [4711424/8742050 (53.9%)]	Loss: 0.136883
Train Epoch: 1 [4762624/8742050 (54.5%)]	Loss: 0.107331
Train Epoch: 1 [4813824/8742050 (55.1%)]	Loss: 0.105300
Train Epoch: 1 [4865024/8742050 (55.7%)]	Loss: 0.097447
Train Epoch: 1 [4916224/8742050 (56.2%)]	Loss: 0.109379
Train Epoch: 1 [4967424/8742050 (56.8%)]	Loss: 0.124935
Train Epoch: 1 [5018624/8742050 (57.4%)]	Loss: 0.100578
Train Epoch: 1 [5069824/8742050 (58.0%)]	Loss: 0.111563
Train Epoch: 1 [5121024/8742050 (58.6%)]	Loss: 0.125706
Train Epoch: 1 [5172224/8742050 (59.2%)]	Loss: 0.118472
Train Epoch: 1 [5223424/8742050 (59.8%)]	Loss: 0.096695
Train Epoch: 1 [5274624/8742050 (60.3%)]	Loss: 0.116107
Train Epoch: 1 [5325824/8742050 (60.9%)]	Loss: 0.136304
Train Epoch: 1 [5377024/8742050 (61.5%)]	Loss: 0.121774
Train Epoch: 1 [5428224/8742050 (62.1%)]	Loss: 0.129012
Train Epoch: 1 [5479424/8742050 (62.7%)]	Loss: 0.135108
Train Epoch: 1 [5530624/8742050 (63.3%)]	Loss: 0.128629
Train Epoch: 1 [5581824/8742050 (63.9%)]	Loss: 0.111678
Train Epoch: 1 [5633024/8742050 (64.4%)]	Loss: 0.119775
Train Epoch: 1 [5684224/8742050 (65.0%)]	Loss: 0.106192
Train Epoch: 1 [5735424/8742050 (65.6%)]	Loss: 0.117015
Train Epoch: 1 [5786624/8742050 (66.2%)]	Loss: 0.125193
Train Epoch: 1 [5837824/8742050 (66.8%)]	Loss: 0.101403
Train Epoch: 1 [5889024/8742050 (67.4%)]	Loss: 0.114220
Train Epoch: 1 [5940224/8742050 (68.0%)]	Loss: 0.112532
Train Epoch: 1 [5991424/8742050 (68.5%)]	Loss: 0.122661
Train Epoch: 1 [6042624/8742050 (69.1%)]	Loss: 0.121229
Train Epoch: 1 [6093824/8742050 (69.7%)]	Loss: 0.097912
Train Epoch: 1 [6145024/8742050 (70.3%)]	Loss: 0.119173
Train Epoch: 1 [6196224/8742050 (70.9%)]	Loss: 0.113907
Train Epoch: 1 [6247424/8742050 (71.5%)]	Loss: 0.110632
Train Epoch: 1 [6298624/8742050 (72.0%)]	Loss: 0.125181
Train Epoch: 1 [6349824/8742050 (72.6%)]	Loss: 0.109685
Train Epoch: 1 [6401024/8742050 (73.2%)]	Loss: 0.121012
Train Epoch: 1 [6452224/8742050 (73.8%)]	Loss: 0.108159
Train Epoch: 1 [6503424/8742050 (74.4%)]	Loss: 0.111453
Train Epoch: 1 [6554624/8742050 (75.0%)]	Loss: 0.118304
Train Epoch: 1 [6605824/8742050 (75.6%)]	Loss: 0.108238
Train Epoch: 1 [6657024/8742050 (76.1%)]	Loss: 0.097988
Train Epoch: 1 [6708224/8742050 (76.7%)]	Loss: 0.113439
Train Epoch: 1 [6759424/8742050 (77.3%)]	Loss: 0.116874
Train Epoch: 1 [6810624/8742050 (77.9%)]	Loss: 0.107950
Train Epoch: 1 [6861824/8742050 (78.5%)]	Loss: 0.117192
Train Epoch: 1 [6913024/8742050 (79.1%)]	Loss: 0.116467
Train Epoch: 1 [6964224/8742050 (79.7%)]	Loss: 0.113248
Train Epoch: 1 [7015424/8742050 (80.2%)]	Loss: 0.106241
Train Epoch: 1 [7066624/8742050 (80.8%)]	Loss: 0.116683
Train Epoch: 1 [7117824/8742050 (81.4%)]	Loss: 0.113054
Train Epoch: 1 [7169024/8742050 (82.0%)]	Loss: 0.119587
Train Epoch: 1 [7220224/8742050 (82.6%)]	Loss: 0.113277
Train Epoch: 1 [7271424/8742050 (83.2%)]	Loss: 0.108629
Train Epoch: 1 [7322624/8742050 (83.8%)]	Loss: 0.123033
Train Epoch: 1 [7373824/8742050 (84.3%)]	Loss: 0.120025
Train Epoch: 1 [7425024/8742050 (84.9%)]	Loss: 0.103909
Train Epoch: 1 [7476224/8742050 (85.5%)]	Loss: 0.105827
Train Epoch: 1 [7527424/8742050 (86.1%)]	Loss: 0.115455
Train Epoch: 1 [7578624/8742050 (86.7%)]	Loss: 0.104217
Train Epoch: 1 [7629824/8742050 (87.3%)]	Loss: 0.113337
Train Epoch: 1 [7681024/8742050 (87.9%)]	Loss: 0.121394
Train Epoch: 1 [7732224/8742050 (88.4%)]	Loss: 0.118219
Train Epoch: 1 [7783424/8742050 (89.0%)]	Loss: 0.104168
Train Epoch: 1 [7834624/8742050 (89.6%)]	Loss: 0.108875
Train Epoch: 1 [7885824/8742050 (90.2%)]	Loss: 0.109024
Train Epoch: 1 [7937024/8742050 (90.8%)]	Loss: 0.102094
Train Epoch: 1 [7988224/8742050 (91.4%)]	Loss: 0.104221
Train Epoch: 1 [8039424/8742050 (92.0%)]	Loss: 0.123068
Train Epoch: 1 [8090624/8742050 (92.5%)]	Loss: 0.113984
Train Epoch: 1 [8141824/8742050 (93.1%)]	Loss: 0.123108
Train Epoch: 1 [8193024/8742050 (93.7%)]	Loss: 0.113609
Train Epoch: 1 [8244224/8742050 (94.3%)]	Loss: 0.109012
Train Epoch: 1 [8295424/8742050 (94.9%)]	Loss: 0.119028
Train Epoch: 1 [8346624/8742050 (95.5%)]	Loss: 0.120915
Train Epoch: 1 [8397824/8742050 (96.1%)]	Loss: 0.107011
Train Epoch: 1 [8449024/8742050 (96.6%)]	Loss: 0.107276
Train Epoch: 1 [8500224/8742050 (97.2%)]	Loss: 0.096263
Train Epoch: 1 [8551424/8742050 (97.8%)]	Loss: 0.113267
Train Epoch: 1 [8602624/8742050 (98.4%)]	Loss: 0.125717
Train Epoch: 1 [8653824/8742050 (99.0%)]	Loss: 0.111843
Train Epoch: 1 [8705024/8742050 (99.6%)]	Loss: 0.110330
Train Epoch: 2 [1024/8742050 (0.0%)]	Loss: 0.108565
Train Epoch: 2 [52224/8742050 (0.6%)]	Loss: 0.111128
Train Epoch: 2 [103424/8742050 (1.2%)]	Loss: 0.108142
Train Epoch: 2 [154624/8742050 (1.8%)]	Loss: 0.106837
Train Epoch: 2 [205824/8742050 (2.4%)]	Loss: 0.107250
Train Epoch: 2 [257024/8742050 (2.9%)]	Loss: 0.119427
Train Epoch: 2 [308224/8742050 (3.5%)]	Loss: 0.101676
Train Epoch: 2 [359424/8742050 (4.1%)]	Loss: 0.099902
Train Epoch: 2 [410624/8742050 (4.7%)]	Loss: 0.116640
Train Epoch: 2 [461824/8742050 (5.3%)]	Loss: 0.109599
Train Epoch: 2 [513024/8742050 (5.9%)]	Loss: 0.108316
Train Epoch: 2 [564224/8742050 (6.5%)]	Loss: 0.110927
Train Epoch: 2 [615424/8742050 (7.0%)]	Loss: 0.112446
Train Epoch: 2 [666624/8742050 (7.6%)]	Loss: 0.121802
Train Epoch: 2 [717824/8742050 (8.2%)]	Loss: 0.109247
Train Epoch: 2 [769024/8742050 (8.8%)]	Loss: 0.117698
Train Epoch: 2 [820224/8742050 (9.4%)]	Loss: 0.108158
Train Epoch: 2 [871424/8742050 (10.0%)]	Loss: 0.114429
Train Epoch: 2 [922624/8742050 (10.6%)]	Loss: 0.111076
Train Epoch: 2 [973824/8742050 (11.1%)]	Loss: 0.115755
Train Epoch: 2 [1025024/8742050 (11.7%)]	Loss: 0.119035
Train Epoch: 2 [1076224/8742050 (12.3%)]	Loss: 0.122864
Train Epoch: 2 [1127424/8742050 (12.9%)]	Loss: 0.098645
Train Epoch: 2 [1178624/8742050 (13.5%)]	Loss: 0.112391
Train Epoch: 2 [1229824/8742050 (14.1%)]	Loss: 0.106981
Train Epoch: 2 [1281024/8742050 (14.7%)]	Loss: 0.104770
Train Epoch: 2 [1332224/8742050 (15.2%)]	Loss: 0.118016
Train Epoch: 2 [1383424/8742050 (15.8%)]	Loss: 0.113197
Train Epoch: 2 [1434624/8742050 (16.4%)]	Loss: 0.108941
Train Epoch: 2 [1485824/8742050 (17.0%)]	Loss: 0.109764
Train Epoch: 2 [1537024/8742050 (17.6%)]	Loss: 0.108129
Train Epoch: 2 [1588224/8742050 (18.2%)]	Loss: 0.116246
Train Epoch: 2 [1639424/8742050 (18.8%)]	Loss: 0.110701
Train Epoch: 2 [1690624/8742050 (19.3%)]	Loss: 0.127930
Train Epoch: 2 [1741824/8742050 (19.9%)]	Loss: 0.115522
Train Epoch: 2 [1793024/8742050 (20.5%)]	Loss: 0.112135
Train Epoch: 2 [1844224/8742050 (21.1%)]	Loss: 0.110769
Train Epoch: 2 [1895424/8742050 (21.7%)]	Loss: 0.116117
Train Epoch: 2 [1946624/8742050 (22.3%)]	Loss: 0.110203
Train Epoch: 2 [1997824/8742050 (22.9%)]	Loss: 0.102073
Train Epoch: 2 [2049024/8742050 (23.4%)]	Loss: 0.123296
Train Epoch: 2 [2100224/8742050 (24.0%)]	Loss: 0.115111
Train Epoch: 2 [2151424/8742050 (24.6%)]	Loss: 0.121976
Train Epoch: 2 [2202624/8742050 (25.2%)]	Loss: 0.105874
Train Epoch: 2 [2253824/8742050 (25.8%)]	Loss: 0.113871
Train Epoch: 2 [2305024/8742050 (26.4%)]	Loss: 0.115578
Train Epoch: 2 [2356224/8742050 (27.0%)]	Loss: 0.115152
Train Epoch: 2 [2407424/8742050 (27.5%)]	Loss: 0.101985
Train Epoch: 2 [2458624/8742050 (28.1%)]	Loss: 0.107591
Train Epoch: 2 [2509824/8742050 (28.7%)]	Loss: 0.111680
Train Epoch: 2 [2561024/8742050 (29.3%)]	Loss: 0.105365
Train Epoch: 2 [2612224/8742050 (29.9%)]	Loss: 0.119243
Train Epoch: 2 [2663424/8742050 (30.5%)]	Loss: 0.107215
Train Epoch: 2 [2714624/8742050 (31.1%)]	Loss: 0.100623
Train Epoch: 2 [2765824/8742050 (31.6%)]	Loss: 0.114006
Train Epoch: 2 [2817024/8742050 (32.2%)]	Loss: 0.108364
Train Epoch: 2 [2868224/8742050 (32.8%)]	Loss: 0.111433
Train Epoch: 2 [2919424/8742050 (33.4%)]	Loss: 0.119062
Train Epoch: 2 [2970624/8742050 (34.0%)]	Loss: 0.111218
Train Epoch: 2 [3021824/8742050 (34.6%)]	Loss: 0.117366
Train Epoch: 2 [3073024/8742050 (35.2%)]	Loss: 0.096715
Train Epoch: 2 [3124224/8742050 (35.7%)]	Loss: 0.110752
Train Epoch: 2 [3175424/8742050 (36.3%)]	Loss: 0.107448
Train Epoch: 2 [3226624/8742050 (36.9%)]	Loss: 0.111585
Train Epoch: 2 [3277824/8742050 (37.5%)]	Loss: 0.107189
Train Epoch: 2 [3329024/8742050 (38.1%)]	Loss: 0.109457
Train Epoch: 2 [3380224/8742050 (38.7%)]	Loss: 0.118259
Train Epoch: 2 [3431424/8742050 (39.3%)]	Loss: 0.099252
Train Epoch: 2 [3482624/8742050 (39.8%)]	Loss: 0.103281
Train Epoch: 2 [3533824/8742050 (40.4%)]	Loss: 0.114138
Train Epoch: 2 [3585024/8742050 (41.0%)]	Loss: 0.118408
Train Epoch: 2 [3636224/8742050 (41.6%)]	Loss: 0.113823
Train Epoch: 2 [3687424/8742050 (42.2%)]	Loss: 0.108004
Train Epoch: 2 [3738624/8742050 (42.8%)]	Loss: 0.106386
Train Epoch: 2 [3789824/8742050 (43.4%)]	Loss: 0.104530
Train Epoch: 2 [3841024/8742050 (43.9%)]	Loss: 0.105193
Train Epoch: 2 [3892224/8742050 (44.5%)]	Loss: 0.099906
Train Epoch: 2 [3943424/8742050 (45.1%)]	Loss: 0.107714
Train Epoch: 2 [3994624/8742050 (45.7%)]	Loss: 0.107033
Train Epoch: 2 [4045824/8742050 (46.3%)]	Loss: 0.116188
Train Epoch: 2 [4097024/8742050 (46.9%)]	Loss: 0.099130
Train Epoch: 2 [4148224/8742050 (47.5%)]	Loss: 0.107384
Train Epoch: 2 [4199424/8742050 (48.0%)]	Loss: 0.088609
Train Epoch: 2 [4250624/8742050 (48.6%)]	Loss: 0.127649
Train Epoch: 2 [4301824/8742050 (49.2%)]	Loss: 0.104641
Train Epoch: 2 [4353024/8742050 (49.8%)]	Loss: 0.109295
Train Epoch: 2 [4404224/8742050 (50.4%)]	Loss: 0.112246
Train Epoch: 2 [4455424/8742050 (51.0%)]	Loss: 0.115168
Train Epoch: 2 [4506624/8742050 (51.6%)]	Loss: 0.102237
Train Epoch: 2 [4557824/8742050 (52.1%)]	Loss: 0.114499
Train Epoch: 2 [4609024/8742050 (52.7%)]	Loss: 0.104258
Train Epoch: 2 [4660224/8742050 (53.3%)]	Loss: 0.112255
Train Epoch: 2 [4711424/8742050 (53.9%)]	Loss: 0.108616
Train Epoch: 2 [4762624/8742050 (54.5%)]	Loss: 0.100640
Train Epoch: 2 [4813824/8742050 (55.1%)]	Loss: 0.107664
Train Epoch: 2 [4865024/8742050 (55.7%)]	Loss: 0.112820
Train Epoch: 2 [4916224/8742050 (56.2%)]	Loss: 0.110353
Train Epoch: 2 [4967424/8742050 (56.8%)]	Loss: 0.123268
Train Epoch: 2 [5018624/8742050 (57.4%)]	Loss: 0.102764
Train Epoch: 2 [5069824/8742050 (58.0%)]	Loss: 0.105316
Train Epoch: 2 [5121024/8742050 (58.6%)]	Loss: 0.115145
Train Epoch: 2 [5172224/8742050 (59.2%)]	Loss: 0.112989
Train Epoch: 2 [5223424/8742050 (59.8%)]	Loss: 0.094630
Train Epoch: 2 [5274624/8742050 (60.3%)]	Loss: 0.106965
Train Epoch: 2 [5325824/8742050 (60.9%)]	Loss: 0.104439
Train Epoch: 2 [5377024/8742050 (61.5%)]	Loss: 0.105435
Train Epoch: 2 [5428224/8742050 (62.1%)]	Loss: 0.112075
Train Epoch: 2 [5479424/8742050 (62.7%)]	Loss: 0.100444
Train Epoch: 2 [5530624/8742050 (63.3%)]	Loss: 0.115161
Train Epoch: 2 [5581824/8742050 (63.9%)]	Loss: 0.103984
Train Epoch: 2 [5633024/8742050 (64.4%)]	Loss: 0.101504
Train Epoch: 2 [5684224/8742050 (65.0%)]	Loss: 0.113886
Train Epoch: 2 [5735424/8742050 (65.6%)]	Loss: 0.116752
Train Epoch: 2 [5786624/8742050 (66.2%)]	Loss: 0.104819
Train Epoch: 2 [5837824/8742050 (66.8%)]	Loss: 0.107333
Train Epoch: 2 [5889024/8742050 (67.4%)]	Loss: 0.101457
Train Epoch: 2 [5940224/8742050 (68.0%)]	Loss: 0.107454
Train Epoch: 2 [5991424/8742050 (68.5%)]	Loss: 0.112901
Train Epoch: 2 [6042624/8742050 (69.1%)]	Loss: 0.123037
Train Epoch: 2 [6093824/8742050 (69.7%)]	Loss: 0.106390
Train Epoch: 2 [6145024/8742050 (70.3%)]	Loss: 0.120769
Train Epoch: 2 [6196224/8742050 (70.9%)]	Loss: 0.106791
Train Epoch: 2 [6247424/8742050 (71.5%)]	Loss: 0.111019
Train Epoch: 2 [6298624/8742050 (72.0%)]	Loss: 0.108799
Train Epoch: 2 [6349824/8742050 (72.6%)]	Loss: 0.117887
Train Epoch: 2 [6401024/8742050 (73.2%)]	Loss: 0.103627
Train Epoch: 2 [6452224/8742050 (73.8%)]	Loss: 0.114166
Train Epoch: 2 [6503424/8742050 (74.4%)]	Loss: 0.117342
Train Epoch: 2 [6554624/8742050 (75.0%)]	Loss: 0.117482
Train Epoch: 2 [6605824/8742050 (75.6%)]	Loss: 0.115666
Train Epoch: 2 [6657024/8742050 (76.1%)]	Loss: 0.131091
Train Epoch: 2 [6708224/8742050 (76.7%)]	Loss: 0.117222
Train Epoch: 2 [6759424/8742050 (77.3%)]	Loss: 0.118022
Train Epoch: 2 [6810624/8742050 (77.9%)]	Loss: 0.115037
Train Epoch: 2 [6861824/8742050 (78.5%)]	Loss: 0.115138
Train Epoch: 2 [6913024/8742050 (79.1%)]	Loss: 0.108036
Train Epoch: 2 [6964224/8742050 (79.7%)]	Loss: 0.107739
Train Epoch: 2 [7015424/8742050 (80.2%)]	Loss: 0.106692
Train Epoch: 2 [7066624/8742050 (80.8%)]	Loss: 0.111079
Train Epoch: 2 [7117824/8742050 (81.4%)]	Loss: 0.111988
Train Epoch: 2 [7169024/8742050 (82.0%)]	Loss: 0.106956
Train Epoch: 2 [7220224/8742050 (82.6%)]	Loss: 0.111755
Train Epoch: 2 [7271424/8742050 (83.2%)]	Loss: 0.120712
Train Epoch: 2 [7322624/8742050 (83.8%)]	Loss: 0.113820
Train Epoch: 2 [7373824/8742050 (84.3%)]	Loss: 0.103755
Train Epoch: 2 [7425024/8742050 (84.9%)]	Loss: 0.107555
Train Epoch: 2 [7476224/8742050 (85.5%)]	Loss: 0.107095
Train Epoch: 2 [7527424/8742050 (86.1%)]	Loss: 0.111366
Train Epoch: 2 [7578624/8742050 (86.7%)]	Loss: 0.102553
Train Epoch: 2 [7629824/8742050 (87.3%)]	Loss: 0.128782
Train Epoch: 2 [7681024/8742050 (87.9%)]	Loss: 0.100240
Train Epoch: 2 [7732224/8742050 (88.4%)]	Loss: 0.106598
Train Epoch: 2 [7783424/8742050 (89.0%)]	Loss: 0.104583
Train Epoch: 2 [7834624/8742050 (89.6%)]	Loss: 0.113998
Train Epoch: 2 [7885824/8742050 (90.2%)]	Loss: 0.097222
Train Epoch: 2 [7937024/8742050 (90.8%)]	Loss: 0.103912
Train Epoch: 2 [7988224/8742050 (91.4%)]	Loss: 0.099102
Train Epoch: 2 [8039424/8742050 (92.0%)]	Loss: 0.117088
Train Epoch: 2 [8090624/8742050 (92.5%)]	Loss: 0.110416
Train Epoch: 2 [8141824/8742050 (93.1%)]	Loss: 0.124146
Train Epoch: 2 [8193024/8742050 (93.7%)]	Loss: 0.100643
Train Epoch: 2 [8244224/8742050 (94.3%)]	Loss: 0.088121
Train Epoch: 2 [8295424/8742050 (94.9%)]	Loss: 0.103189
Train Epoch: 2 [8346624/8742050 (95.5%)]	Loss: 0.109416
Train Epoch: 2 [8397824/8742050 (96.1%)]	Loss: 0.106887
Train Epoch: 2 [8449024/8742050 (96.6%)]	Loss: 0.106530
Train Epoch: 2 [8500224/8742050 (97.2%)]	Loss: 0.118253
Train Epoch: 2 [8551424/8742050 (97.8%)]	Loss: 0.109767
Train Epoch: 2 [8602624/8742050 (98.4%)]	Loss: 0.127364
Train Epoch: 2 [8653824/8742050 (99.0%)]	Loss: 0.106207
Train Epoch: 2 [8705024/8742050 (99.6%)]	Loss: 0.100737

ACC in fold#0 was 0.915


Balanced ACC in fold#0 was 0.868


MCC in fold#0 was 0.794


Confusion Matrix in fold#0: 
           nonRipple   Ripple
nonRipple     489443   163136
Ripple         22652  1510282


Classification Report in fold#0: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.956        0.903  ...        0.929         0.918
recall            0.750        0.985  ...        0.868         0.915
f1-score          0.840        0.942  ...        0.891         0.912
sample size  652579.000  1532934.000  ...  2185513.000   2185513.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/8742050 (0.0%)]	Loss: 0.351193
Train Epoch: 1 [52224/8742050 (0.6%)]	Loss: 0.157634
Train Epoch: 1 [103424/8742050 (1.2%)]	Loss: 0.119277
Train Epoch: 1 [154624/8742050 (1.8%)]	Loss: 0.124371
Train Epoch: 1 [205824/8742050 (2.4%)]	Loss: 0.134527
Train Epoch: 1 [257024/8742050 (2.9%)]	Loss: 0.130706
Train Epoch: 1 [308224/8742050 (3.5%)]	Loss: 0.130148
Train Epoch: 1 [359424/8742050 (4.1%)]	Loss: 0.110181
Train Epoch: 1 [410624/8742050 (4.7%)]	Loss: 0.136719
Train Epoch: 1 [461824/8742050 (5.3%)]	Loss: 0.127635
Train Epoch: 1 [513024/8742050 (5.9%)]	Loss: 0.107536
Train Epoch: 1 [564224/8742050 (6.5%)]	Loss: 0.123588
Train Epoch: 1 [615424/8742050 (7.0%)]	Loss: 0.122035
Train Epoch: 1 [666624/8742050 (7.6%)]	Loss: 0.137870
Train Epoch: 1 [717824/8742050 (8.2%)]	Loss: 0.124580
Train Epoch: 1 [769024/8742050 (8.8%)]	Loss: 0.114122
Train Epoch: 1 [820224/8742050 (9.4%)]	Loss: 0.125672
Train Epoch: 1 [871424/8742050 (10.0%)]	Loss: 0.109374
Train Epoch: 1 [922624/8742050 (10.6%)]	Loss: 0.108900
Train Epoch: 1 [973824/8742050 (11.1%)]	Loss: 0.138640
Train Epoch: 1 [1025024/8742050 (11.7%)]	Loss: 0.115883
Train Epoch: 1 [1076224/8742050 (12.3%)]	Loss: 0.121793
Train Epoch: 1 [1127424/8742050 (12.9%)]	Loss: 0.120047
Train Epoch: 1 [1178624/8742050 (13.5%)]	Loss: 0.126703
Train Epoch: 1 [1229824/8742050 (14.1%)]	Loss: 0.122838
Train Epoch: 1 [1281024/8742050 (14.7%)]	Loss: 0.108422
Train Epoch: 1 [1332224/8742050 (15.2%)]	Loss: 0.112899
Train Epoch: 1 [1383424/8742050 (15.8%)]	Loss: 0.112304
Train Epoch: 1 [1434624/8742050 (16.4%)]	Loss: 0.119563
Train Epoch: 1 [1485824/8742050 (17.0%)]	Loss: 0.117116
Train Epoch: 1 [1537024/8742050 (17.6%)]	Loss: 0.122866
Train Epoch: 1 [1588224/8742050 (18.2%)]	Loss: 0.100190
Train Epoch: 1 [1639424/8742050 (18.8%)]	Loss: 0.115111
Train Epoch: 1 [1690624/8742050 (19.3%)]	Loss: 0.112741
Train Epoch: 1 [1741824/8742050 (19.9%)]	Loss: 0.117676
Train Epoch: 1 [1793024/8742050 (20.5%)]	Loss: 0.119738
Train Epoch: 1 [1844224/8742050 (21.1%)]	Loss: 0.113360
Train Epoch: 1 [1895424/8742050 (21.7%)]	Loss: 0.112265
Train Epoch: 1 [1946624/8742050 (22.3%)]	Loss: 0.108501
Train Epoch: 1 [1997824/8742050 (22.9%)]	Loss: 0.104534
Train Epoch: 1 [2049024/8742050 (23.4%)]	Loss: 0.104028
Train Epoch: 1 [2100224/8742050 (24.0%)]	Loss: 0.118846
Train Epoch: 1 [2151424/8742050 (24.6%)]	Loss: 0.122987
Train Epoch: 1 [2202624/8742050 (25.2%)]	Loss: 0.113358
Train Epoch: 1 [2253824/8742050 (25.8%)]	Loss: 0.121081
Train Epoch: 1 [2305024/8742050 (26.4%)]	Loss: 0.117037
Train Epoch: 1 [2356224/8742050 (27.0%)]	Loss: 0.119366
Train Epoch: 1 [2407424/8742050 (27.5%)]	Loss: 0.122730
Train Epoch: 1 [2458624/8742050 (28.1%)]	Loss: 0.116533
Train Epoch: 1 [2509824/8742050 (28.7%)]	Loss: 0.125050
Train Epoch: 1 [2561024/8742050 (29.3%)]	Loss: 0.107432
Train Epoch: 1 [2612224/8742050 (29.9%)]	Loss: 0.130340
Train Epoch: 1 [2663424/8742050 (30.5%)]	Loss: 0.125448
Train Epoch: 1 [2714624/8742050 (31.1%)]	Loss: 0.116479
Train Epoch: 1 [2765824/8742050 (31.6%)]	Loss: 0.112076
Train Epoch: 1 [2817024/8742050 (32.2%)]	Loss: 0.121271
Train Epoch: 1 [2868224/8742050 (32.8%)]	Loss: 0.115479
Train Epoch: 1 [2919424/8742050 (33.4%)]	Loss: 0.117499
Train Epoch: 1 [2970624/8742050 (34.0%)]	Loss: 0.108315
Train Epoch: 1 [3021824/8742050 (34.6%)]	Loss: 0.106243
Train Epoch: 1 [3073024/8742050 (35.2%)]	Loss: 0.115694
Train Epoch: 1 [3124224/8742050 (35.7%)]	Loss: 0.119922
Train Epoch: 1 [3175424/8742050 (36.3%)]	Loss: 0.111357
Train Epoch: 1 [3226624/8742050 (36.9%)]	Loss: 0.108952
Train Epoch: 1 [3277824/8742050 (37.5%)]	Loss: 0.113963
Train Epoch: 1 [3329024/8742050 (38.1%)]	Loss: 0.113349
Train Epoch: 1 [3380224/8742050 (38.7%)]	Loss: 0.117472
Train Epoch: 1 [3431424/8742050 (39.3%)]	Loss: 0.125376
Train Epoch: 1 [3482624/8742050 (39.8%)]	Loss: 0.127198
Train Epoch: 1 [3533824/8742050 (40.4%)]	Loss: 0.130915
Train Epoch: 1 [3585024/8742050 (41.0%)]	Loss: 0.122665
Train Epoch: 1 [3636224/8742050 (41.6%)]	Loss: 0.110527
Train Epoch: 1 [3687424/8742050 (42.2%)]	Loss: 0.124134
Train Epoch: 1 [3738624/8742050 (42.8%)]	Loss: 0.110466
Train Epoch: 1 [3789824/8742050 (43.4%)]	Loss: 0.129527
Train Epoch: 1 [3841024/8742050 (43.9%)]	Loss: 0.111232
Train Epoch: 1 [3892224/8742050 (44.5%)]	Loss: 0.104662
Train Epoch: 1 [3943424/8742050 (45.1%)]	Loss: 0.097962
Train Epoch: 1 [3994624/8742050 (45.7%)]	Loss: 0.127321
Train Epoch: 1 [4045824/8742050 (46.3%)]	Loss: 0.113417
Train Epoch: 1 [4097024/8742050 (46.9%)]	Loss: 0.115540
Train Epoch: 1 [4148224/8742050 (47.5%)]	Loss: 0.129752
Train Epoch: 1 [4199424/8742050 (48.0%)]	Loss: 0.114765
Train Epoch: 1 [4250624/8742050 (48.6%)]	Loss: 0.107183
Train Epoch: 1 [4301824/8742050 (49.2%)]	Loss: 0.115488
Train Epoch: 1 [4353024/8742050 (49.8%)]	Loss: 0.113448
Train Epoch: 1 [4404224/8742050 (50.4%)]	Loss: 0.125031
Train Epoch: 1 [4455424/8742050 (51.0%)]	Loss: 0.122179
Train Epoch: 1 [4506624/8742050 (51.6%)]	Loss: 0.118365
Train Epoch: 1 [4557824/8742050 (52.1%)]	Loss: 0.113371
Train Epoch: 1 [4609024/8742050 (52.7%)]	Loss: 0.118641
Train Epoch: 1 [4660224/8742050 (53.3%)]	Loss: 0.106211
Train Epoch: 1 [4711424/8742050 (53.9%)]	Loss: 0.123619
Train Epoch: 1 [4762624/8742050 (54.5%)]	Loss: 0.128495
Train Epoch: 1 [4813824/8742050 (55.1%)]	Loss: 0.107308
Train Epoch: 1 [4865024/8742050 (55.7%)]	Loss: 0.101393
Train Epoch: 1 [4916224/8742050 (56.2%)]	Loss: 0.128197
Train Epoch: 1 [4967424/8742050 (56.8%)]	Loss: 0.116644
Train Epoch: 1 [5018624/8742050 (57.4%)]	Loss: 0.113456
Train Epoch: 1 [5069824/8742050 (58.0%)]	Loss: 0.106717
Train Epoch: 1 [5121024/8742050 (58.6%)]	Loss: 0.114056
Train Epoch: 1 [5172224/8742050 (59.2%)]	Loss: 0.100801
Train Epoch: 1 [5223424/8742050 (59.8%)]	Loss: 0.119878
Train Epoch: 1 [5274624/8742050 (60.3%)]	Loss: 0.100337
Train Epoch: 1 [5325824/8742050 (60.9%)]	Loss: 0.111008
Train Epoch: 1 [5377024/8742050 (61.5%)]	Loss: 0.114700
Train Epoch: 1 [5428224/8742050 (62.1%)]	Loss: 0.103629
Train Epoch: 1 [5479424/8742050 (62.7%)]	Loss: 0.100162
Train Epoch: 1 [5530624/8742050 (63.3%)]	Loss: 0.112223
Train Epoch: 1 [5581824/8742050 (63.9%)]	Loss: 0.108417
Train Epoch: 1 [5633024/8742050 (64.4%)]	Loss: 0.113268
Train Epoch: 1 [5684224/8742050 (65.0%)]	Loss: 0.114694
Train Epoch: 1 [5735424/8742050 (65.6%)]	Loss: 0.112081
Train Epoch: 1 [5786624/8742050 (66.2%)]	Loss: 0.115492
Train Epoch: 1 [5837824/8742050 (66.8%)]	Loss: 0.108208
Train Epoch: 1 [5889024/8742050 (67.4%)]	Loss: 0.117308
Train Epoch: 1 [5940224/8742050 (68.0%)]	Loss: 0.112776
Train Epoch: 1 [5991424/8742050 (68.5%)]	Loss: 0.109507
Train Epoch: 1 [6042624/8742050 (69.1%)]	Loss: 0.112319
Train Epoch: 1 [6093824/8742050 (69.7%)]	Loss: 0.124019
Train Epoch: 1 [6145024/8742050 (70.3%)]	Loss: 0.115500
Train Epoch: 1 [6196224/8742050 (70.9%)]	Loss: 0.108980
Train Epoch: 1 [6247424/8742050 (71.5%)]	Loss: 0.122993
Train Epoch: 1 [6298624/8742050 (72.0%)]	Loss: 0.113252
Train Epoch: 1 [6349824/8742050 (72.6%)]	Loss: 0.094304
Train Epoch: 1 [6401024/8742050 (73.2%)]	Loss: 0.098429
Train Epoch: 1 [6452224/8742050 (73.8%)]	Loss: 0.104064
Train Epoch: 1 [6503424/8742050 (74.4%)]	Loss: 0.107502
Train Epoch: 1 [6554624/8742050 (75.0%)]	Loss: 0.124616
Train Epoch: 1 [6605824/8742050 (75.6%)]	Loss: 0.106346
Train Epoch: 1 [6657024/8742050 (76.1%)]	Loss: 0.099156
Train Epoch: 1 [6708224/8742050 (76.7%)]	Loss: 0.111085
Train Epoch: 1 [6759424/8742050 (77.3%)]	Loss: 0.101512
Train Epoch: 1 [6810624/8742050 (77.9%)]	Loss: 0.110922
Train Epoch: 1 [6861824/8742050 (78.5%)]	Loss: 0.105431
Train Epoch: 1 [6913024/8742050 (79.1%)]	Loss: 0.115950
Train Epoch: 1 [6964224/8742050 (79.7%)]	Loss: 0.113182
Train Epoch: 1 [7015424/8742050 (80.2%)]	Loss: 0.122489
Train Epoch: 1 [7066624/8742050 (80.8%)]	Loss: 0.097711
Train Epoch: 1 [7117824/8742050 (81.4%)]	Loss: 0.123965
Train Epoch: 1 [7169024/8742050 (82.0%)]	Loss: 0.116823
Train Epoch: 1 [7220224/8742050 (82.6%)]	Loss: 0.120914
Train Epoch: 1 [7271424/8742050 (83.2%)]	Loss: 0.115970
Train Epoch: 1 [7322624/8742050 (83.8%)]	Loss: 0.118048
Train Epoch: 1 [7373824/8742050 (84.3%)]	Loss: 0.111112
Train Epoch: 1 [7425024/8742050 (84.9%)]	Loss: 0.116766
Train Epoch: 1 [7476224/8742050 (85.5%)]	Loss: 0.131469
Train Epoch: 1 [7527424/8742050 (86.1%)]	Loss: 0.112384
Train Epoch: 1 [7578624/8742050 (86.7%)]	Loss: 0.109610
Train Epoch: 1 [7629824/8742050 (87.3%)]	Loss: 0.098460
Train Epoch: 1 [7681024/8742050 (87.9%)]	Loss: 0.103608
Train Epoch: 1 [7732224/8742050 (88.4%)]	Loss: 0.110176
Train Epoch: 1 [7783424/8742050 (89.0%)]	Loss: 0.096326
Train Epoch: 1 [7834624/8742050 (89.6%)]	Loss: 0.112384
Train Epoch: 1 [7885824/8742050 (90.2%)]	Loss: 0.122384
Train Epoch: 1 [7937024/8742050 (90.8%)]	Loss: 0.105212
Train Epoch: 1 [7988224/8742050 (91.4%)]	Loss: 0.107920
Train Epoch: 1 [8039424/8742050 (92.0%)]	Loss: 0.110682
Train Epoch: 1 [8090624/8742050 (92.5%)]	Loss: 0.106239
Train Epoch: 1 [8141824/8742050 (93.1%)]	Loss: 0.109551
Train Epoch: 1 [8193024/8742050 (93.7%)]	Loss: 0.114273
Train Epoch: 1 [8244224/8742050 (94.3%)]	Loss: 0.107216
Train Epoch: 1 [8295424/8742050 (94.9%)]	Loss: 0.101329
Train Epoch: 1 [8346624/8742050 (95.5%)]	Loss: 0.121755
Train Epoch: 1 [8397824/8742050 (96.1%)]	Loss: 0.106019
Train Epoch: 1 [8449024/8742050 (96.6%)]	Loss: 0.108138
Train Epoch: 1 [8500224/8742050 (97.2%)]	Loss: 0.096338
Train Epoch: 1 [8551424/8742050 (97.8%)]	Loss: 0.109880
Train Epoch: 1 [8602624/8742050 (98.4%)]	Loss: 0.107640
Train Epoch: 1 [8653824/8742050 (99.0%)]	Loss: 0.109521
Train Epoch: 1 [8705024/8742050 (99.6%)]	Loss: 0.113430
Train Epoch: 2 [1024/8742050 (0.0%)]	Loss: 0.108975
Train Epoch: 2 [52224/8742050 (0.6%)]	Loss: 0.109563
Train Epoch: 2 [103424/8742050 (1.2%)]	Loss: 0.116602
Train Epoch: 2 [154624/8742050 (1.8%)]	Loss: 0.112951
Train Epoch: 2 [205824/8742050 (2.4%)]	Loss: 0.126811
Train Epoch: 2 [257024/8742050 (2.9%)]	Loss: 0.116923
Train Epoch: 2 [308224/8742050 (3.5%)]	Loss: 0.111164
Train Epoch: 2 [359424/8742050 (4.1%)]	Loss: 0.120023
Train Epoch: 2 [410624/8742050 (4.7%)]	Loss: 0.112229
Train Epoch: 2 [461824/8742050 (5.3%)]	Loss: 0.098242
Train Epoch: 2 [513024/8742050 (5.9%)]	Loss: 0.095469
Train Epoch: 2 [564224/8742050 (6.5%)]	Loss: 0.116534
Train Epoch: 2 [615424/8742050 (7.0%)]	Loss: 0.117402
Train Epoch: 2 [666624/8742050 (7.6%)]	Loss: 0.113888
Train Epoch: 2 [717824/8742050 (8.2%)]	Loss: 0.094981
Train Epoch: 2 [769024/8742050 (8.8%)]	Loss: 0.106777
Train Epoch: 2 [820224/8742050 (9.4%)]	Loss: 0.117694
Train Epoch: 2 [871424/8742050 (10.0%)]	Loss: 0.112803
Train Epoch: 2 [922624/8742050 (10.6%)]	Loss: 0.091464
Train Epoch: 2 [973824/8742050 (11.1%)]	Loss: 0.105165
Train Epoch: 2 [1025024/8742050 (11.7%)]	Loss: 0.109683
Train Epoch: 2 [1076224/8742050 (12.3%)]	Loss: 0.114921
Train Epoch: 2 [1127424/8742050 (12.9%)]	Loss: 0.110621
Train Epoch: 2 [1178624/8742050 (13.5%)]	Loss: 0.106297
Train Epoch: 2 [1229824/8742050 (14.1%)]	Loss: 0.112587
Train Epoch: 2 [1281024/8742050 (14.7%)]	Loss: 0.103898
Train Epoch: 2 [1332224/8742050 (15.2%)]	Loss: 0.117430
Train Epoch: 2 [1383424/8742050 (15.8%)]	Loss: 0.103473
Train Epoch: 2 [1434624/8742050 (16.4%)]	Loss: 0.125594
Train Epoch: 2 [1485824/8742050 (17.0%)]	Loss: 0.116382
Train Epoch: 2 [1537024/8742050 (17.6%)]	Loss: 0.112551
Train Epoch: 2 [1588224/8742050 (18.2%)]	Loss: 0.113128
Train Epoch: 2 [1639424/8742050 (18.8%)]	Loss: 0.115533
Train Epoch: 2 [1690624/8742050 (19.3%)]	Loss: 0.098402
Train Epoch: 2 [1741824/8742050 (19.9%)]	Loss: 0.100031
Train Epoch: 2 [1793024/8742050 (20.5%)]	Loss: 0.101151
Train Epoch: 2 [1844224/8742050 (21.1%)]	Loss: 0.107344
Train Epoch: 2 [1895424/8742050 (21.7%)]	Loss: 0.104087
Train Epoch: 2 [1946624/8742050 (22.3%)]	Loss: 0.114929
Train Epoch: 2 [1997824/8742050 (22.9%)]	Loss: 0.106593
Train Epoch: 2 [2049024/8742050 (23.4%)]	Loss: 0.108852
Train Epoch: 2 [2100224/8742050 (24.0%)]	Loss: 0.099655
Train Epoch: 2 [2151424/8742050 (24.6%)]	Loss: 0.123172
Train Epoch: 2 [2202624/8742050 (25.2%)]	Loss: 0.117434
Train Epoch: 2 [2253824/8742050 (25.8%)]	Loss: 0.110310
Train Epoch: 2 [2305024/8742050 (26.4%)]	Loss: 0.116847
Train Epoch: 2 [2356224/8742050 (27.0%)]	Loss: 0.101548
Train Epoch: 2 [2407424/8742050 (27.5%)]	Loss: 0.100759
Train Epoch: 2 [2458624/8742050 (28.1%)]	Loss: 0.114126
Train Epoch: 2 [2509824/8742050 (28.7%)]	Loss: 0.110738
Train Epoch: 2 [2561024/8742050 (29.3%)]	Loss: 0.119704
Train Epoch: 2 [2612224/8742050 (29.9%)]	Loss: 0.116733
Train Epoch: 2 [2663424/8742050 (30.5%)]	Loss: 0.104140
Train Epoch: 2 [2714624/8742050 (31.1%)]	Loss: 0.109237
Train Epoch: 2 [2765824/8742050 (31.6%)]	Loss: 0.113022
Train Epoch: 2 [2817024/8742050 (32.2%)]	Loss: 0.098351
Train Epoch: 2 [2868224/8742050 (32.8%)]	Loss: 0.107245
Train Epoch: 2 [2919424/8742050 (33.4%)]	Loss: 0.116240
Train Epoch: 2 [2970624/8742050 (34.0%)]	Loss: 0.098003
Train Epoch: 2 [3021824/8742050 (34.6%)]	Loss: 0.120067
Train Epoch: 2 [3073024/8742050 (35.2%)]	Loss: 0.114391
Train Epoch: 2 [3124224/8742050 (35.7%)]	Loss: 0.098594
Train Epoch: 2 [3175424/8742050 (36.3%)]	Loss: 0.111147
Train Epoch: 2 [3226624/8742050 (36.9%)]	Loss: 0.106133
Train Epoch: 2 [3277824/8742050 (37.5%)]	Loss: 0.110720
Train Epoch: 2 [3329024/8742050 (38.1%)]	Loss: 0.101337
Train Epoch: 2 [3380224/8742050 (38.7%)]	Loss: 0.116817
Train Epoch: 2 [3431424/8742050 (39.3%)]	Loss: 0.109958
Train Epoch: 2 [3482624/8742050 (39.8%)]	Loss: 0.098305
Train Epoch: 2 [3533824/8742050 (40.4%)]	Loss: 0.107149
Train Epoch: 2 [3585024/8742050 (41.0%)]	Loss: 0.101502
Train Epoch: 2 [3636224/8742050 (41.6%)]	Loss: 0.117563
Train Epoch: 2 [3687424/8742050 (42.2%)]	Loss: 0.109472
Train Epoch: 2 [3738624/8742050 (42.8%)]	Loss: 0.102915
Train Epoch: 2 [3789824/8742050 (43.4%)]	Loss: 0.100203
Train Epoch: 2 [3841024/8742050 (43.9%)]	Loss: 0.109757
Train Epoch: 2 [3892224/8742050 (44.5%)]	Loss: 0.107689
Train Epoch: 2 [3943424/8742050 (45.1%)]	Loss: 0.119374
Train Epoch: 2 [3994624/8742050 (45.7%)]	Loss: 0.119741
Train Epoch: 2 [4045824/8742050 (46.3%)]	Loss: 0.114326
Train Epoch: 2 [4097024/8742050 (46.9%)]	Loss: 0.107510
Train Epoch: 2 [4148224/8742050 (47.5%)]	Loss: 0.109738
Train Epoch: 2 [4199424/8742050 (48.0%)]	Loss: 0.099715
Train Epoch: 2 [4250624/8742050 (48.6%)]	Loss: 0.100354
Train Epoch: 2 [4301824/8742050 (49.2%)]	Loss: 0.116440
Train Epoch: 2 [4353024/8742050 (49.8%)]	Loss: 0.137237
Train Epoch: 2 [4404224/8742050 (50.4%)]	Loss: 0.107750
Train Epoch: 2 [4455424/8742050 (51.0%)]	Loss: 0.112105
Train Epoch: 2 [4506624/8742050 (51.6%)]	Loss: 0.108421
Train Epoch: 2 [4557824/8742050 (52.1%)]	Loss: 0.107365
Train Epoch: 2 [4609024/8742050 (52.7%)]	Loss: 0.111772
Train Epoch: 2 [4660224/8742050 (53.3%)]	Loss: 0.116404
Train Epoch: 2 [4711424/8742050 (53.9%)]	Loss: 0.094466
Train Epoch: 2 [4762624/8742050 (54.5%)]	Loss: 0.104661
Train Epoch: 2 [4813824/8742050 (55.1%)]	Loss: 0.118183
Train Epoch: 2 [4865024/8742050 (55.7%)]	Loss: 0.112144
Train Epoch: 2 [4916224/8742050 (56.2%)]	Loss: 0.101647
Train Epoch: 2 [4967424/8742050 (56.8%)]	Loss: 0.106678
Train Epoch: 2 [5018624/8742050 (57.4%)]	Loss: 0.097331
Train Epoch: 2 [5069824/8742050 (58.0%)]	Loss: 0.111743
Train Epoch: 2 [5121024/8742050 (58.6%)]	Loss: 0.113827
Train Epoch: 2 [5172224/8742050 (59.2%)]	Loss: 0.130400
Train Epoch: 2 [5223424/8742050 (59.8%)]	Loss: 0.109229
Train Epoch: 2 [5274624/8742050 (60.3%)]	Loss: 0.112132
Train Epoch: 2 [5325824/8742050 (60.9%)]	Loss: 0.107316
Train Epoch: 2 [5377024/8742050 (61.5%)]	Loss: 0.102590
Train Epoch: 2 [5428224/8742050 (62.1%)]	Loss: 0.115718
Train Epoch: 2 [5479424/8742050 (62.7%)]	Loss: 0.105527
Train Epoch: 2 [5530624/8742050 (63.3%)]	Loss: 0.112392
Train Epoch: 2 [5581824/8742050 (63.9%)]	Loss: 0.124080
Train Epoch: 2 [5633024/8742050 (64.4%)]	Loss: 0.102319
Train Epoch: 2 [5684224/8742050 (65.0%)]	Loss: 0.124521
Train Epoch: 2 [5735424/8742050 (65.6%)]	Loss: 0.099352
Train Epoch: 2 [5786624/8742050 (66.2%)]	Loss: 0.109805
Train Epoch: 2 [5837824/8742050 (66.8%)]	Loss: 0.103812
Train Epoch: 2 [5889024/8742050 (67.4%)]	Loss: 0.108624
Train Epoch: 2 [5940224/8742050 (68.0%)]	Loss: 0.112255
Train Epoch: 2 [5991424/8742050 (68.5%)]	Loss: 0.126784
Train Epoch: 2 [6042624/8742050 (69.1%)]	Loss: 0.127213
Train Epoch: 2 [6093824/8742050 (69.7%)]	Loss: 0.115695
Train Epoch: 2 [6145024/8742050 (70.3%)]	Loss: 0.112989
Train Epoch: 2 [6196224/8742050 (70.9%)]	Loss: 0.113667
Train Epoch: 2 [6247424/8742050 (71.5%)]	Loss: 0.097536
Train Epoch: 2 [6298624/8742050 (72.0%)]	Loss: 0.110581
Train Epoch: 2 [6349824/8742050 (72.6%)]	Loss: 0.129870
Train Epoch: 2 [6401024/8742050 (73.2%)]	Loss: 0.102632
Train Epoch: 2 [6452224/8742050 (73.8%)]	Loss: 0.107375
Train Epoch: 2 [6503424/8742050 (74.4%)]	Loss: 0.102935
Train Epoch: 2 [6554624/8742050 (75.0%)]	Loss: 0.111954
Train Epoch: 2 [6605824/8742050 (75.6%)]	Loss: 0.120737
Train Epoch: 2 [6657024/8742050 (76.1%)]	Loss: 0.107178
Train Epoch: 2 [6708224/8742050 (76.7%)]	Loss: 0.116069
Train Epoch: 2 [6759424/8742050 (77.3%)]	Loss: 0.095962
Train Epoch: 2 [6810624/8742050 (77.9%)]	Loss: 0.115028
Train Epoch: 2 [6861824/8742050 (78.5%)]	Loss: 0.110075
Train Epoch: 2 [6913024/8742050 (79.1%)]	Loss: 0.105662
Train Epoch: 2 [6964224/8742050 (79.7%)]	Loss: 0.111683
Train Epoch: 2 [7015424/8742050 (80.2%)]	Loss: 0.100909
Train Epoch: 2 [7066624/8742050 (80.8%)]	Loss: 0.115070
Train Epoch: 2 [7117824/8742050 (81.4%)]	Loss: 0.091368
Train Epoch: 2 [7169024/8742050 (82.0%)]	Loss: 0.103438
Train Epoch: 2 [7220224/8742050 (82.6%)]	Loss: 0.099126
Train Epoch: 2 [7271424/8742050 (83.2%)]	Loss: 0.114536
Train Epoch: 2 [7322624/8742050 (83.8%)]	Loss: 0.112146
Train Epoch: 2 [7373824/8742050 (84.3%)]	Loss: 0.121664
Train Epoch: 2 [7425024/8742050 (84.9%)]	Loss: 0.117867
Train Epoch: 2 [7476224/8742050 (85.5%)]	Loss: 0.111537
Train Epoch: 2 [7527424/8742050 (86.1%)]	Loss: 0.102451
Train Epoch: 2 [7578624/8742050 (86.7%)]	Loss: 0.106590
Train Epoch: 2 [7629824/8742050 (87.3%)]	Loss: 0.110070
Train Epoch: 2 [7681024/8742050 (87.9%)]	Loss: 0.106530
Train Epoch: 2 [7732224/8742050 (88.4%)]	Loss: 0.110078
Train Epoch: 2 [7783424/8742050 (89.0%)]	Loss: 0.126482
Train Epoch: 2 [7834624/8742050 (89.6%)]	Loss: 0.089342
Train Epoch: 2 [7885824/8742050 (90.2%)]	Loss: 0.095239
Train Epoch: 2 [7937024/8742050 (90.8%)]	Loss: 0.126033
Train Epoch: 2 [7988224/8742050 (91.4%)]	Loss: 0.101319
Train Epoch: 2 [8039424/8742050 (92.0%)]	Loss: 0.121981
Train Epoch: 2 [8090624/8742050 (92.5%)]	Loss: 0.112262
Train Epoch: 2 [8141824/8742050 (93.1%)]	Loss: 0.124435
Train Epoch: 2 [8193024/8742050 (93.7%)]	Loss: 0.099058
Train Epoch: 2 [8244224/8742050 (94.3%)]	Loss: 0.109493
Train Epoch: 2 [8295424/8742050 (94.9%)]	Loss: 0.110004
Train Epoch: 2 [8346624/8742050 (95.5%)]	Loss: 0.110751
Train Epoch: 2 [8397824/8742050 (96.1%)]	Loss: 0.114529
Train Epoch: 2 [8449024/8742050 (96.6%)]	Loss: 0.119427
Train Epoch: 2 [8500224/8742050 (97.2%)]	Loss: 0.110435
Train Epoch: 2 [8551424/8742050 (97.8%)]	Loss: 0.122443
Train Epoch: 2 [8602624/8742050 (98.4%)]	Loss: 0.103443
Train Epoch: 2 [8653824/8742050 (99.0%)]	Loss: 0.109231
Train Epoch: 2 [8705024/8742050 (99.6%)]	Loss: 0.112484

ACC in fold#1 was 0.908


Balanced ACC in fold#1 was 0.869


MCC in fold#1 was 0.776


Confusion Matrix in fold#1: 
           nonRipple   Ripple
nonRipple     503811   148768
Ripple         51297  1481637


Classification Report in fold#1: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.908        0.909  ...        0.908         0.908
recall            0.772        0.967  ...        0.869         0.908
f1-score          0.834        0.937  ...        0.886         0.906
sample size  652579.000  1532934.000  ...  2185513.000   2185513.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/8742050 (0.0%)]	Loss: 0.358775
Train Epoch: 1 [52224/8742050 (0.6%)]	Loss: 0.158571
Train Epoch: 1 [103424/8742050 (1.2%)]	Loss: 0.123063
Train Epoch: 1 [154624/8742050 (1.8%)]	Loss: 0.116434
Train Epoch: 1 [205824/8742050 (2.4%)]	Loss: 0.107411
Train Epoch: 1 [257024/8742050 (2.9%)]	Loss: 0.114297
Train Epoch: 1 [308224/8742050 (3.5%)]	Loss: 0.112651
Train Epoch: 1 [359424/8742050 (4.1%)]	Loss: 0.116080
Train Epoch: 1 [410624/8742050 (4.7%)]	Loss: 0.127048
Train Epoch: 1 [461824/8742050 (5.3%)]	Loss: 0.114515
Train Epoch: 1 [513024/8742050 (5.9%)]	Loss: 0.118061
Train Epoch: 1 [564224/8742050 (6.5%)]	Loss: 0.123991
Train Epoch: 1 [615424/8742050 (7.0%)]	Loss: 0.107736
Train Epoch: 1 [666624/8742050 (7.6%)]	Loss: 0.110784
Train Epoch: 1 [717824/8742050 (8.2%)]	Loss: 0.107002
Train Epoch: 1 [769024/8742050 (8.8%)]	Loss: 0.108354
Train Epoch: 1 [820224/8742050 (9.4%)]	Loss: 0.121516
Train Epoch: 1 [871424/8742050 (10.0%)]	Loss: 0.123081
Train Epoch: 1 [922624/8742050 (10.6%)]	Loss: 0.118757
Train Epoch: 1 [973824/8742050 (11.1%)]	Loss: 0.127586
Train Epoch: 1 [1025024/8742050 (11.7%)]	Loss: 0.126194
Train Epoch: 1 [1076224/8742050 (12.3%)]	Loss: 0.109024
Train Epoch: 1 [1127424/8742050 (12.9%)]	Loss: 0.123368
Train Epoch: 1 [1178624/8742050 (13.5%)]	Loss: 0.116897
Train Epoch: 1 [1229824/8742050 (14.1%)]	Loss: 0.111215
Train Epoch: 1 [1281024/8742050 (14.7%)]	Loss: 0.105452
Train Epoch: 1 [1332224/8742050 (15.2%)]	Loss: 0.121504
Train Epoch: 1 [1383424/8742050 (15.8%)]	Loss: 0.119678
Train Epoch: 1 [1434624/8742050 (16.4%)]	Loss: 0.118983
Train Epoch: 1 [1485824/8742050 (17.0%)]	Loss: 0.115753
Train Epoch: 1 [1537024/8742050 (17.6%)]	Loss: 0.111648
Train Epoch: 1 [1588224/8742050 (18.2%)]	Loss: 0.120078
Train Epoch: 1 [1639424/8742050 (18.8%)]	Loss: 0.118014
Train Epoch: 1 [1690624/8742050 (19.3%)]	Loss: 0.122185
Train Epoch: 1 [1741824/8742050 (19.9%)]	Loss: 0.117474
Train Epoch: 1 [1793024/8742050 (20.5%)]	Loss: 0.112235
Train Epoch: 1 [1844224/8742050 (21.1%)]	Loss: 0.110202
Train Epoch: 1 [1895424/8742050 (21.7%)]	Loss: 0.118213
Train Epoch: 1 [1946624/8742050 (22.3%)]	Loss: 0.116013
Train Epoch: 1 [1997824/8742050 (22.9%)]	Loss: 0.113855
Train Epoch: 1 [2049024/8742050 (23.4%)]	Loss: 0.110468
Train Epoch: 1 [2100224/8742050 (24.0%)]	Loss: 0.102291
Train Epoch: 1 [2151424/8742050 (24.6%)]	Loss: 0.113149
Train Epoch: 1 [2202624/8742050 (25.2%)]	Loss: 0.121339
Train Epoch: 1 [2253824/8742050 (25.8%)]	Loss: 0.110187
Train Epoch: 1 [2305024/8742050 (26.4%)]	Loss: 0.108130
Train Epoch: 1 [2356224/8742050 (27.0%)]	Loss: 0.114516
Train Epoch: 1 [2407424/8742050 (27.5%)]	Loss: 0.098359
Train Epoch: 1 [2458624/8742050 (28.1%)]	Loss: 0.114300
Train Epoch: 1 [2509824/8742050 (28.7%)]	Loss: 0.121070
Train Epoch: 1 [2561024/8742050 (29.3%)]	Loss: 0.101446
Train Epoch: 1 [2612224/8742050 (29.9%)]	Loss: 0.117382
Train Epoch: 1 [2663424/8742050 (30.5%)]	Loss: 0.105012
Train Epoch: 1 [2714624/8742050 (31.1%)]	Loss: 0.117280
Train Epoch: 1 [2765824/8742050 (31.6%)]	Loss: 0.100027
Train Epoch: 1 [2817024/8742050 (32.2%)]	Loss: 0.114622
Train Epoch: 1 [2868224/8742050 (32.8%)]	Loss: 0.110266
Train Epoch: 1 [2919424/8742050 (33.4%)]	Loss: 0.109365
Train Epoch: 1 [2970624/8742050 (34.0%)]	Loss: 0.100262
Train Epoch: 1 [3021824/8742050 (34.6%)]	Loss: 0.122797
Train Epoch: 1 [3073024/8742050 (35.2%)]	Loss: 0.116637
Train Epoch: 1 [3124224/8742050 (35.7%)]	Loss: 0.113825
Train Epoch: 1 [3175424/8742050 (36.3%)]	Loss: 0.116693
Train Epoch: 1 [3226624/8742050 (36.9%)]	Loss: 0.108782
Train Epoch: 1 [3277824/8742050 (37.5%)]	Loss: 0.103074
Train Epoch: 1 [3329024/8742050 (38.1%)]	Loss: 0.115666
Train Epoch: 1 [3380224/8742050 (38.7%)]	Loss: 0.111727
Train Epoch: 1 [3431424/8742050 (39.3%)]	Loss: 0.117346
Train Epoch: 1 [3482624/8742050 (39.8%)]	Loss: 0.107416
Train Epoch: 1 [3533824/8742050 (40.4%)]	Loss: 0.101797
Train Epoch: 1 [3585024/8742050 (41.0%)]	Loss: 0.119885
Train Epoch: 1 [3636224/8742050 (41.6%)]	Loss: 0.096378
Train Epoch: 1 [3687424/8742050 (42.2%)]	Loss: 0.104347
Train Epoch: 1 [3738624/8742050 (42.8%)]	Loss: 0.119487
Train Epoch: 1 [3789824/8742050 (43.4%)]	Loss: 0.100736
Train Epoch: 1 [3841024/8742050 (43.9%)]	Loss: 0.122680
Train Epoch: 1 [3892224/8742050 (44.5%)]	Loss: 0.109110
Train Epoch: 1 [3943424/8742050 (45.1%)]	Loss: 0.127240
Train Epoch: 1 [3994624/8742050 (45.7%)]	Loss: 0.107892
Train Epoch: 1 [4045824/8742050 (46.3%)]	Loss: 0.119541
Train Epoch: 1 [4097024/8742050 (46.9%)]	Loss: 0.099769
Train Epoch: 1 [4148224/8742050 (47.5%)]	Loss: 0.120644
Train Epoch: 1 [4199424/8742050 (48.0%)]	Loss: 0.115248
Train Epoch: 1 [4250624/8742050 (48.6%)]	Loss: 0.105252
Train Epoch: 1 [4301824/8742050 (49.2%)]	Loss: 0.104086
Train Epoch: 1 [4353024/8742050 (49.8%)]	Loss: 0.106018
Train Epoch: 1 [4404224/8742050 (50.4%)]	Loss: 0.114168
Train Epoch: 1 [4455424/8742050 (51.0%)]	Loss: 0.106722
Train Epoch: 1 [4506624/8742050 (51.6%)]	Loss: 0.124669
Train Epoch: 1 [4557824/8742050 (52.1%)]	Loss: 0.124350
Train Epoch: 1 [4609024/8742050 (52.7%)]	Loss: 0.098538
Train Epoch: 1 [4660224/8742050 (53.3%)]	Loss: 0.106751
Train Epoch: 1 [4711424/8742050 (53.9%)]	Loss: 0.104107
Train Epoch: 1 [4762624/8742050 (54.5%)]	Loss: 0.105555
Train Epoch: 1 [4813824/8742050 (55.1%)]	Loss: 0.108500
Train Epoch: 1 [4865024/8742050 (55.7%)]	Loss: 0.116458
Train Epoch: 1 [4916224/8742050 (56.2%)]	Loss: 0.113684
Train Epoch: 1 [4967424/8742050 (56.8%)]	Loss: 0.106663
Train Epoch: 1 [5018624/8742050 (57.4%)]	Loss: 0.117887
Train Epoch: 1 [5069824/8742050 (58.0%)]	Loss: 0.098655
Train Epoch: 1 [5121024/8742050 (58.6%)]	Loss: 0.115995
Train Epoch: 1 [5172224/8742050 (59.2%)]	Loss: 0.115154
Train Epoch: 1 [5223424/8742050 (59.8%)]	Loss: 0.111434
Train Epoch: 1 [5274624/8742050 (60.3%)]	Loss: 0.099504
Train Epoch: 1 [5325824/8742050 (60.9%)]	Loss: 0.107729
Train Epoch: 1 [5377024/8742050 (61.5%)]	Loss: 0.115707
Train Epoch: 1 [5428224/8742050 (62.1%)]	Loss: 0.108108
Train Epoch: 1 [5479424/8742050 (62.7%)]	Loss: 0.114058
Train Epoch: 1 [5530624/8742050 (63.3%)]	Loss: 0.102450
Train Epoch: 1 [5581824/8742050 (63.9%)]	Loss: 0.100039
Train Epoch: 1 [5633024/8742050 (64.4%)]	Loss: 0.115369
Train Epoch: 1 [5684224/8742050 (65.0%)]	Loss: 0.112684
Train Epoch: 1 [5735424/8742050 (65.6%)]	Loss: 0.096512
Train Epoch: 1 [5786624/8742050 (66.2%)]	Loss: 0.117860
Train Epoch: 1 [5837824/8742050 (66.8%)]	Loss: 0.105581
Train Epoch: 1 [5889024/8742050 (67.4%)]	Loss: 0.122552
Train Epoch: 1 [5940224/8742050 (68.0%)]	Loss: 0.103154
Train Epoch: 1 [5991424/8742050 (68.5%)]	Loss: 0.099285
Train Epoch: 1 [6042624/8742050 (69.1%)]	Loss: 0.111456
Train Epoch: 1 [6093824/8742050 (69.7%)]	Loss: 0.106201
Train Epoch: 1 [6145024/8742050 (70.3%)]	Loss: 0.108077
Train Epoch: 1 [6196224/8742050 (70.9%)]	Loss: 0.092204
Train Epoch: 1 [6247424/8742050 (71.5%)]	Loss: 0.106612
Train Epoch: 1 [6298624/8742050 (72.0%)]	Loss: 0.117292
Train Epoch: 1 [6349824/8742050 (72.6%)]	Loss: 0.092632
Train Epoch: 1 [6401024/8742050 (73.2%)]	Loss: 0.112122
Train Epoch: 1 [6452224/8742050 (73.8%)]	Loss: 0.101684
Train Epoch: 1 [6503424/8742050 (74.4%)]	Loss: 0.118169
Train Epoch: 1 [6554624/8742050 (75.0%)]	Loss: 0.127301
Train Epoch: 1 [6605824/8742050 (75.6%)]	Loss: 0.105417
Train Epoch: 1 [6657024/8742050 (76.1%)]	Loss: 0.128548
Train Epoch: 1 [6708224/8742050 (76.7%)]	Loss: 0.116279
Train Epoch: 1 [6759424/8742050 (77.3%)]	Loss: 0.108661
Train Epoch: 1 [6810624/8742050 (77.9%)]	Loss: 0.125477
Train Epoch: 1 [6861824/8742050 (78.5%)]	Loss: 0.125206
Train Epoch: 1 [6913024/8742050 (79.1%)]	Loss: 0.110057
Train Epoch: 1 [6964224/8742050 (79.7%)]	Loss: 0.111574
Train Epoch: 1 [7015424/8742050 (80.2%)]	Loss: 0.106970
Train Epoch: 1 [7066624/8742050 (80.8%)]	Loss: 0.100895
Train Epoch: 1 [7117824/8742050 (81.4%)]	Loss: 0.123441
Train Epoch: 1 [7169024/8742050 (82.0%)]	Loss: 0.098187
Train Epoch: 1 [7220224/8742050 (82.6%)]	Loss: 0.110214
Train Epoch: 1 [7271424/8742050 (83.2%)]	Loss: 0.110369
Train Epoch: 1 [7322624/8742050 (83.8%)]	Loss: 0.095340
Train Epoch: 1 [7373824/8742050 (84.3%)]	Loss: 0.097112
Train Epoch: 1 [7425024/8742050 (84.9%)]	Loss: 0.103380
Train Epoch: 1 [7476224/8742050 (85.5%)]	Loss: 0.110509
Train Epoch: 1 [7527424/8742050 (86.1%)]	Loss: 0.120597
Train Epoch: 1 [7578624/8742050 (86.7%)]	Loss: 0.109725
Train Epoch: 1 [7629824/8742050 (87.3%)]	Loss: 0.114570
Train Epoch: 1 [7681024/8742050 (87.9%)]	Loss: 0.099962
Train Epoch: 1 [7732224/8742050 (88.4%)]	Loss: 0.090088
Train Epoch: 1 [7783424/8742050 (89.0%)]	Loss: 0.107569
Train Epoch: 1 [7834624/8742050 (89.6%)]	Loss: 0.102796
Train Epoch: 1 [7885824/8742050 (90.2%)]	Loss: 0.109069
Train Epoch: 1 [7937024/8742050 (90.8%)]	Loss: 0.110943
Train Epoch: 1 [7988224/8742050 (91.4%)]	Loss: 0.106351
Train Epoch: 1 [8039424/8742050 (92.0%)]	Loss: 0.114818
Train Epoch: 1 [8090624/8742050 (92.5%)]	Loss: 0.097603
Train Epoch: 1 [8141824/8742050 (93.1%)]	Loss: 0.104157
Train Epoch: 1 [8193024/8742050 (93.7%)]	Loss: 0.110227
Train Epoch: 1 [8244224/8742050 (94.3%)]	Loss: 0.108444
Train Epoch: 1 [8295424/8742050 (94.9%)]	Loss: 0.124917
Train Epoch: 1 [8346624/8742050 (95.5%)]	Loss: 0.109800
Train Epoch: 1 [8397824/8742050 (96.1%)]	Loss: 0.107855
Train Epoch: 1 [8449024/8742050 (96.6%)]	Loss: 0.114025
Train Epoch: 1 [8500224/8742050 (97.2%)]	Loss: 0.117962
Train Epoch: 1 [8551424/8742050 (97.8%)]	Loss: 0.104694
Train Epoch: 1 [8602624/8742050 (98.4%)]	Loss: 0.108806
Train Epoch: 1 [8653824/8742050 (99.0%)]	Loss: 0.112473
Train Epoch: 1 [8705024/8742050 (99.6%)]	Loss: 0.106855
Train Epoch: 2 [1024/8742050 (0.0%)]	Loss: 0.104543
Train Epoch: 2 [52224/8742050 (0.6%)]	Loss: 0.104850
Train Epoch: 2 [103424/8742050 (1.2%)]	Loss: 0.089429
Train Epoch: 2 [154624/8742050 (1.8%)]	Loss: 0.110205
Train Epoch: 2 [205824/8742050 (2.4%)]	Loss: 0.115524
Train Epoch: 2 [257024/8742050 (2.9%)]	Loss: 0.119775
Train Epoch: 2 [308224/8742050 (3.5%)]	Loss: 0.111034
Train Epoch: 2 [359424/8742050 (4.1%)]	Loss: 0.106903
Train Epoch: 2 [410624/8742050 (4.7%)]	Loss: 0.101669
Train Epoch: 2 [461824/8742050 (5.3%)]	Loss: 0.102927
Train Epoch: 2 [513024/8742050 (5.9%)]	Loss: 0.115094
Train Epoch: 2 [564224/8742050 (6.5%)]	Loss: 0.116955
Train Epoch: 2 [615424/8742050 (7.0%)]	Loss: 0.102749
Train Epoch: 2 [666624/8742050 (7.6%)]	Loss: 0.111012
Train Epoch: 2 [717824/8742050 (8.2%)]	Loss: 0.094797
Train Epoch: 2 [769024/8742050 (8.8%)]	Loss: 0.111659
Train Epoch: 2 [820224/8742050 (9.4%)]	Loss: 0.105929
Train Epoch: 2 [871424/8742050 (10.0%)]	Loss: 0.106611
Train Epoch: 2 [922624/8742050 (10.6%)]	Loss: 0.107397
Train Epoch: 2 [973824/8742050 (11.1%)]	Loss: 0.109720
Train Epoch: 2 [1025024/8742050 (11.7%)]	Loss: 0.119919
Train Epoch: 2 [1076224/8742050 (12.3%)]	Loss: 0.109913
Train Epoch: 2 [1127424/8742050 (12.9%)]	Loss: 0.100269
Train Epoch: 2 [1178624/8742050 (13.5%)]	Loss: 0.102577
Train Epoch: 2 [1229824/8742050 (14.1%)]	Loss: 0.118230
Train Epoch: 2 [1281024/8742050 (14.7%)]	Loss: 0.112801
Train Epoch: 2 [1332224/8742050 (15.2%)]	Loss: 0.113151
Train Epoch: 2 [1383424/8742050 (15.8%)]	Loss: 0.095931
Train Epoch: 2 [1434624/8742050 (16.4%)]	Loss: 0.105087
Train Epoch: 2 [1485824/8742050 (17.0%)]	Loss: 0.122184
Train Epoch: 2 [1537024/8742050 (17.6%)]	Loss: 0.101478
Train Epoch: 2 [1588224/8742050 (18.2%)]	Loss: 0.112200
Train Epoch: 2 [1639424/8742050 (18.8%)]	Loss: 0.114383
Train Epoch: 2 [1690624/8742050 (19.3%)]	Loss: 0.096563
Train Epoch: 2 [1741824/8742050 (19.9%)]	Loss: 0.101861
Train Epoch: 2 [1793024/8742050 (20.5%)]	Loss: 0.119864
Train Epoch: 2 [1844224/8742050 (21.1%)]	Loss: 0.120521
Train Epoch: 2 [1895424/8742050 (21.7%)]	Loss: 0.117473
Train Epoch: 2 [1946624/8742050 (22.3%)]	Loss: 0.109364
Train Epoch: 2 [1997824/8742050 (22.9%)]	Loss: 0.103094
Train Epoch: 2 [2049024/8742050 (23.4%)]	Loss: 0.107939
Train Epoch: 2 [2100224/8742050 (24.0%)]	Loss: 0.106539
Train Epoch: 2 [2151424/8742050 (24.6%)]	Loss: 0.105051
Train Epoch: 2 [2202624/8742050 (25.2%)]	Loss: 0.110062
Train Epoch: 2 [2253824/8742050 (25.8%)]	Loss: 0.096527
Train Epoch: 2 [2305024/8742050 (26.4%)]	Loss: 0.109994
Train Epoch: 2 [2356224/8742050 (27.0%)]	Loss: 0.110309
Train Epoch: 2 [2407424/8742050 (27.5%)]	Loss: 0.115876
Train Epoch: 2 [2458624/8742050 (28.1%)]	Loss: 0.104660
Train Epoch: 2 [2509824/8742050 (28.7%)]	Loss: 0.095313
Train Epoch: 2 [2561024/8742050 (29.3%)]	Loss: 0.106457
Train Epoch: 2 [2612224/8742050 (29.9%)]	Loss: 0.097681
Train Epoch: 2 [2663424/8742050 (30.5%)]	Loss: 0.112249
Train Epoch: 2 [2714624/8742050 (31.1%)]	Loss: 0.106358
Train Epoch: 2 [2765824/8742050 (31.6%)]	Loss: 0.114250
Train Epoch: 2 [2817024/8742050 (32.2%)]	Loss: 0.101247
Train Epoch: 2 [2868224/8742050 (32.8%)]	Loss: 0.098106
Train Epoch: 2 [2919424/8742050 (33.4%)]	Loss: 0.112776
Train Epoch: 2 [2970624/8742050 (34.0%)]	Loss: 0.112927
Train Epoch: 2 [3021824/8742050 (34.6%)]	Loss: 0.101658
Train Epoch: 2 [3073024/8742050 (35.2%)]	Loss: 0.101482
Train Epoch: 2 [3124224/8742050 (35.7%)]	Loss: 0.105653
Train Epoch: 2 [3175424/8742050 (36.3%)]	Loss: 0.113574
Train Epoch: 2 [3226624/8742050 (36.9%)]	Loss: 0.103282
Train Epoch: 2 [3277824/8742050 (37.5%)]	Loss: 0.104554
Train Epoch: 2 [3329024/8742050 (38.1%)]	Loss: 0.114385
Train Epoch: 2 [3380224/8742050 (38.7%)]	Loss: 0.098669
Train Epoch: 2 [3431424/8742050 (39.3%)]	Loss: 0.111395
Train Epoch: 2 [3482624/8742050 (39.8%)]	Loss: 0.104470
Train Epoch: 2 [3533824/8742050 (40.4%)]	Loss: 0.100380
Train Epoch: 2 [3585024/8742050 (41.0%)]	Loss: 0.097035
Train Epoch: 2 [3636224/8742050 (41.6%)]	Loss: 0.094007
Train Epoch: 2 [3687424/8742050 (42.2%)]	Loss: 0.099097
Train Epoch: 2 [3738624/8742050 (42.8%)]	Loss: 0.108395
Train Epoch: 2 [3789824/8742050 (43.4%)]	Loss: 0.095728
Train Epoch: 2 [3841024/8742050 (43.9%)]	Loss: 0.105927
Train Epoch: 2 [3892224/8742050 (44.5%)]	Loss: 0.107411
Train Epoch: 2 [3943424/8742050 (45.1%)]	Loss: 0.104379
Train Epoch: 2 [3994624/8742050 (45.7%)]	Loss: 0.114549
Train Epoch: 2 [4045824/8742050 (46.3%)]	Loss: 0.107841
Train Epoch: 2 [4097024/8742050 (46.9%)]	Loss: 0.109557
Train Epoch: 2 [4148224/8742050 (47.5%)]	Loss: 0.111355
Train Epoch: 2 [4199424/8742050 (48.0%)]	Loss: 0.113938
Train Epoch: 2 [4250624/8742050 (48.6%)]	Loss: 0.098790
Train Epoch: 2 [4301824/8742050 (49.2%)]	Loss: 0.104538
Train Epoch: 2 [4353024/8742050 (49.8%)]	Loss: 0.110715
Train Epoch: 2 [4404224/8742050 (50.4%)]	Loss: 0.116464
Train Epoch: 2 [4455424/8742050 (51.0%)]	Loss: 0.119852
Train Epoch: 2 [4506624/8742050 (51.6%)]	Loss: 0.105225
Train Epoch: 2 [4557824/8742050 (52.1%)]	Loss: 0.093444
Train Epoch: 2 [4609024/8742050 (52.7%)]	Loss: 0.116932
Train Epoch: 2 [4660224/8742050 (53.3%)]	Loss: 0.116653
Train Epoch: 2 [4711424/8742050 (53.9%)]	Loss: 0.116629
Train Epoch: 2 [4762624/8742050 (54.5%)]	Loss: 0.102996
Train Epoch: 2 [4813824/8742050 (55.1%)]	Loss: 0.103066
Train Epoch: 2 [4865024/8742050 (55.7%)]	Loss: 0.103414
Train Epoch: 2 [4916224/8742050 (56.2%)]	Loss: 0.104912
Train Epoch: 2 [4967424/8742050 (56.8%)]	Loss: 0.098864
Train Epoch: 2 [5018624/8742050 (57.4%)]	Loss: 0.098885
Train Epoch: 2 [5069824/8742050 (58.0%)]	Loss: 0.094755
Train Epoch: 2 [5121024/8742050 (58.6%)]	Loss: 0.101135
Train Epoch: 2 [5172224/8742050 (59.2%)]	Loss: 0.107981
Train Epoch: 2 [5223424/8742050 (59.8%)]	Loss: 0.101294
Train Epoch: 2 [5274624/8742050 (60.3%)]	Loss: 0.103263
Train Epoch: 2 [5325824/8742050 (60.9%)]	Loss: 0.110327
Train Epoch: 2 [5377024/8742050 (61.5%)]	Loss: 0.108498
Train Epoch: 2 [5428224/8742050 (62.1%)]	Loss: 0.108338
Train Epoch: 2 [5479424/8742050 (62.7%)]	Loss: 0.104854
Train Epoch: 2 [5530624/8742050 (63.3%)]	Loss: 0.118423
Train Epoch: 2 [5581824/8742050 (63.9%)]	Loss: 0.108794
Train Epoch: 2 [5633024/8742050 (64.4%)]	Loss: 0.099798
Train Epoch: 2 [5684224/8742050 (65.0%)]	Loss: 0.107201
Train Epoch: 2 [5735424/8742050 (65.6%)]	Loss: 0.120012
Train Epoch: 2 [5786624/8742050 (66.2%)]	Loss: 0.114607
Train Epoch: 2 [5837824/8742050 (66.8%)]	Loss: 0.096650
Train Epoch: 2 [5889024/8742050 (67.4%)]	Loss: 0.116240
Train Epoch: 2 [5940224/8742050 (68.0%)]	Loss: 0.110016
Train Epoch: 2 [5991424/8742050 (68.5%)]	Loss: 0.094003
Train Epoch: 2 [6042624/8742050 (69.1%)]	Loss: 0.098829
Train Epoch: 2 [6093824/8742050 (69.7%)]	Loss: 0.106122
Train Epoch: 2 [6145024/8742050 (70.3%)]	Loss: 0.128217
Train Epoch: 2 [6196224/8742050 (70.9%)]	Loss: 0.101769
Train Epoch: 2 [6247424/8742050 (71.5%)]	Loss: 0.107396
Train Epoch: 2 [6298624/8742050 (72.0%)]	Loss: 0.090685
Train Epoch: 2 [6349824/8742050 (72.6%)]	Loss: 0.111617
Train Epoch: 2 [6401024/8742050 (73.2%)]	Loss: 0.108147
Train Epoch: 2 [6452224/8742050 (73.8%)]	Loss: 0.112383
Train Epoch: 2 [6503424/8742050 (74.4%)]	Loss: 0.113682
Train Epoch: 2 [6554624/8742050 (75.0%)]	Loss: 0.113752
Train Epoch: 2 [6605824/8742050 (75.6%)]	Loss: 0.092935
Train Epoch: 2 [6657024/8742050 (76.1%)]	Loss: 0.108179
Train Epoch: 2 [6708224/8742050 (76.7%)]	Loss: 0.096997
Train Epoch: 2 [6759424/8742050 (77.3%)]	Loss: 0.110664
Train Epoch: 2 [6810624/8742050 (77.9%)]	Loss: 0.104227
Train Epoch: 2 [6861824/8742050 (78.5%)]	Loss: 0.104446
Train Epoch: 2 [6913024/8742050 (79.1%)]	Loss: 0.113001
Train Epoch: 2 [6964224/8742050 (79.7%)]	Loss: 0.105153
Train Epoch: 2 [7015424/8742050 (80.2%)]	Loss: 0.096765
Train Epoch: 2 [7066624/8742050 (80.8%)]	Loss: 0.108172
Train Epoch: 2 [7117824/8742050 (81.4%)]	Loss: 0.096911
Train Epoch: 2 [7169024/8742050 (82.0%)]	Loss: 0.109855
Train Epoch: 2 [7220224/8742050 (82.6%)]	Loss: 0.101741
Train Epoch: 2 [7271424/8742050 (83.2%)]	Loss: 0.111521
Train Epoch: 2 [7322624/8742050 (83.8%)]	Loss: 0.103438
Train Epoch: 2 [7373824/8742050 (84.3%)]	Loss: 0.100214
Train Epoch: 2 [7425024/8742050 (84.9%)]	Loss: 0.105493
Train Epoch: 2 [7476224/8742050 (85.5%)]	Loss: 0.102230
Train Epoch: 2 [7527424/8742050 (86.1%)]	Loss: 0.097928
Train Epoch: 2 [7578624/8742050 (86.7%)]	Loss: 0.103676
Train Epoch: 2 [7629824/8742050 (87.3%)]	Loss: 0.117381
Train Epoch: 2 [7681024/8742050 (87.9%)]	Loss: 0.099696
Train Epoch: 2 [7732224/8742050 (88.4%)]	Loss: 0.108853
Train Epoch: 2 [7783424/8742050 (89.0%)]	Loss: 0.100073
Train Epoch: 2 [7834624/8742050 (89.6%)]	Loss: 0.098729
Train Epoch: 2 [7885824/8742050 (90.2%)]	Loss: 0.129657
Train Epoch: 2 [7937024/8742050 (90.8%)]	Loss: 0.119696
Train Epoch: 2 [7988224/8742050 (91.4%)]	Loss: 0.121008
Train Epoch: 2 [8039424/8742050 (92.0%)]	Loss: 0.099233
Train Epoch: 2 [8090624/8742050 (92.5%)]	Loss: 0.113365
Train Epoch: 2 [8141824/8742050 (93.1%)]	Loss: 0.108578
Train Epoch: 2 [8193024/8742050 (93.7%)]	Loss: 0.108730
Train Epoch: 2 [8244224/8742050 (94.3%)]	Loss: 0.109455
Train Epoch: 2 [8295424/8742050 (94.9%)]	Loss: 0.090398
Train Epoch: 2 [8346624/8742050 (95.5%)]	Loss: 0.108526
Train Epoch: 2 [8397824/8742050 (96.1%)]	Loss: 0.118333
Train Epoch: 2 [8449024/8742050 (96.6%)]	Loss: 0.125529
Train Epoch: 2 [8500224/8742050 (97.2%)]	Loss: 0.101869
Train Epoch: 2 [8551424/8742050 (97.8%)]	Loss: 0.098043
Train Epoch: 2 [8602624/8742050 (98.4%)]	Loss: 0.103012
Train Epoch: 2 [8653824/8742050 (99.0%)]	Loss: 0.125802
Train Epoch: 2 [8705024/8742050 (99.6%)]	Loss: 0.106741

ACC in fold#2 was 0.882


Balanced ACC in fold#2 was 0.807


MCC in fold#2 was 0.716


Confusion Matrix in fold#2: 
           nonRipple   Ripple
nonRipple     405149   247430
Ripple         10538  1522396


Classification Report in fold#2: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.975        0.860  ...        0.917         0.894
recall            0.621        0.993  ...        0.807         0.882
f1-score          0.759        0.922  ...        0.840         0.873
sample size  652579.000  1532934.000  ...  2185513.000   2185513.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/8742051 (0.0%)]	Loss: 0.379132
Train Epoch: 1 [52224/8742051 (0.6%)]	Loss: 0.153998
Train Epoch: 1 [103424/8742051 (1.2%)]	Loss: 0.139365
Train Epoch: 1 [154624/8742051 (1.8%)]	Loss: 0.108295
Train Epoch: 1 [205824/8742051 (2.4%)]	Loss: 0.112076
Train Epoch: 1 [257024/8742051 (2.9%)]	Loss: 0.130152
Train Epoch: 1 [308224/8742051 (3.5%)]	Loss: 0.125292
Train Epoch: 1 [359424/8742051 (4.1%)]	Loss: 0.112130
Train Epoch: 1 [410624/8742051 (4.7%)]	Loss: 0.097652
Train Epoch: 1 [461824/8742051 (5.3%)]	Loss: 0.118098
Train Epoch: 1 [513024/8742051 (5.9%)]	Loss: 0.116342
Train Epoch: 1 [564224/8742051 (6.5%)]	Loss: 0.131449
Train Epoch: 1 [615424/8742051 (7.0%)]	Loss: 0.121501
Train Epoch: 1 [666624/8742051 (7.6%)]	Loss: 0.107377
Train Epoch: 1 [717824/8742051 (8.2%)]	Loss: 0.122858
Train Epoch: 1 [769024/8742051 (8.8%)]	Loss: 0.117858
Train Epoch: 1 [820224/8742051 (9.4%)]	Loss: 0.122890
Train Epoch: 1 [871424/8742051 (10.0%)]	Loss: 0.136976
Train Epoch: 1 [922624/8742051 (10.6%)]	Loss: 0.113902
Train Epoch: 1 [973824/8742051 (11.1%)]	Loss: 0.100271
Train Epoch: 1 [1025024/8742051 (11.7%)]	Loss: 0.140915
Train Epoch: 1 [1076224/8742051 (12.3%)]	Loss: 0.114746
Train Epoch: 1 [1127424/8742051 (12.9%)]	Loss: 0.116925
Train Epoch: 1 [1178624/8742051 (13.5%)]	Loss: 0.113924
Train Epoch: 1 [1229824/8742051 (14.1%)]	Loss: 0.124401
Train Epoch: 1 [1281024/8742051 (14.7%)]	Loss: 0.119491
Train Epoch: 1 [1332224/8742051 (15.2%)]	Loss: 0.117903
Train Epoch: 1 [1383424/8742051 (15.8%)]	Loss: 0.108995
Train Epoch: 1 [1434624/8742051 (16.4%)]	Loss: 0.119429
Train Epoch: 1 [1485824/8742051 (17.0%)]	Loss: 0.112245
Train Epoch: 1 [1537024/8742051 (17.6%)]	Loss: 0.111915
Train Epoch: 1 [1588224/8742051 (18.2%)]	Loss: 0.114870
Train Epoch: 1 [1639424/8742051 (18.8%)]	Loss: 0.119666
Train Epoch: 1 [1690624/8742051 (19.3%)]	Loss: 0.112690
Train Epoch: 1 [1741824/8742051 (19.9%)]	Loss: 0.113530
Train Epoch: 1 [1793024/8742051 (20.5%)]	Loss: 0.106389
Train Epoch: 1 [1844224/8742051 (21.1%)]	Loss: 0.117829
Train Epoch: 1 [1895424/8742051 (21.7%)]	Loss: 0.116791
Train Epoch: 1 [1946624/8742051 (22.3%)]	Loss: 0.119657
Train Epoch: 1 [1997824/8742051 (22.9%)]	Loss: 0.116700
Train Epoch: 1 [2049024/8742051 (23.4%)]	Loss: 0.120697
Train Epoch: 1 [2100224/8742051 (24.0%)]	Loss: 0.110131
Train Epoch: 1 [2151424/8742051 (24.6%)]	Loss: 0.117788
Train Epoch: 1 [2202624/8742051 (25.2%)]	Loss: 0.114529
Train Epoch: 1 [2253824/8742051 (25.8%)]	Loss: 0.114320
Train Epoch: 1 [2305024/8742051 (26.4%)]	Loss: 0.126250
Train Epoch: 1 [2356224/8742051 (27.0%)]	Loss: 0.111563
Train Epoch: 1 [2407424/8742051 (27.5%)]	Loss: 0.107958
Train Epoch: 1 [2458624/8742051 (28.1%)]	Loss: 0.115349
Train Epoch: 1 [2509824/8742051 (28.7%)]	Loss: 0.114874
Train Epoch: 1 [2561024/8742051 (29.3%)]	Loss: 0.102966
Train Epoch: 1 [2612224/8742051 (29.9%)]	Loss: 0.110965
Train Epoch: 1 [2663424/8742051 (30.5%)]	Loss: 0.116691
Train Epoch: 1 [2714624/8742051 (31.1%)]	Loss: 0.110825
Train Epoch: 1 [2765824/8742051 (31.6%)]	Loss: 0.120439
Train Epoch: 1 [2817024/8742051 (32.2%)]	Loss: 0.108909
Train Epoch: 1 [2868224/8742051 (32.8%)]	Loss: 0.100667
Train Epoch: 1 [2919424/8742051 (33.4%)]	Loss: 0.120261
Train Epoch: 1 [2970624/8742051 (34.0%)]	Loss: 0.121731
Train Epoch: 1 [3021824/8742051 (34.6%)]	Loss: 0.112696
Train Epoch: 1 [3073024/8742051 (35.2%)]	Loss: 0.098216
Train Epoch: 1 [3124224/8742051 (35.7%)]	Loss: 0.112725
Train Epoch: 1 [3175424/8742051 (36.3%)]	Loss: 0.118081
Train Epoch: 1 [3226624/8742051 (36.9%)]	Loss: 0.107133
Train Epoch: 1 [3277824/8742051 (37.5%)]	Loss: 0.107584
Train Epoch: 1 [3329024/8742051 (38.1%)]	Loss: 0.119973
Train Epoch: 1 [3380224/8742051 (38.7%)]	Loss: 0.101037
Train Epoch: 1 [3431424/8742051 (39.3%)]	Loss: 0.113888
Train Epoch: 1 [3482624/8742051 (39.8%)]	Loss: 0.108587
Train Epoch: 1 [3533824/8742051 (40.4%)]	Loss: 0.105402
Train Epoch: 1 [3585024/8742051 (41.0%)]	Loss: 0.108178
Train Epoch: 1 [3636224/8742051 (41.6%)]	Loss: 0.101294
Train Epoch: 1 [3687424/8742051 (42.2%)]	Loss: 0.109327
Train Epoch: 1 [3738624/8742051 (42.8%)]	Loss: 0.108968
Train Epoch: 1 [3789824/8742051 (43.4%)]	Loss: 0.105968
Train Epoch: 1 [3841024/8742051 (43.9%)]	Loss: 0.117838
Train Epoch: 1 [3892224/8742051 (44.5%)]	Loss: 0.107136
Train Epoch: 1 [3943424/8742051 (45.1%)]	Loss: 0.112725
Train Epoch: 1 [3994624/8742051 (45.7%)]	Loss: 0.099153
Train Epoch: 1 [4045824/8742051 (46.3%)]	Loss: 0.098056
Train Epoch: 1 [4097024/8742051 (46.9%)]	Loss: 0.092984
Train Epoch: 1 [4148224/8742051 (47.5%)]	Loss: 0.117876
Train Epoch: 1 [4199424/8742051 (48.0%)]	Loss: 0.104659
Train Epoch: 1 [4250624/8742051 (48.6%)]	Loss: 0.115161
Train Epoch: 1 [4301824/8742051 (49.2%)]	Loss: 0.097855
Train Epoch: 1 [4353024/8742051 (49.8%)]	Loss: 0.099638
Train Epoch: 1 [4404224/8742051 (50.4%)]	Loss: 0.099184
Train Epoch: 1 [4455424/8742051 (51.0%)]	Loss: 0.115188
Train Epoch: 1 [4506624/8742051 (51.6%)]	Loss: 0.116354
Train Epoch: 1 [4557824/8742051 (52.1%)]	Loss: 0.106445
Train Epoch: 1 [4609024/8742051 (52.7%)]	Loss: 0.116284
Train Epoch: 1 [4660224/8742051 (53.3%)]	Loss: 0.111916
Train Epoch: 1 [4711424/8742051 (53.9%)]	Loss: 0.101937
Train Epoch: 1 [4762624/8742051 (54.5%)]	Loss: 0.115573
Train Epoch: 1 [4813824/8742051 (55.1%)]	Loss: 0.110346
Train Epoch: 1 [4865024/8742051 (55.7%)]	Loss: 0.109733
Train Epoch: 1 [4916224/8742051 (56.2%)]	Loss: 0.100744
Train Epoch: 1 [4967424/8742051 (56.8%)]	Loss: 0.107135
Train Epoch: 1 [5018624/8742051 (57.4%)]	Loss: 0.102653
Train Epoch: 1 [5069824/8742051 (58.0%)]	Loss: 0.113886
Train Epoch: 1 [5121024/8742051 (58.6%)]	Loss: 0.112558
Train Epoch: 1 [5172224/8742051 (59.2%)]	Loss: 0.111642
Train Epoch: 1 [5223424/8742051 (59.8%)]	Loss: 0.106805
Train Epoch: 1 [5274624/8742051 (60.3%)]	Loss: 0.112785
Train Epoch: 1 [5325824/8742051 (60.9%)]	Loss: 0.128545
Train Epoch: 1 [5377024/8742051 (61.5%)]	Loss: 0.086465
Train Epoch: 1 [5428224/8742051 (62.1%)]	Loss: 0.123357
Train Epoch: 1 [5479424/8742051 (62.7%)]	Loss: 0.102080
Train Epoch: 1 [5530624/8742051 (63.3%)]	Loss: 0.112202
Train Epoch: 1 [5581824/8742051 (63.9%)]	Loss: 0.109009
Train Epoch: 1 [5633024/8742051 (64.4%)]	Loss: 0.129357
Train Epoch: 1 [5684224/8742051 (65.0%)]	Loss: 0.100175
Train Epoch: 1 [5735424/8742051 (65.6%)]	Loss: 0.095521
Train Epoch: 1 [5786624/8742051 (66.2%)]	Loss: 0.099842
Train Epoch: 1 [5837824/8742051 (66.8%)]	Loss: 0.101736
Train Epoch: 1 [5889024/8742051 (67.4%)]	Loss: 0.118603
Train Epoch: 1 [5940224/8742051 (68.0%)]	Loss: 0.104487
Train Epoch: 1 [5991424/8742051 (68.5%)]	Loss: 0.098625
Train Epoch: 1 [6042624/8742051 (69.1%)]	Loss: 0.099176
Train Epoch: 1 [6093824/8742051 (69.7%)]	Loss: 0.107776
Train Epoch: 1 [6145024/8742051 (70.3%)]	Loss: 0.109679
Train Epoch: 1 [6196224/8742051 (70.9%)]	Loss: 0.119773
Train Epoch: 1 [6247424/8742051 (71.5%)]	Loss: 0.111605
Train Epoch: 1 [6298624/8742051 (72.0%)]	Loss: 0.117033
Train Epoch: 1 [6349824/8742051 (72.6%)]	Loss: 0.092617
Train Epoch: 1 [6401024/8742051 (73.2%)]	Loss: 0.119841
Train Epoch: 1 [6452224/8742051 (73.8%)]	Loss: 0.124179
Train Epoch: 1 [6503424/8742051 (74.4%)]	Loss: 0.103362
Train Epoch: 1 [6554624/8742051 (75.0%)]	Loss: 0.112810
Train Epoch: 1 [6605824/8742051 (75.6%)]	Loss: 0.102682
Train Epoch: 1 [6657024/8742051 (76.1%)]	Loss: 0.135031
Train Epoch: 1 [6708224/8742051 (76.7%)]	Loss: 0.096914
Train Epoch: 1 [6759424/8742051 (77.3%)]	Loss: 0.099798
Train Epoch: 1 [6810624/8742051 (77.9%)]	Loss: 0.112509
Train Epoch: 1 [6861824/8742051 (78.5%)]	Loss: 0.092066
Train Epoch: 1 [6913024/8742051 (79.1%)]	Loss: 0.110982
Train Epoch: 1 [6964224/8742051 (79.7%)]	Loss: 0.110890
Train Epoch: 1 [7015424/8742051 (80.2%)]	Loss: 0.116998
Train Epoch: 1 [7066624/8742051 (80.8%)]	Loss: 0.104365
Train Epoch: 1 [7117824/8742051 (81.4%)]	Loss: 0.116033
Train Epoch: 1 [7169024/8742051 (82.0%)]	Loss: 0.115294
Train Epoch: 1 [7220224/8742051 (82.6%)]	Loss: 0.109710
Train Epoch: 1 [7271424/8742051 (83.2%)]	Loss: 0.114915
Train Epoch: 1 [7322624/8742051 (83.8%)]	Loss: 0.106551
Train Epoch: 1 [7373824/8742051 (84.3%)]	Loss: 0.091234
Train Epoch: 1 [7425024/8742051 (84.9%)]	Loss: 0.122121
Train Epoch: 1 [7476224/8742051 (85.5%)]	Loss: 0.126884
Train Epoch: 1 [7527424/8742051 (86.1%)]	Loss: 0.103816
Train Epoch: 1 [7578624/8742051 (86.7%)]	Loss: 0.101112
Train Epoch: 1 [7629824/8742051 (87.3%)]	Loss: 0.102960
Train Epoch: 1 [7681024/8742051 (87.9%)]	Loss: 0.101863
Train Epoch: 1 [7732224/8742051 (88.4%)]	Loss: 0.120495
Train Epoch: 1 [7783424/8742051 (89.0%)]	Loss: 0.123320
Train Epoch: 1 [7834624/8742051 (89.6%)]	Loss: 0.108742
Train Epoch: 1 [7885824/8742051 (90.2%)]	Loss: 0.113741
Train Epoch: 1 [7937024/8742051 (90.8%)]	Loss: 0.118009
Train Epoch: 1 [7988224/8742051 (91.4%)]	Loss: 0.104427
Train Epoch: 1 [8039424/8742051 (92.0%)]	Loss: 0.109930
Train Epoch: 1 [8090624/8742051 (92.5%)]	Loss: 0.115773
Train Epoch: 1 [8141824/8742051 (93.1%)]	Loss: 0.102185
Train Epoch: 1 [8193024/8742051 (93.7%)]	Loss: 0.098412
Train Epoch: 1 [8244224/8742051 (94.3%)]	Loss: 0.103325
Train Epoch: 1 [8295424/8742051 (94.9%)]	Loss: 0.096745
Train Epoch: 1 [8346624/8742051 (95.5%)]	Loss: 0.100742
Train Epoch: 1 [8397824/8742051 (96.1%)]	Loss: 0.098727
Train Epoch: 1 [8449024/8742051 (96.6%)]	Loss: 0.120579
Train Epoch: 1 [8500224/8742051 (97.2%)]	Loss: 0.116124
Train Epoch: 1 [8551424/8742051 (97.8%)]	Loss: 0.105667
Train Epoch: 1 [8602624/8742051 (98.4%)]	Loss: 0.096088
Train Epoch: 1 [8653824/8742051 (99.0%)]	Loss: 0.106696
Train Epoch: 1 [8705024/8742051 (99.6%)]	Loss: 0.128898
Train Epoch: 2 [1024/8742051 (0.0%)]	Loss: 0.101045
Train Epoch: 2 [52224/8742051 (0.6%)]	Loss: 0.120161
Train Epoch: 2 [103424/8742051 (1.2%)]	Loss: 0.100548
Train Epoch: 2 [154624/8742051 (1.8%)]	Loss: 0.111679
Train Epoch: 2 [205824/8742051 (2.4%)]	Loss: 0.098413
Train Epoch: 2 [257024/8742051 (2.9%)]	Loss: 0.096284
Train Epoch: 2 [308224/8742051 (3.5%)]	Loss: 0.106907
Train Epoch: 2 [359424/8742051 (4.1%)]	Loss: 0.099391
Train Epoch: 2 [410624/8742051 (4.7%)]	Loss: 0.115232
Train Epoch: 2 [461824/8742051 (5.3%)]	Loss: 0.114497
Train Epoch: 2 [513024/8742051 (5.9%)]	Loss: 0.128326
Train Epoch: 2 [564224/8742051 (6.5%)]	Loss: 0.113810
Train Epoch: 2 [615424/8742051 (7.0%)]	Loss: 0.111465
Train Epoch: 2 [666624/8742051 (7.6%)]	Loss: 0.104737
Train Epoch: 2 [717824/8742051 (8.2%)]	Loss: 0.115846
Train Epoch: 2 [769024/8742051 (8.8%)]	Loss: 0.115476
Train Epoch: 2 [820224/8742051 (9.4%)]	Loss: 0.105558
Train Epoch: 2 [871424/8742051 (10.0%)]	Loss: 0.109938
Train Epoch: 2 [922624/8742051 (10.6%)]	Loss: 0.086038
Train Epoch: 2 [973824/8742051 (11.1%)]	Loss: 0.095716
Train Epoch: 2 [1025024/8742051 (11.7%)]	Loss: 0.104381
Train Epoch: 2 [1076224/8742051 (12.3%)]	Loss: 0.103623
Train Epoch: 2 [1127424/8742051 (12.9%)]	Loss: 0.105634
Train Epoch: 2 [1178624/8742051 (13.5%)]	Loss: 0.102976
Train Epoch: 2 [1229824/8742051 (14.1%)]	Loss: 0.105752
Train Epoch: 2 [1281024/8742051 (14.7%)]	Loss: 0.109881
Train Epoch: 2 [1332224/8742051 (15.2%)]	Loss: 0.112275
Train Epoch: 2 [1383424/8742051 (15.8%)]	Loss: 0.110025
Train Epoch: 2 [1434624/8742051 (16.4%)]	Loss: 0.098032
Train Epoch: 2 [1485824/8742051 (17.0%)]	Loss: 0.115301
Train Epoch: 2 [1537024/8742051 (17.6%)]	Loss: 0.121065
Train Epoch: 2 [1588224/8742051 (18.2%)]	Loss: 0.101942
Train Epoch: 2 [1639424/8742051 (18.8%)]	Loss: 0.101533
Train Epoch: 2 [1690624/8742051 (19.3%)]	Loss: 0.097402
Train Epoch: 2 [1741824/8742051 (19.9%)]	Loss: 0.094424
Train Epoch: 2 [1793024/8742051 (20.5%)]	Loss: 0.109870
Train Epoch: 2 [1844224/8742051 (21.1%)]	Loss: 0.120010
Train Epoch: 2 [1895424/8742051 (21.7%)]	Loss: 0.093946
Train Epoch: 2 [1946624/8742051 (22.3%)]	Loss: 0.112068
Train Epoch: 2 [1997824/8742051 (22.9%)]	Loss: 0.108639
Train Epoch: 2 [2049024/8742051 (23.4%)]	Loss: 0.108812
Train Epoch: 2 [2100224/8742051 (24.0%)]	Loss: 0.108864
Train Epoch: 2 [2151424/8742051 (24.6%)]	Loss: 0.110652
Train Epoch: 2 [2202624/8742051 (25.2%)]	Loss: 0.103916
Train Epoch: 2 [2253824/8742051 (25.8%)]	Loss: 0.100587
Train Epoch: 2 [2305024/8742051 (26.4%)]	Loss: 0.103288
Train Epoch: 2 [2356224/8742051 (27.0%)]	Loss: 0.105625
Train Epoch: 2 [2407424/8742051 (27.5%)]	Loss: 0.107592
Train Epoch: 2 [2458624/8742051 (28.1%)]	Loss: 0.101997
Train Epoch: 2 [2509824/8742051 (28.7%)]	Loss: 0.102742
Train Epoch: 2 [2561024/8742051 (29.3%)]	Loss: 0.100502
Train Epoch: 2 [2612224/8742051 (29.9%)]	Loss: 0.096765
Train Epoch: 2 [2663424/8742051 (30.5%)]	Loss: 0.105844
Train Epoch: 2 [2714624/8742051 (31.1%)]	Loss: 0.104967
Train Epoch: 2 [2765824/8742051 (31.6%)]	Loss: 0.102253
Train Epoch: 2 [2817024/8742051 (32.2%)]	Loss: 0.122219
Train Epoch: 2 [2868224/8742051 (32.8%)]	Loss: 0.114476
Train Epoch: 2 [2919424/8742051 (33.4%)]	Loss: 0.115304
Train Epoch: 2 [2970624/8742051 (34.0%)]	Loss: 0.097095
Train Epoch: 2 [3021824/8742051 (34.6%)]	Loss: 0.105330
Train Epoch: 2 [3073024/8742051 (35.2%)]	Loss: 0.110920
Train Epoch: 2 [3124224/8742051 (35.7%)]	Loss: 0.111954
Train Epoch: 2 [3175424/8742051 (36.3%)]	Loss: 0.097173
Train Epoch: 2 [3226624/8742051 (36.9%)]	Loss: 0.107525
Train Epoch: 2 [3277824/8742051 (37.5%)]	Loss: 0.107383
Train Epoch: 2 [3329024/8742051 (38.1%)]	Loss: 0.120762
Train Epoch: 2 [3380224/8742051 (38.7%)]	Loss: 0.111717
Train Epoch: 2 [3431424/8742051 (39.3%)]	Loss: 0.102770
Train Epoch: 2 [3482624/8742051 (39.8%)]	Loss: 0.099313
Train Epoch: 2 [3533824/8742051 (40.4%)]	Loss: 0.110123
Train Epoch: 2 [3585024/8742051 (41.0%)]	Loss: 0.113658
Train Epoch: 2 [3636224/8742051 (41.6%)]	Loss: 0.103705
Train Epoch: 2 [3687424/8742051 (42.2%)]	Loss: 0.119577
Train Epoch: 2 [3738624/8742051 (42.8%)]	Loss: 0.109087
Train Epoch: 2 [3789824/8742051 (43.4%)]	Loss: 0.132240
Train Epoch: 2 [3841024/8742051 (43.9%)]	Loss: 0.115737
Train Epoch: 2 [3892224/8742051 (44.5%)]	Loss: 0.110871
Train Epoch: 2 [3943424/8742051 (45.1%)]	Loss: 0.101390
Train Epoch: 2 [3994624/8742051 (45.7%)]	Loss: 0.114360
Train Epoch: 2 [4045824/8742051 (46.3%)]	Loss: 0.094208
Train Epoch: 2 [4097024/8742051 (46.9%)]	Loss: 0.096370
Train Epoch: 2 [4148224/8742051 (47.5%)]	Loss: 0.111630
Train Epoch: 2 [4199424/8742051 (48.0%)]	Loss: 0.103754
Train Epoch: 2 [4250624/8742051 (48.6%)]	Loss: 0.107764
Train Epoch: 2 [4301824/8742051 (49.2%)]	Loss: 0.101523
Train Epoch: 2 [4353024/8742051 (49.8%)]	Loss: 0.103170
Train Epoch: 2 [4404224/8742051 (50.4%)]	Loss: 0.102645
Train Epoch: 2 [4455424/8742051 (51.0%)]	Loss: 0.109066
Train Epoch: 2 [4506624/8742051 (51.6%)]	Loss: 0.114668
Train Epoch: 2 [4557824/8742051 (52.1%)]	Loss: 0.118560
Train Epoch: 2 [4609024/8742051 (52.7%)]	Loss: 0.105370
Train Epoch: 2 [4660224/8742051 (53.3%)]	Loss: 0.127789
Train Epoch: 2 [4711424/8742051 (53.9%)]	Loss: 0.104084
Train Epoch: 2 [4762624/8742051 (54.5%)]	Loss: 0.102793
Train Epoch: 2 [4813824/8742051 (55.1%)]	Loss: 0.104770
Train Epoch: 2 [4865024/8742051 (55.7%)]	Loss: 0.098596
Train Epoch: 2 [4916224/8742051 (56.2%)]	Loss: 0.101219
Train Epoch: 2 [4967424/8742051 (56.8%)]	Loss: 0.105430
Train Epoch: 2 [5018624/8742051 (57.4%)]	Loss: 0.095279
Train Epoch: 2 [5069824/8742051 (58.0%)]	Loss: 0.107354
Train Epoch: 2 [5121024/8742051 (58.6%)]	Loss: 0.107681
Train Epoch: 2 [5172224/8742051 (59.2%)]	Loss: 0.111223
Train Epoch: 2 [5223424/8742051 (59.8%)]	Loss: 0.105747
Train Epoch: 2 [5274624/8742051 (60.3%)]	Loss: 0.117524
Train Epoch: 2 [5325824/8742051 (60.9%)]	Loss: 0.100959
Train Epoch: 2 [5377024/8742051 (61.5%)]	Loss: 0.100527
Train Epoch: 2 [5428224/8742051 (62.1%)]	Loss: 0.110335
Train Epoch: 2 [5479424/8742051 (62.7%)]	Loss: 0.087120
Train Epoch: 2 [5530624/8742051 (63.3%)]	Loss: 0.112054
Train Epoch: 2 [5581824/8742051 (63.9%)]	Loss: 0.101690
Train Epoch: 2 [5633024/8742051 (64.4%)]	Loss: 0.110016
Train Epoch: 2 [5684224/8742051 (65.0%)]	Loss: 0.095785
Train Epoch: 2 [5735424/8742051 (65.6%)]	Loss: 0.118235
Train Epoch: 2 [5786624/8742051 (66.2%)]	Loss: 0.105173
Train Epoch: 2 [5837824/8742051 (66.8%)]	Loss: 0.115600
Train Epoch: 2 [5889024/8742051 (67.4%)]	Loss: 0.104216
Train Epoch: 2 [5940224/8742051 (68.0%)]	Loss: 0.106436
Train Epoch: 2 [5991424/8742051 (68.5%)]	Loss: 0.119951
Train Epoch: 2 [6042624/8742051 (69.1%)]	Loss: 0.093460
Train Epoch: 2 [6093824/8742051 (69.7%)]	Loss: 0.093267
Train Epoch: 2 [6145024/8742051 (70.3%)]	Loss: 0.113969
Train Epoch: 2 [6196224/8742051 (70.9%)]	Loss: 0.103495
Train Epoch: 2 [6247424/8742051 (71.5%)]	Loss: 0.104892
Train Epoch: 2 [6298624/8742051 (72.0%)]	Loss: 0.117083
Train Epoch: 2 [6349824/8742051 (72.6%)]	Loss: 0.096645
Train Epoch: 2 [6401024/8742051 (73.2%)]	Loss: 0.112736
Train Epoch: 2 [6452224/8742051 (73.8%)]	Loss: 0.094191
Train Epoch: 2 [6503424/8742051 (74.4%)]	Loss: 0.114871
Train Epoch: 2 [6554624/8742051 (75.0%)]	Loss: 0.114690
Train Epoch: 2 [6605824/8742051 (75.6%)]	Loss: 0.107370
Train Epoch: 2 [6657024/8742051 (76.1%)]	Loss: 0.098981
Train Epoch: 2 [6708224/8742051 (76.7%)]	Loss: 0.120189
Train Epoch: 2 [6759424/8742051 (77.3%)]	Loss: 0.103640
Train Epoch: 2 [6810624/8742051 (77.9%)]	Loss: 0.110715
Train Epoch: 2 [6861824/8742051 (78.5%)]	Loss: 0.114444
Train Epoch: 2 [6913024/8742051 (79.1%)]	Loss: 0.114534
Train Epoch: 2 [6964224/8742051 (79.7%)]	Loss: 0.095256
Train Epoch: 2 [7015424/8742051 (80.2%)]	Loss: 0.104457
Train Epoch: 2 [7066624/8742051 (80.8%)]	Loss: 0.103743
Train Epoch: 2 [7117824/8742051 (81.4%)]	Loss: 0.092594
Train Epoch: 2 [7169024/8742051 (82.0%)]	Loss: 0.114937
Train Epoch: 2 [7220224/8742051 (82.6%)]	Loss: 0.100538
Train Epoch: 2 [7271424/8742051 (83.2%)]	Loss: 0.103415
Train Epoch: 2 [7322624/8742051 (83.8%)]	Loss: 0.109649
Train Epoch: 2 [7373824/8742051 (84.3%)]	Loss: 0.111428
Train Epoch: 2 [7425024/8742051 (84.9%)]	Loss: 0.092414
Train Epoch: 2 [7476224/8742051 (85.5%)]	Loss: 0.107386
Train Epoch: 2 [7527424/8742051 (86.1%)]	Loss: 0.110014
Train Epoch: 2 [7578624/8742051 (86.7%)]	Loss: 0.100822
Train Epoch: 2 [7629824/8742051 (87.3%)]	Loss: 0.104967
Train Epoch: 2 [7681024/8742051 (87.9%)]	Loss: 0.124651
Train Epoch: 2 [7732224/8742051 (88.4%)]	Loss: 0.106015
Train Epoch: 2 [7783424/8742051 (89.0%)]	Loss: 0.101278
Train Epoch: 2 [7834624/8742051 (89.6%)]	Loss: 0.101865
Train Epoch: 2 [7885824/8742051 (90.2%)]	Loss: 0.100654
Train Epoch: 2 [7937024/8742051 (90.8%)]	Loss: 0.112850
Train Epoch: 2 [7988224/8742051 (91.4%)]	Loss: 0.107899
Train Epoch: 2 [8039424/8742051 (92.0%)]	Loss: 0.101858
Train Epoch: 2 [8090624/8742051 (92.5%)]	Loss: 0.113743
Train Epoch: 2 [8141824/8742051 (93.1%)]	Loss: 0.108630
Train Epoch: 2 [8193024/8742051 (93.7%)]	Loss: 0.111392
Train Epoch: 2 [8244224/8742051 (94.3%)]	Loss: 0.100438
Train Epoch: 2 [8295424/8742051 (94.9%)]	Loss: 0.100663
Train Epoch: 2 [8346624/8742051 (95.5%)]	Loss: 0.098286
Train Epoch: 2 [8397824/8742051 (96.1%)]	Loss: 0.119748
Train Epoch: 2 [8449024/8742051 (96.6%)]	Loss: 0.105885
Train Epoch: 2 [8500224/8742051 (97.2%)]	Loss: 0.097924
Train Epoch: 2 [8551424/8742051 (97.8%)]	Loss: 0.099969
Train Epoch: 2 [8602624/8742051 (98.4%)]	Loss: 0.125049
Train Epoch: 2 [8653824/8742051 (99.0%)]	Loss: 0.107554
Train Epoch: 2 [8705024/8742051 (99.6%)]	Loss: 0.099381

ACC in fold#3 was 0.889


Balanced ACC in fold#3 was 0.863


MCC in fold#3 was 0.733


Confusion Matrix in fold#3: 
           nonRipple   Ripple
nonRipple     521214   131364
Ripple        111311  1421623


Classification Report in fold#3: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.824        0.915  ...        0.870         0.888
recall            0.799        0.927  ...        0.863         0.889
f1-score          0.811        0.921  ...        0.866         0.888
sample size  652578.000  1532934.000  ...  2185512.000   2185512.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/8742051 (0.0%)]	Loss: 0.359934
Train Epoch: 1 [52224/8742051 (0.6%)]	Loss: 0.165554
Train Epoch: 1 [103424/8742051 (1.2%)]	Loss: 0.138240
Train Epoch: 1 [154624/8742051 (1.8%)]	Loss: 0.119743
Train Epoch: 1 [205824/8742051 (2.4%)]	Loss: 0.128664
Train Epoch: 1 [257024/8742051 (2.9%)]	Loss: 0.119752
Train Epoch: 1 [308224/8742051 (3.5%)]	Loss: 0.129335
Train Epoch: 1 [359424/8742051 (4.1%)]	Loss: 0.106879
Train Epoch: 1 [410624/8742051 (4.7%)]	Loss: 0.130414
Train Epoch: 1 [461824/8742051 (5.3%)]	Loss: 0.109394
Train Epoch: 1 [513024/8742051 (5.9%)]	Loss: 0.102862
Train Epoch: 1 [564224/8742051 (6.5%)]	Loss: 0.102983
Train Epoch: 1 [615424/8742051 (7.0%)]	Loss: 0.130523
Train Epoch: 1 [666624/8742051 (7.6%)]	Loss: 0.129053
Train Epoch: 1 [717824/8742051 (8.2%)]	Loss: 0.107991
Train Epoch: 1 [769024/8742051 (8.8%)]	Loss: 0.129184
Train Epoch: 1 [820224/8742051 (9.4%)]	Loss: 0.120055
Train Epoch: 1 [871424/8742051 (10.0%)]	Loss: 0.128140
Train Epoch: 1 [922624/8742051 (10.6%)]	Loss: 0.124173
Train Epoch: 1 [973824/8742051 (11.1%)]	Loss: 0.113400
Train Epoch: 1 [1025024/8742051 (11.7%)]	Loss: 0.122572
Train Epoch: 1 [1076224/8742051 (12.3%)]	Loss: 0.123928
Train Epoch: 1 [1127424/8742051 (12.9%)]	Loss: 0.116600
Train Epoch: 1 [1178624/8742051 (13.5%)]	Loss: 0.122353
Train Epoch: 1 [1229824/8742051 (14.1%)]	Loss: 0.109538
Train Epoch: 1 [1281024/8742051 (14.7%)]	Loss: 0.120448
Train Epoch: 1 [1332224/8742051 (15.2%)]	Loss: 0.101682
Train Epoch: 1 [1383424/8742051 (15.8%)]	Loss: 0.103726
Train Epoch: 1 [1434624/8742051 (16.4%)]	Loss: 0.112305
Train Epoch: 1 [1485824/8742051 (17.0%)]	Loss: 0.105683
Train Epoch: 1 [1537024/8742051 (17.6%)]	Loss: 0.120161
Train Epoch: 1 [1588224/8742051 (18.2%)]	Loss: 0.116029
Train Epoch: 1 [1639424/8742051 (18.8%)]	Loss: 0.118504
Train Epoch: 1 [1690624/8742051 (19.3%)]	Loss: 0.112741
Train Epoch: 1 [1741824/8742051 (19.9%)]	Loss: 0.106977
Train Epoch: 1 [1793024/8742051 (20.5%)]	Loss: 0.112674
Train Epoch: 1 [1844224/8742051 (21.1%)]	Loss: 0.116510
Train Epoch: 1 [1895424/8742051 (21.7%)]	Loss: 0.110593
Train Epoch: 1 [1946624/8742051 (22.3%)]	Loss: 0.123358
Train Epoch: 1 [1997824/8742051 (22.9%)]	Loss: 0.106573
Train Epoch: 1 [2049024/8742051 (23.4%)]	Loss: 0.104017
Train Epoch: 1 [2100224/8742051 (24.0%)]	Loss: 0.112235
Train Epoch: 1 [2151424/8742051 (24.6%)]	Loss: 0.122992
Train Epoch: 1 [2202624/8742051 (25.2%)]	Loss: 0.122922
Train Epoch: 1 [2253824/8742051 (25.8%)]	Loss: 0.115728
Train Epoch: 1 [2305024/8742051 (26.4%)]	Loss: 0.118678
Train Epoch: 1 [2356224/8742051 (27.0%)]	Loss: 0.135874
Train Epoch: 1 [2407424/8742051 (27.5%)]	Loss: 0.124672
Train Epoch: 1 [2458624/8742051 (28.1%)]	Loss: 0.107216
Train Epoch: 1 [2509824/8742051 (28.7%)]	Loss: 0.109626
Train Epoch: 1 [2561024/8742051 (29.3%)]	Loss: 0.128290
Train Epoch: 1 [2612224/8742051 (29.9%)]	Loss: 0.104457
Train Epoch: 1 [2663424/8742051 (30.5%)]	Loss: 0.095473
Train Epoch: 1 [2714624/8742051 (31.1%)]	Loss: 0.121336
Train Epoch: 1 [2765824/8742051 (31.6%)]	Loss: 0.130020
Train Epoch: 1 [2817024/8742051 (32.2%)]	Loss: 0.115921
Train Epoch: 1 [2868224/8742051 (32.8%)]	Loss: 0.101115
Train Epoch: 1 [2919424/8742051 (33.4%)]	Loss: 0.115272
Train Epoch: 1 [2970624/8742051 (34.0%)]	Loss: 0.117484
Train Epoch: 1 [3021824/8742051 (34.6%)]	Loss: 0.107126
Train Epoch: 1 [3073024/8742051 (35.2%)]	Loss: 0.126737
Train Epoch: 1 [3124224/8742051 (35.7%)]	Loss: 0.122202
Train Epoch: 1 [3175424/8742051 (36.3%)]	Loss: 0.099763
Train Epoch: 1 [3226624/8742051 (36.9%)]	Loss: 0.117377
Train Epoch: 1 [3277824/8742051 (37.5%)]	Loss: 0.122153
Train Epoch: 1 [3329024/8742051 (38.1%)]	Loss: 0.119389
Train Epoch: 1 [3380224/8742051 (38.7%)]	Loss: 0.104876
Train Epoch: 1 [3431424/8742051 (39.3%)]	Loss: 0.102594
Train Epoch: 1 [3482624/8742051 (39.8%)]	Loss: 0.105238
Train Epoch: 1 [3533824/8742051 (40.4%)]	Loss: 0.111649
Train Epoch: 1 [3585024/8742051 (41.0%)]	Loss: 0.093452
Train Epoch: 1 [3636224/8742051 (41.6%)]	Loss: 0.117331
Train Epoch: 1 [3687424/8742051 (42.2%)]	Loss: 0.113018
Train Epoch: 1 [3738624/8742051 (42.8%)]	Loss: 0.099084
Train Epoch: 1 [3789824/8742051 (43.4%)]	Loss: 0.114107
Train Epoch: 1 [3841024/8742051 (43.9%)]	Loss: 0.114795
Train Epoch: 1 [3892224/8742051 (44.5%)]	Loss: 0.111345
Train Epoch: 1 [3943424/8742051 (45.1%)]	Loss: 0.106110
Train Epoch: 1 [3994624/8742051 (45.7%)]	Loss: 0.107362
Train Epoch: 1 [4045824/8742051 (46.3%)]	Loss: 0.121520
Train Epoch: 1 [4097024/8742051 (46.9%)]	Loss: 0.140003
Train Epoch: 1 [4148224/8742051 (47.5%)]	Loss: 0.113408
Train Epoch: 1 [4199424/8742051 (48.0%)]	Loss: 0.113222
Train Epoch: 1 [4250624/8742051 (48.6%)]	Loss: 0.112451
Train Epoch: 1 [4301824/8742051 (49.2%)]	Loss: 0.114673
Train Epoch: 1 [4353024/8742051 (49.8%)]	Loss: 0.110694
Train Epoch: 1 [4404224/8742051 (50.4%)]	Loss: 0.116919
Train Epoch: 1 [4455424/8742051 (51.0%)]	Loss: 0.112727
Train Epoch: 1 [4506624/8742051 (51.6%)]	Loss: 0.106446
Train Epoch: 1 [4557824/8742051 (52.1%)]	Loss: 0.121366
Train Epoch: 1 [4609024/8742051 (52.7%)]	Loss: 0.107542
Train Epoch: 1 [4660224/8742051 (53.3%)]	Loss: 0.127999
Train Epoch: 1 [4711424/8742051 (53.9%)]	Loss: 0.113400
Train Epoch: 1 [4762624/8742051 (54.5%)]	Loss: 0.108325
Train Epoch: 1 [4813824/8742051 (55.1%)]	Loss: 0.097928
Train Epoch: 1 [4865024/8742051 (55.7%)]	Loss: 0.107537
Train Epoch: 1 [4916224/8742051 (56.2%)]	Loss: 0.110575
Train Epoch: 1 [4967424/8742051 (56.8%)]	Loss: 0.114405
Train Epoch: 1 [5018624/8742051 (57.4%)]	Loss: 0.105153
Train Epoch: 1 [5069824/8742051 (58.0%)]	Loss: 0.108209
Train Epoch: 1 [5121024/8742051 (58.6%)]	Loss: 0.123155
Train Epoch: 1 [5172224/8742051 (59.2%)]	Loss: 0.098056
Train Epoch: 1 [5223424/8742051 (59.8%)]	Loss: 0.103237
Train Epoch: 1 [5274624/8742051 (60.3%)]	Loss: 0.100768
Train Epoch: 1 [5325824/8742051 (60.9%)]	Loss: 0.114116
Train Epoch: 1 [5377024/8742051 (61.5%)]	Loss: 0.126705
Train Epoch: 1 [5428224/8742051 (62.1%)]	Loss: 0.101851
Train Epoch: 1 [5479424/8742051 (62.7%)]	Loss: 0.117806
Train Epoch: 1 [5530624/8742051 (63.3%)]	Loss: 0.100024
Train Epoch: 1 [5581824/8742051 (63.9%)]	Loss: 0.105432
Train Epoch: 1 [5633024/8742051 (64.4%)]	Loss: 0.104821
Train Epoch: 1 [5684224/8742051 (65.0%)]	Loss: 0.104395
Train Epoch: 1 [5735424/8742051 (65.6%)]	Loss: 0.113079
Train Epoch: 1 [5786624/8742051 (66.2%)]	Loss: 0.099511
Train Epoch: 1 [5837824/8742051 (66.8%)]	Loss: 0.103132
Train Epoch: 1 [5889024/8742051 (67.4%)]	Loss: 0.099516
Train Epoch: 1 [5940224/8742051 (68.0%)]	Loss: 0.113190
Train Epoch: 1 [5991424/8742051 (68.5%)]	Loss: 0.106945
Train Epoch: 1 [6042624/8742051 (69.1%)]	Loss: 0.119459
Train Epoch: 1 [6093824/8742051 (69.7%)]	Loss: 0.116165
Train Epoch: 1 [6145024/8742051 (70.3%)]	Loss: 0.121334
Train Epoch: 1 [6196224/8742051 (70.9%)]	Loss: 0.113460
Train Epoch: 1 [6247424/8742051 (71.5%)]	Loss: 0.110130
Train Epoch: 1 [6298624/8742051 (72.0%)]	Loss: 0.109841
Train Epoch: 1 [6349824/8742051 (72.6%)]	Loss: 0.116033
Train Epoch: 1 [6401024/8742051 (73.2%)]	Loss: 0.110877
Train Epoch: 1 [6452224/8742051 (73.8%)]	Loss: 0.108895
Train Epoch: 1 [6503424/8742051 (74.4%)]	Loss: 0.119582
Train Epoch: 1 [6554624/8742051 (75.0%)]	Loss: 0.117601
Train Epoch: 1 [6605824/8742051 (75.6%)]	Loss: 0.109332
Train Epoch: 1 [6657024/8742051 (76.1%)]	Loss: 0.106205
Train Epoch: 1 [6708224/8742051 (76.7%)]	Loss: 0.115341
Train Epoch: 1 [6759424/8742051 (77.3%)]	Loss: 0.115678
Train Epoch: 1 [6810624/8742051 (77.9%)]	Loss: 0.106561
Train Epoch: 1 [6861824/8742051 (78.5%)]	Loss: 0.116061
Train Epoch: 1 [6913024/8742051 (79.1%)]	Loss: 0.103092
Train Epoch: 1 [6964224/8742051 (79.7%)]	Loss: 0.129490
Train Epoch: 1 [7015424/8742051 (80.2%)]	Loss: 0.116847
Train Epoch: 1 [7066624/8742051 (80.8%)]	Loss: 0.118530
Train Epoch: 1 [7117824/8742051 (81.4%)]	Loss: 0.098010
Train Epoch: 1 [7169024/8742051 (82.0%)]	Loss: 0.109351
Train Epoch: 1 [7220224/8742051 (82.6%)]	Loss: 0.103602
Train Epoch: 1 [7271424/8742051 (83.2%)]	Loss: 0.104242
Train Epoch: 1 [7322624/8742051 (83.8%)]	Loss: 0.110060
Train Epoch: 1 [7373824/8742051 (84.3%)]	Loss: 0.111332
Train Epoch: 1 [7425024/8742051 (84.9%)]	Loss: 0.103986
Train Epoch: 1 [7476224/8742051 (85.5%)]	Loss: 0.111305
Train Epoch: 1 [7527424/8742051 (86.1%)]	Loss: 0.110041
Train Epoch: 1 [7578624/8742051 (86.7%)]	Loss: 0.117080
Train Epoch: 1 [7629824/8742051 (87.3%)]	Loss: 0.098828
Train Epoch: 1 [7681024/8742051 (87.9%)]	Loss: 0.131631
Train Epoch: 1 [7732224/8742051 (88.4%)]	Loss: 0.118962
Train Epoch: 1 [7783424/8742051 (89.0%)]	Loss: 0.107979
Train Epoch: 1 [7834624/8742051 (89.6%)]	Loss: 0.105090
Train Epoch: 1 [7885824/8742051 (90.2%)]	Loss: 0.100883
Train Epoch: 1 [7937024/8742051 (90.8%)]	Loss: 0.118062
Train Epoch: 1 [7988224/8742051 (91.4%)]	Loss: 0.108813
Train Epoch: 1 [8039424/8742051 (92.0%)]	Loss: 0.120009
Train Epoch: 1 [8090624/8742051 (92.5%)]	Loss: 0.113747
Train Epoch: 1 [8141824/8742051 (93.1%)]	Loss: 0.113713
Train Epoch: 1 [8193024/8742051 (93.7%)]	Loss: 0.116805
Train Epoch: 1 [8244224/8742051 (94.3%)]	Loss: 0.103742
Train Epoch: 1 [8295424/8742051 (94.9%)]	Loss: 0.104466
Train Epoch: 1 [8346624/8742051 (95.5%)]	Loss: 0.099805
Train Epoch: 1 [8397824/8742051 (96.1%)]	Loss: 0.107249
Train Epoch: 1 [8449024/8742051 (96.6%)]	Loss: 0.103423
Train Epoch: 1 [8500224/8742051 (97.2%)]	Loss: 0.106040
Train Epoch: 1 [8551424/8742051 (97.8%)]	Loss: 0.113282
Train Epoch: 1 [8602624/8742051 (98.4%)]	Loss: 0.097656
Train Epoch: 1 [8653824/8742051 (99.0%)]	Loss: 0.097767
Train Epoch: 1 [8705024/8742051 (99.6%)]	Loss: 0.110145
Train Epoch: 2 [1024/8742051 (0.0%)]	Loss: 0.108684
Train Epoch: 2 [52224/8742051 (0.6%)]	Loss: 0.101245
Train Epoch: 2 [103424/8742051 (1.2%)]	Loss: 0.120022
Train Epoch: 2 [154624/8742051 (1.8%)]	Loss: 0.099954
Train Epoch: 2 [205824/8742051 (2.4%)]	Loss: 0.103093
Train Epoch: 2 [257024/8742051 (2.9%)]	Loss: 0.106529
Train Epoch: 2 [308224/8742051 (3.5%)]	Loss: 0.114093
Train Epoch: 2 [359424/8742051 (4.1%)]	Loss: 0.126701
Train Epoch: 2 [410624/8742051 (4.7%)]	Loss: 0.105093
Train Epoch: 2 [461824/8742051 (5.3%)]	Loss: 0.106823
Train Epoch: 2 [513024/8742051 (5.9%)]	Loss: 0.125321
Train Epoch: 2 [564224/8742051 (6.5%)]	Loss: 0.109540
Train Epoch: 2 [615424/8742051 (7.0%)]	Loss: 0.111024
Train Epoch: 2 [666624/8742051 (7.6%)]	Loss: 0.113142
Train Epoch: 2 [717824/8742051 (8.2%)]	Loss: 0.118327
Train Epoch: 2 [769024/8742051 (8.8%)]	Loss: 0.123027
Train Epoch: 2 [820224/8742051 (9.4%)]	Loss: 0.107180
Train Epoch: 2 [871424/8742051 (10.0%)]	Loss: 0.122863
Train Epoch: 2 [922624/8742051 (10.6%)]	Loss: 0.099647
Train Epoch: 2 [973824/8742051 (11.1%)]	Loss: 0.104660
Train Epoch: 2 [1025024/8742051 (11.7%)]	Loss: 0.108626
Train Epoch: 2 [1076224/8742051 (12.3%)]	Loss: 0.107615
Train Epoch: 2 [1127424/8742051 (12.9%)]	Loss: 0.108591
Train Epoch: 2 [1178624/8742051 (13.5%)]	Loss: 0.114228
Train Epoch: 2 [1229824/8742051 (14.1%)]	Loss: 0.103485
Train Epoch: 2 [1281024/8742051 (14.7%)]	Loss: 0.121148
Train Epoch: 2 [1332224/8742051 (15.2%)]	Loss: 0.108676
Train Epoch: 2 [1383424/8742051 (15.8%)]	Loss: 0.120251
Train Epoch: 2 [1434624/8742051 (16.4%)]	Loss: 0.121115
Train Epoch: 2 [1485824/8742051 (17.0%)]	Loss: 0.106425
Train Epoch: 2 [1537024/8742051 (17.6%)]	Loss: 0.107182
Train Epoch: 2 [1588224/8742051 (18.2%)]	Loss: 0.121417
Train Epoch: 2 [1639424/8742051 (18.8%)]	Loss: 0.106572
Train Epoch: 2 [1690624/8742051 (19.3%)]	Loss: 0.122064
Train Epoch: 2 [1741824/8742051 (19.9%)]	Loss: 0.102846
Train Epoch: 2 [1793024/8742051 (20.5%)]	Loss: 0.099062
Train Epoch: 2 [1844224/8742051 (21.1%)]	Loss: 0.110974
Train Epoch: 2 [1895424/8742051 (21.7%)]	Loss: 0.099059
Train Epoch: 2 [1946624/8742051 (22.3%)]	Loss: 0.102841
Train Epoch: 2 [1997824/8742051 (22.9%)]	Loss: 0.103454
Train Epoch: 2 [2049024/8742051 (23.4%)]	Loss: 0.106806
Train Epoch: 2 [2100224/8742051 (24.0%)]	Loss: 0.105488
Train Epoch: 2 [2151424/8742051 (24.6%)]	Loss: 0.110000
Train Epoch: 2 [2202624/8742051 (25.2%)]	Loss: 0.110310
Train Epoch: 2 [2253824/8742051 (25.8%)]	Loss: 0.110438
Train Epoch: 2 [2305024/8742051 (26.4%)]	Loss: 0.114678
Train Epoch: 2 [2356224/8742051 (27.0%)]	Loss: 0.105145
Train Epoch: 2 [2407424/8742051 (27.5%)]	Loss: 0.120087
Train Epoch: 2 [2458624/8742051 (28.1%)]	Loss: 0.113277
Train Epoch: 2 [2509824/8742051 (28.7%)]	Loss: 0.106964
Train Epoch: 2 [2561024/8742051 (29.3%)]	Loss: 0.091547
Train Epoch: 2 [2612224/8742051 (29.9%)]	Loss: 0.107825
Train Epoch: 2 [2663424/8742051 (30.5%)]	Loss: 0.108556
Train Epoch: 2 [2714624/8742051 (31.1%)]	Loss: 0.125398
Train Epoch: 2 [2765824/8742051 (31.6%)]	Loss: 0.108405
Train Epoch: 2 [2817024/8742051 (32.2%)]	Loss: 0.104745
Train Epoch: 2 [2868224/8742051 (32.8%)]	Loss: 0.094898
Train Epoch: 2 [2919424/8742051 (33.4%)]	Loss: 0.106767
Train Epoch: 2 [2970624/8742051 (34.0%)]	Loss: 0.108660
Train Epoch: 2 [3021824/8742051 (34.6%)]	Loss: 0.110489
Train Epoch: 2 [3073024/8742051 (35.2%)]	Loss: 0.102699
Train Epoch: 2 [3124224/8742051 (35.7%)]	Loss: 0.112755
Train Epoch: 2 [3175424/8742051 (36.3%)]	Loss: 0.101715
Train Epoch: 2 [3226624/8742051 (36.9%)]	Loss: 0.105121
Train Epoch: 2 [3277824/8742051 (37.5%)]	Loss: 0.116619
Train Epoch: 2 [3329024/8742051 (38.1%)]	Loss: 0.122951
Train Epoch: 2 [3380224/8742051 (38.7%)]	Loss: 0.107775
Train Epoch: 2 [3431424/8742051 (39.3%)]	Loss: 0.101084
Train Epoch: 2 [3482624/8742051 (39.8%)]	Loss: 0.103974
Train Epoch: 2 [3533824/8742051 (40.4%)]	Loss: 0.109915
Train Epoch: 2 [3585024/8742051 (41.0%)]	Loss: 0.119434
Train Epoch: 2 [3636224/8742051 (41.6%)]	Loss: 0.096790
Train Epoch: 2 [3687424/8742051 (42.2%)]	Loss: 0.114718
Train Epoch: 2 [3738624/8742051 (42.8%)]	Loss: 0.107955
Train Epoch: 2 [3789824/8742051 (43.4%)]	Loss: 0.093902
Train Epoch: 2 [3841024/8742051 (43.9%)]	Loss: 0.098613
Train Epoch: 2 [3892224/8742051 (44.5%)]	Loss: 0.106442
Train Epoch: 2 [3943424/8742051 (45.1%)]	Loss: 0.103777
Train Epoch: 2 [3994624/8742051 (45.7%)]	Loss: 0.126793
Train Epoch: 2 [4045824/8742051 (46.3%)]	Loss: 0.088858
Train Epoch: 2 [4097024/8742051 (46.9%)]	Loss: 0.110715
Train Epoch: 2 [4148224/8742051 (47.5%)]	Loss: 0.099100
Train Epoch: 2 [4199424/8742051 (48.0%)]	Loss: 0.100314
Train Epoch: 2 [4250624/8742051 (48.6%)]	Loss: 0.116760
Train Epoch: 2 [4301824/8742051 (49.2%)]	Loss: 0.110944
Train Epoch: 2 [4353024/8742051 (49.8%)]	Loss: 0.118064
Train Epoch: 2 [4404224/8742051 (50.4%)]	Loss: 0.111912
Train Epoch: 2 [4455424/8742051 (51.0%)]	Loss: 0.117240
Train Epoch: 2 [4506624/8742051 (51.6%)]	Loss: 0.115937
Train Epoch: 2 [4557824/8742051 (52.1%)]	Loss: 0.115870
Train Epoch: 2 [4609024/8742051 (52.7%)]	Loss: 0.121206
Train Epoch: 2 [4660224/8742051 (53.3%)]	Loss: 0.113996
Train Epoch: 2 [4711424/8742051 (53.9%)]	Loss: 0.108134
Train Epoch: 2 [4762624/8742051 (54.5%)]	Loss: 0.094319
Train Epoch: 2 [4813824/8742051 (55.1%)]	Loss: 0.100615
Train Epoch: 2 [4865024/8742051 (55.7%)]	Loss: 0.106950
Train Epoch: 2 [4916224/8742051 (56.2%)]	Loss: 0.096897
Train Epoch: 2 [4967424/8742051 (56.8%)]	Loss: 0.105081
Train Epoch: 2 [5018624/8742051 (57.4%)]	Loss: 0.109141
Train Epoch: 2 [5069824/8742051 (58.0%)]	Loss: 0.125061
Train Epoch: 2 [5121024/8742051 (58.6%)]	Loss: 0.120165
Train Epoch: 2 [5172224/8742051 (59.2%)]	Loss: 0.109581
Train Epoch: 2 [5223424/8742051 (59.8%)]	Loss: 0.111872
Train Epoch: 2 [5274624/8742051 (60.3%)]	Loss: 0.108557
Train Epoch: 2 [5325824/8742051 (60.9%)]	Loss: 0.097397
Train Epoch: 2 [5377024/8742051 (61.5%)]	Loss: 0.103123
Train Epoch: 2 [5428224/8742051 (62.1%)]	Loss: 0.103960
Train Epoch: 2 [5479424/8742051 (62.7%)]	Loss: 0.100483
Train Epoch: 2 [5530624/8742051 (63.3%)]	Loss: 0.094521
Train Epoch: 2 [5581824/8742051 (63.9%)]	Loss: 0.102435
Train Epoch: 2 [5633024/8742051 (64.4%)]	Loss: 0.109999
Train Epoch: 2 [5684224/8742051 (65.0%)]	Loss: 0.108229
Train Epoch: 2 [5735424/8742051 (65.6%)]	Loss: 0.103324
Train Epoch: 2 [5786624/8742051 (66.2%)]	Loss: 0.107249
Train Epoch: 2 [5837824/8742051 (66.8%)]	Loss: 0.107630
Train Epoch: 2 [5889024/8742051 (67.4%)]	Loss: 0.104056
Train Epoch: 2 [5940224/8742051 (68.0%)]	Loss: 0.114640
Train Epoch: 2 [5991424/8742051 (68.5%)]	Loss: 0.097093
Train Epoch: 2 [6042624/8742051 (69.1%)]	Loss: 0.107985
Train Epoch: 2 [6093824/8742051 (69.7%)]	Loss: 0.102607
Train Epoch: 2 [6145024/8742051 (70.3%)]	Loss: 0.107634
Train Epoch: 2 [6196224/8742051 (70.9%)]	Loss: 0.086086
Train Epoch: 2 [6247424/8742051 (71.5%)]	Loss: 0.097288
Train Epoch: 2 [6298624/8742051 (72.0%)]	Loss: 0.102904
Train Epoch: 2 [6349824/8742051 (72.6%)]	Loss: 0.105161
Train Epoch: 2 [6401024/8742051 (73.2%)]	Loss: 0.114485
Train Epoch: 2 [6452224/8742051 (73.8%)]	Loss: 0.101067
Train Epoch: 2 [6503424/8742051 (74.4%)]	Loss: 0.116042
Train Epoch: 2 [6554624/8742051 (75.0%)]	Loss: 0.112100
Train Epoch: 2 [6605824/8742051 (75.6%)]	Loss: 0.106778
Train Epoch: 2 [6657024/8742051 (76.1%)]	Loss: 0.115006
Train Epoch: 2 [6708224/8742051 (76.7%)]	Loss: 0.105466
Train Epoch: 2 [6759424/8742051 (77.3%)]	Loss: 0.094363
Train Epoch: 2 [6810624/8742051 (77.9%)]	Loss: 0.120189
Train Epoch: 2 [6861824/8742051 (78.5%)]	Loss: 0.106807
Train Epoch: 2 [6913024/8742051 (79.1%)]	Loss: 0.109339
Train Epoch: 2 [6964224/8742051 (79.7%)]	Loss: 0.098417
Train Epoch: 2 [7015424/8742051 (80.2%)]	Loss: 0.104871
Train Epoch: 2 [7066624/8742051 (80.8%)]	Loss: 0.095492
Train Epoch: 2 [7117824/8742051 (81.4%)]	Loss: 0.114730
Train Epoch: 2 [7169024/8742051 (82.0%)]	Loss: 0.115544
Train Epoch: 2 [7220224/8742051 (82.6%)]	Loss: 0.112537
Train Epoch: 2 [7271424/8742051 (83.2%)]	Loss: 0.115584
Train Epoch: 2 [7322624/8742051 (83.8%)]	Loss: 0.103507
Train Epoch: 2 [7373824/8742051 (84.3%)]	Loss: 0.108779
Train Epoch: 2 [7425024/8742051 (84.9%)]	Loss: 0.089977
Train Epoch: 2 [7476224/8742051 (85.5%)]	Loss: 0.097304
Train Epoch: 2 [7527424/8742051 (86.1%)]	Loss: 0.115708
Train Epoch: 2 [7578624/8742051 (86.7%)]	Loss: 0.103620
Train Epoch: 2 [7629824/8742051 (87.3%)]	Loss: 0.117444
Train Epoch: 2 [7681024/8742051 (87.9%)]	Loss: 0.106560
Train Epoch: 2 [7732224/8742051 (88.4%)]	Loss: 0.102063
Train Epoch: 2 [7783424/8742051 (89.0%)]	Loss: 0.115361
Train Epoch: 2 [7834624/8742051 (89.6%)]	Loss: 0.098324
Train Epoch: 2 [7885824/8742051 (90.2%)]	Loss: 0.109451
Train Epoch: 2 [7937024/8742051 (90.8%)]	Loss: 0.108194
Train Epoch: 2 [7988224/8742051 (91.4%)]	Loss: 0.109276
Train Epoch: 2 [8039424/8742051 (92.0%)]	Loss: 0.101180
Train Epoch: 2 [8090624/8742051 (92.5%)]	Loss: 0.102056
Train Epoch: 2 [8141824/8742051 (93.1%)]	Loss: 0.098026
Train Epoch: 2 [8193024/8742051 (93.7%)]	Loss: 0.101097
Train Epoch: 2 [8244224/8742051 (94.3%)]	Loss: 0.113678
Train Epoch: 2 [8295424/8742051 (94.9%)]	Loss: 0.104732
Train Epoch: 2 [8346624/8742051 (95.5%)]	Loss: 0.113976
Train Epoch: 2 [8397824/8742051 (96.1%)]	Loss: 0.113835
Train Epoch: 2 [8449024/8742051 (96.6%)]	Loss: 0.117749
Train Epoch: 2 [8500224/8742051 (97.2%)]	Loss: 0.098096
Train Epoch: 2 [8551424/8742051 (97.8%)]	Loss: 0.100418
Train Epoch: 2 [8602624/8742051 (98.4%)]	Loss: 0.110128
Train Epoch: 2 [8653824/8742051 (99.0%)]	Loss: 0.099139
Train Epoch: 2 [8705024/8742051 (99.6%)]	Loss: 0.107900

ACC in fold#4 was 0.862


Balanced ACC in fold#4 was 0.883


MCC in fold#4 was 0.717


Confusion Matrix in fold#4: 
           nonRipple   Ripple
nonRipple     610906    41673
Ripple        259597  1273336


Classification Report in fold#4: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.702        0.968  ...        0.835         0.889
recall            0.936        0.831  ...        0.883         0.862
f1-score          0.802        0.894  ...        0.848         0.867
sample size  652579.000  1532933.000  ...  2185512.000   2185512.000

[4 rows x 5 columns]


Label Errors Rate:
0.031


 --- 5-fold CV overall metrics --- 


The Mattews correlation coefficient: 0.747 +/- 0.032 (mean +/- std.; n=5)


Balanced Accuracy Score: 0.858 +/- 0.026 (mean +/- std.; n=5)


Confusion Matrix (Test; sum; num. folds=5)
           nonRipple   Ripple
nonRipple    2530523   732371
Ripple        455395  7209274


Classification Report (Test; mean; num. folds=5)
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.873        0.911  ...        0.892         0.899
recall            0.776        0.941  ...        0.858         0.891
f1-score          0.809        0.923  ...        0.866         0.889
sample size  652578.800  1532933.800  ...  2185512.600   2185512.600

[4 rows x 5 columns]


Classification Report (Test; std; num. folds=5)
             nonRipple  Ripple  balanced accuracy  macro avg  weighted avg
precision        0.100   0.034              0.026      0.035         0.012
recall           0.101   0.059              0.026      0.026         0.019
f1-score         0.029   0.017              0.026      0.020         0.018
sample size      0.400   0.400              0.026      0.490         0.490


ROC AUC micro Score: 0.961 +/- 0.011 (mean +/- std.; n=5)


ROC AUC macro Score: 0.96 +/- 0.007 (mean +/- std.; n=5)


Precision-Recall AUC micro Score: 0.961 +/- 0.011 (mean +/- std.; n=5)


Precision-Recall AUC macro Score: 0.956 +/- 0.008 (mean +/- std.; n=5)


Saved to: ./data/okada/cleanlab_results/D05-/are_errors.npy


Saved to: ./data/okada/cleanlab_results/D05-/are_ripple_GMM.npy


Saved to: ./data/okada/cleanlab_results/D05-/psx_ripple.npy


Saved to: ./data/okada/cleanlab_results/D05-/mccs.csv


Saved to: ./data/okada/cleanlab_results/D05-/balanced_accs.csv


Saved to: ./data/okada/cleanlab_results/D05-/conf_mat/fold#0.png


Saved to: ./data/okada/cleanlab_results/D05-/conf_mat/fold#1.png


Saved to: ./data/okada/cleanlab_results/D05-/conf_mat/fold#2.png


Saved to: ./data/okada/cleanlab_results/D05-/conf_mat/fold#3.png


Saved to: ./data/okada/cleanlab_results/D05-/conf_mat/fold#4.png


Saved to: ./data/okada/cleanlab_results/D05-/conf_mat/conf_mats.csv


Saved to: ./data/okada/cleanlab_results/D05-/conf_mat/overall_sum.png


Saved to: ./data/okada/cleanlab_results/D05-/clf_reports.csv


Saved to: ./data/okada/cleanlab_results/D05-/aucs.csv


Saved to: ./data/okada/cleanlab_results/D05-/roc_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D05-/roc_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D05-/roc_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D05-/roc_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D05-/roc_curves/fold#4.png


Saved to: ./data/okada/cleanlab_results/D05-/pr_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D05-/pr_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D05-/pr_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D05-/pr_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D05-/pr_curves/fold#4.png


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-2_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-3_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D05-/tt7-4_fp16.pkl

