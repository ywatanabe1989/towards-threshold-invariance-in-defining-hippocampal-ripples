
Random seeds have been fixed as 42


dataset_key: D03-

['./data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/01/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy']

2021-0810-0246

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/1637208 (0.1%)]	Loss: 0.355981
Train Epoch: 1 [52224/1637208 (3.2%)]	Loss: 0.207296
Train Epoch: 1 [103424/1637208 (6.3%)]	Loss: 0.168700
Train Epoch: 1 [154624/1637208 (9.4%)]	Loss: 0.164444
Train Epoch: 1 [205824/1637208 (12.6%)]	Loss: 0.151086
Train Epoch: 1 [257024/1637208 (15.7%)]	Loss: 0.154335
Train Epoch: 1 [308224/1637208 (18.8%)]	Loss: 0.156323
Train Epoch: 1 [359424/1637208 (22.0%)]	Loss: 0.155612
Train Epoch: 1 [410624/1637208 (25.1%)]	Loss: 0.165131
Train Epoch: 1 [461824/1637208 (28.2%)]	Loss: 0.154317
Train Epoch: 1 [513024/1637208 (31.3%)]	Loss: 0.150678
Train Epoch: 1 [564224/1637208 (34.5%)]	Loss: 0.169468
Train Epoch: 1 [615424/1637208 (37.6%)]	Loss: 0.144908
Train Epoch: 1 [666624/1637208 (40.7%)]	Loss: 0.153086
Train Epoch: 1 [717824/1637208 (43.8%)]	Loss: 0.157389
Train Epoch: 1 [769024/1637208 (47.0%)]	Loss: 0.148576
Train Epoch: 1 [820224/1637208 (50.1%)]	Loss: 0.164323
Train Epoch: 1 [871424/1637208 (53.2%)]	Loss: 0.148523
Train Epoch: 1 [922624/1637208 (56.4%)]	Loss: 0.158427
Train Epoch: 1 [973824/1637208 (59.5%)]	Loss: 0.141005
Train Epoch: 1 [1025024/1637208 (62.6%)]	Loss: 0.164772
Train Epoch: 1 [1076224/1637208 (65.7%)]	Loss: 0.144777
Train Epoch: 1 [1127424/1637208 (68.9%)]	Loss: 0.140900
Train Epoch: 1 [1178624/1637208 (72.0%)]	Loss: 0.172667
Train Epoch: 1 [1229824/1637208 (75.1%)]	Loss: 0.156286
Train Epoch: 1 [1281024/1637208 (78.2%)]	Loss: 0.138530
Train Epoch: 1 [1332224/1637208 (81.4%)]	Loss: 0.146996
Train Epoch: 1 [1383424/1637208 (84.5%)]	Loss: 0.159700
Train Epoch: 1 [1434624/1637208 (87.6%)]	Loss: 0.147675
Train Epoch: 1 [1485824/1637208 (90.8%)]	Loss: 0.150688
Train Epoch: 1 [1537024/1637208 (93.9%)]	Loss: 0.172208
Train Epoch: 1 [1588224/1637208 (97.0%)]	Loss: 0.146165
Train Epoch: 2 [1024/1637208 (0.1%)]	Loss: 0.140290
Train Epoch: 2 [52224/1637208 (3.2%)]	Loss: 0.159254
Train Epoch: 2 [103424/1637208 (6.3%)]	Loss: 0.157785
Train Epoch: 2 [154624/1637208 (9.4%)]	Loss: 0.141615
Train Epoch: 2 [205824/1637208 (12.6%)]	Loss: 0.170332
Train Epoch: 2 [257024/1637208 (15.7%)]	Loss: 0.149290
Train Epoch: 2 [308224/1637208 (18.8%)]	Loss: 0.151676
Train Epoch: 2 [359424/1637208 (22.0%)]	Loss: 0.148606
Train Epoch: 2 [410624/1637208 (25.1%)]	Loss: 0.151994
Train Epoch: 2 [461824/1637208 (28.2%)]	Loss: 0.147275
Train Epoch: 2 [513024/1637208 (31.3%)]	Loss: 0.171647
Train Epoch: 2 [564224/1637208 (34.5%)]	Loss: 0.141657
Train Epoch: 2 [615424/1637208 (37.6%)]	Loss: 0.151053
Train Epoch: 2 [666624/1637208 (40.7%)]	Loss: 0.161369
Train Epoch: 2 [717824/1637208 (43.8%)]	Loss: 0.142826
Train Epoch: 2 [769024/1637208 (47.0%)]	Loss: 0.155512
Train Epoch: 2 [820224/1637208 (50.1%)]	Loss: 0.146060
Train Epoch: 2 [871424/1637208 (53.2%)]	Loss: 0.141855
Train Epoch: 2 [922624/1637208 (56.4%)]	Loss: 0.153025
Train Epoch: 2 [973824/1637208 (59.5%)]	Loss: 0.142318
Train Epoch: 2 [1025024/1637208 (62.6%)]	Loss: 0.149613
Train Epoch: 2 [1076224/1637208 (65.7%)]	Loss: 0.139127
Train Epoch: 2 [1127424/1637208 (68.9%)]	Loss: 0.135724
Train Epoch: 2 [1178624/1637208 (72.0%)]	Loss: 0.146409
Train Epoch: 2 [1229824/1637208 (75.1%)]	Loss: 0.144594
Train Epoch: 2 [1281024/1637208 (78.2%)]	Loss: 0.149075
Train Epoch: 2 [1332224/1637208 (81.4%)]	Loss: 0.160491
Train Epoch: 2 [1383424/1637208 (84.5%)]	Loss: 0.139942
Train Epoch: 2 [1434624/1637208 (87.6%)]	Loss: 0.143200
Train Epoch: 2 [1485824/1637208 (90.8%)]	Loss: 0.150657
Train Epoch: 2 [1537024/1637208 (93.9%)]	Loss: 0.134356
Train Epoch: 2 [1588224/1637208 (97.0%)]	Loss: 0.142813

ACC in fold#0 was 0.827


Balanced ACC in fold#0 was 0.822


MCC in fold#0 was 0.644


Confusion Matrix in fold#0: 
           nonRipple   Ripple
nonRipple     734414   193067
Ripple        191509  1106003


Classification Report in fold#0: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.793        0.851  ...        0.822         0.827
recall            0.792        0.852  ...        0.822         0.827
f1-score          0.793        0.852  ...        0.822         0.827
sample size  927481.000  1297512.000  ...  2224993.000   2224993.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/1632224 (0.1%)]	Loss: 0.366003
Train Epoch: 1 [52224/1632224 (3.2%)]	Loss: 0.207916
Train Epoch: 1 [103424/1632224 (6.3%)]	Loss: 0.173933
Train Epoch: 1 [154624/1632224 (9.5%)]	Loss: 0.169496
Train Epoch: 1 [205824/1632224 (12.6%)]	Loss: 0.167017
Train Epoch: 1 [257024/1632224 (15.7%)]	Loss: 0.175083
Train Epoch: 1 [308224/1632224 (18.9%)]	Loss: 0.165560
Train Epoch: 1 [359424/1632224 (22.0%)]	Loss: 0.146969
Train Epoch: 1 [410624/1632224 (25.2%)]	Loss: 0.175195
Train Epoch: 1 [461824/1632224 (28.3%)]	Loss: 0.164694
Train Epoch: 1 [513024/1632224 (31.4%)]	Loss: 0.175338
Train Epoch: 1 [564224/1632224 (34.6%)]	Loss: 0.154998
Train Epoch: 1 [615424/1632224 (37.7%)]	Loss: 0.144557
Train Epoch: 1 [666624/1632224 (40.8%)]	Loss: 0.155203
Train Epoch: 1 [717824/1632224 (44.0%)]	Loss: 0.169416
Train Epoch: 1 [769024/1632224 (47.1%)]	Loss: 0.149621
Train Epoch: 1 [820224/1632224 (50.3%)]	Loss: 0.153509
Train Epoch: 1 [871424/1632224 (53.4%)]	Loss: 0.154457
Train Epoch: 1 [922624/1632224 (56.5%)]	Loss: 0.154336
Train Epoch: 1 [973824/1632224 (59.7%)]	Loss: 0.149608
Train Epoch: 1 [1025024/1632224 (62.8%)]	Loss: 0.155742
Train Epoch: 1 [1076224/1632224 (65.9%)]	Loss: 0.144556
Train Epoch: 1 [1127424/1632224 (69.1%)]	Loss: 0.154463
Train Epoch: 1 [1178624/1632224 (72.2%)]	Loss: 0.155868
Train Epoch: 1 [1229824/1632224 (75.3%)]	Loss: 0.144913
Train Epoch: 1 [1281024/1632224 (78.5%)]	Loss: 0.169645
Train Epoch: 1 [1332224/1632224 (81.6%)]	Loss: 0.150600
Train Epoch: 1 [1383424/1632224 (84.8%)]	Loss: 0.150679
Train Epoch: 1 [1434624/1632224 (87.9%)]	Loss: 0.147372
Train Epoch: 1 [1485824/1632224 (91.0%)]	Loss: 0.162103
Train Epoch: 1 [1537024/1632224 (94.2%)]	Loss: 0.165611
Train Epoch: 1 [1588224/1632224 (97.3%)]	Loss: 0.162618
Train Epoch: 2 [1024/1632224 (0.1%)]	Loss: 0.143566
Train Epoch: 2 [52224/1632224 (3.2%)]	Loss: 0.153924
Train Epoch: 2 [103424/1632224 (6.3%)]	Loss: 0.168171
Train Epoch: 2 [154624/1632224 (9.5%)]	Loss: 0.146831
Train Epoch: 2 [205824/1632224 (12.6%)]	Loss: 0.158415
Train Epoch: 2 [257024/1632224 (15.7%)]	Loss: 0.164827
Train Epoch: 2 [308224/1632224 (18.9%)]	Loss: 0.140934
Train Epoch: 2 [359424/1632224 (22.0%)]	Loss: 0.169888
Train Epoch: 2 [410624/1632224 (25.2%)]	Loss: 0.178424
Train Epoch: 2 [461824/1632224 (28.3%)]	Loss: 0.162486
Train Epoch: 2 [513024/1632224 (31.4%)]	Loss: 0.160749
Train Epoch: 2 [564224/1632224 (34.6%)]	Loss: 0.152376
Train Epoch: 2 [615424/1632224 (37.7%)]	Loss: 0.166605
Train Epoch: 2 [666624/1632224 (40.8%)]	Loss: 0.149615
Train Epoch: 2 [717824/1632224 (44.0%)]	Loss: 0.158421
Train Epoch: 2 [769024/1632224 (47.1%)]	Loss: 0.137680
Train Epoch: 2 [820224/1632224 (50.3%)]	Loss: 0.172831
Train Epoch: 2 [871424/1632224 (53.4%)]	Loss: 0.153434
Train Epoch: 2 [922624/1632224 (56.5%)]	Loss: 0.148147
Train Epoch: 2 [973824/1632224 (59.7%)]	Loss: 0.151233
Train Epoch: 2 [1025024/1632224 (62.8%)]	Loss: 0.143473
Train Epoch: 2 [1076224/1632224 (65.9%)]	Loss: 0.155776
Train Epoch: 2 [1127424/1632224 (69.1%)]	Loss: 0.140724
Train Epoch: 2 [1178624/1632224 (72.2%)]	Loss: 0.139970
Train Epoch: 2 [1229824/1632224 (75.3%)]	Loss: 0.146807
Train Epoch: 2 [1281024/1632224 (78.5%)]	Loss: 0.153123
Train Epoch: 2 [1332224/1632224 (81.6%)]	Loss: 0.151997
Train Epoch: 2 [1383424/1632224 (84.8%)]	Loss: 0.159525
Train Epoch: 2 [1434624/1632224 (87.9%)]	Loss: 0.142167
Train Epoch: 2 [1485824/1632224 (91.0%)]	Loss: 0.148147
Train Epoch: 2 [1537024/1632224 (94.2%)]	Loss: 0.153446
Train Epoch: 2 [1588224/1632224 (97.3%)]	Loss: 0.146259

ACC in fold#1 was 0.849


Balanced ACC in fold#1 was 0.844


MCC in fold#1 was 0.686


Confusion Matrix in fold#1: 
           nonRipple   Ripple
nonRipple     715515   161730
Ripple        173177  1174571


Classification Report in fold#1: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.805        0.879  ...        0.842         0.850
recall            0.816        0.872  ...        0.844         0.849
f1-score          0.810        0.875  ...        0.843         0.850
sample size  877245.000  1347748.000  ...  2224993.000   2224993.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/1619912 (0.1%)]	Loss: 0.347211
Train Epoch: 1 [52224/1619912 (3.2%)]	Loss: 0.207365
Train Epoch: 1 [103424/1619912 (6.4%)]	Loss: 0.164845
Train Epoch: 1 [154624/1619912 (9.5%)]	Loss: 0.174292
Train Epoch: 1 [205824/1619912 (12.7%)]	Loss: 0.145704
Train Epoch: 1 [257024/1619912 (15.9%)]	Loss: 0.153467
Train Epoch: 1 [308224/1619912 (19.0%)]	Loss: 0.147438
Train Epoch: 1 [359424/1619912 (22.2%)]	Loss: 0.161464
Train Epoch: 1 [410624/1619912 (25.3%)]	Loss: 0.160292
Train Epoch: 1 [461824/1619912 (28.5%)]	Loss: 0.158435
Train Epoch: 1 [513024/1619912 (31.7%)]	Loss: 0.157972
Train Epoch: 1 [564224/1619912 (34.8%)]	Loss: 0.157695
Train Epoch: 1 [615424/1619912 (38.0%)]	Loss: 0.157876
Train Epoch: 1 [666624/1619912 (41.2%)]	Loss: 0.145582
Train Epoch: 1 [717824/1619912 (44.3%)]	Loss: 0.160140
Train Epoch: 1 [769024/1619912 (47.5%)]	Loss: 0.155485
Train Epoch: 1 [820224/1619912 (50.6%)]	Loss: 0.155307
Train Epoch: 1 [871424/1619912 (53.8%)]	Loss: 0.151673
Train Epoch: 1 [922624/1619912 (57.0%)]	Loss: 0.167808
Train Epoch: 1 [973824/1619912 (60.1%)]	Loss: 0.162927
Train Epoch: 1 [1025024/1619912 (63.3%)]	Loss: 0.157960
Train Epoch: 1 [1076224/1619912 (66.4%)]	Loss: 0.158054
Train Epoch: 1 [1127424/1619912 (69.6%)]	Loss: 0.147763
Train Epoch: 1 [1178624/1619912 (72.8%)]	Loss: 0.165977
Train Epoch: 1 [1229824/1619912 (75.9%)]	Loss: 0.149380
Train Epoch: 1 [1281024/1619912 (79.1%)]	Loss: 0.155646
Train Epoch: 1 [1332224/1619912 (82.2%)]	Loss: 0.168102
Train Epoch: 1 [1383424/1619912 (85.4%)]	Loss: 0.152680
Train Epoch: 1 [1434624/1619912 (88.6%)]	Loss: 0.158450
Train Epoch: 1 [1485824/1619912 (91.7%)]	Loss: 0.145516
Train Epoch: 1 [1537024/1619912 (94.9%)]	Loss: 0.161586
Train Epoch: 1 [1588224/1619912 (98.0%)]	Loss: 0.149699
Train Epoch: 2 [1024/1619912 (0.1%)]	Loss: 0.148461
Train Epoch: 2 [52224/1619912 (3.2%)]	Loss: 0.156462
Train Epoch: 2 [103424/1619912 (6.4%)]	Loss: 0.157458
Train Epoch: 2 [154624/1619912 (9.5%)]	Loss: 0.138670
Train Epoch: 2 [205824/1619912 (12.7%)]	Loss: 0.155848
Train Epoch: 2 [257024/1619912 (15.9%)]	Loss: 0.159084
Train Epoch: 2 [308224/1619912 (19.0%)]	Loss: 0.151782
Train Epoch: 2 [359424/1619912 (22.2%)]	Loss: 0.165253
Train Epoch: 2 [410624/1619912 (25.3%)]	Loss: 0.151630
Train Epoch: 2 [461824/1619912 (28.5%)]	Loss: 0.146745
Train Epoch: 2 [513024/1619912 (31.7%)]	Loss: 0.148651
Train Epoch: 2 [564224/1619912 (34.8%)]	Loss: 0.139004
Train Epoch: 2 [615424/1619912 (38.0%)]	Loss: 0.153934
Train Epoch: 2 [666624/1619912 (41.2%)]	Loss: 0.152815
Train Epoch: 2 [717824/1619912 (44.3%)]	Loss: 0.153770
Train Epoch: 2 [769024/1619912 (47.5%)]	Loss: 0.139383
Train Epoch: 2 [820224/1619912 (50.6%)]	Loss: 0.151502
Train Epoch: 2 [871424/1619912 (53.8%)]	Loss: 0.144617
Train Epoch: 2 [922624/1619912 (57.0%)]	Loss: 0.148744
Train Epoch: 2 [973824/1619912 (60.1%)]	Loss: 0.151340
Train Epoch: 2 [1025024/1619912 (63.3%)]	Loss: 0.151168
Train Epoch: 2 [1076224/1619912 (66.4%)]	Loss: 0.142412
Train Epoch: 2 [1127424/1619912 (69.6%)]	Loss: 0.155328
Train Epoch: 2 [1178624/1619912 (72.8%)]	Loss: 0.149431
Train Epoch: 2 [1229824/1619912 (75.9%)]	Loss: 0.145351
Train Epoch: 2 [1281024/1619912 (79.1%)]	Loss: 0.156850
Train Epoch: 2 [1332224/1619912 (82.2%)]	Loss: 0.156425
Train Epoch: 2 [1383424/1619912 (85.4%)]	Loss: 0.153529
Train Epoch: 2 [1434624/1619912 (88.6%)]	Loss: 0.150894
Train Epoch: 2 [1485824/1619912 (91.7%)]	Loss: 0.130449
Train Epoch: 2 [1537024/1619912 (94.9%)]	Loss: 0.144794
Train Epoch: 2 [1588224/1619912 (98.0%)]	Loss: 0.163583

ACC in fold#2 was 0.824


Balanced ACC in fold#2 was 0.812


MCC in fold#2 was 0.629


Confusion Matrix in fold#2: 
           nonRipple   Ripple
nonRipple     660453   213654
Ripple        177499  1173386


Classification Report in fold#2: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.788        0.846  ...        0.817         0.823
recall            0.756        0.869  ...        0.812         0.824
f1-score          0.772        0.857  ...        0.814         0.824
sample size  874107.000  1350885.000  ...  2224992.000   2224992.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/1547064 (0.1%)]	Loss: 0.361833
Train Epoch: 1 [52224/1547064 (3.4%)]	Loss: 0.210985
Train Epoch: 1 [103424/1547064 (6.7%)]	Loss: 0.168256
Train Epoch: 1 [154624/1547064 (10.0%)]	Loss: 0.169034
Train Epoch: 1 [205824/1547064 (13.3%)]	Loss: 0.140044
Train Epoch: 1 [257024/1547064 (16.6%)]	Loss: 0.160423
Train Epoch: 1 [308224/1547064 (19.9%)]	Loss: 0.144143
Train Epoch: 1 [359424/1547064 (23.2%)]	Loss: 0.159518
Train Epoch: 1 [410624/1547064 (26.5%)]	Loss: 0.169410
Train Epoch: 1 [461824/1547064 (29.9%)]	Loss: 0.155879
Train Epoch: 1 [513024/1547064 (33.2%)]	Loss: 0.149840
Train Epoch: 1 [564224/1547064 (36.5%)]	Loss: 0.143414
Train Epoch: 1 [615424/1547064 (39.8%)]	Loss: 0.158666
Train Epoch: 1 [666624/1547064 (43.1%)]	Loss: 0.147115
Train Epoch: 1 [717824/1547064 (46.4%)]	Loss: 0.141605
Train Epoch: 1 [769024/1547064 (49.7%)]	Loss: 0.166023
Train Epoch: 1 [820224/1547064 (53.0%)]	Loss: 0.159193
Train Epoch: 1 [871424/1547064 (56.3%)]	Loss: 0.146979
Train Epoch: 1 [922624/1547064 (59.6%)]	Loss: 0.152181
Train Epoch: 1 [973824/1547064 (62.9%)]	Loss: 0.157672
Train Epoch: 1 [1025024/1547064 (66.3%)]	Loss: 0.158211
Train Epoch: 1 [1076224/1547064 (69.6%)]	Loss: 0.138120
Train Epoch: 1 [1127424/1547064 (72.9%)]	Loss: 0.150870
Train Epoch: 1 [1178624/1547064 (76.2%)]	Loss: 0.156982
Train Epoch: 1 [1229824/1547064 (79.5%)]	Loss: 0.141030
Train Epoch: 1 [1281024/1547064 (82.8%)]	Loss: 0.145452
Train Epoch: 1 [1332224/1547064 (86.1%)]	Loss: 0.149791
Train Epoch: 1 [1383424/1547064 (89.4%)]	Loss: 0.144123
Train Epoch: 1 [1434624/1547064 (92.7%)]	Loss: 0.144512
Train Epoch: 1 [1485824/1547064 (96.0%)]	Loss: 0.137041
Train Epoch: 1 [1537024/1547064 (99.4%)]	Loss: 0.139579
Train Epoch: 2 [1024/1547064 (0.1%)]	Loss: 0.151360
Train Epoch: 2 [52224/1547064 (3.4%)]	Loss: 0.164588
Train Epoch: 2 [103424/1547064 (6.7%)]	Loss: 0.142976
Train Epoch: 2 [154624/1547064 (10.0%)]	Loss: 0.156505
Train Epoch: 2 [205824/1547064 (13.3%)]	Loss: 0.146787
Train Epoch: 2 [257024/1547064 (16.6%)]	Loss: 0.143097
Train Epoch: 2 [308224/1547064 (19.9%)]	Loss: 0.165520
Train Epoch: 2 [359424/1547064 (23.2%)]	Loss: 0.142204
Train Epoch: 2 [410624/1547064 (26.5%)]	Loss: 0.146790
Train Epoch: 2 [461824/1547064 (29.9%)]	Loss: 0.155210
Train Epoch: 2 [513024/1547064 (33.2%)]	Loss: 0.141309
Train Epoch: 2 [564224/1547064 (36.5%)]	Loss: 0.150585
Train Epoch: 2 [615424/1547064 (39.8%)]	Loss: 0.146432
Train Epoch: 2 [666624/1547064 (43.1%)]	Loss: 0.154215
Train Epoch: 2 [717824/1547064 (46.4%)]	Loss: 0.161626
Train Epoch: 2 [769024/1547064 (49.7%)]	Loss: 0.149969
Train Epoch: 2 [820224/1547064 (53.0%)]	Loss: 0.142149
Train Epoch: 2 [871424/1547064 (56.3%)]	Loss: 0.148104
Train Epoch: 2 [922624/1547064 (59.6%)]	Loss: 0.147893
Train Epoch: 2 [973824/1547064 (62.9%)]	Loss: 0.156107
Train Epoch: 2 [1025024/1547064 (66.3%)]	Loss: 0.152787
Train Epoch: 2 [1076224/1547064 (69.6%)]	Loss: 0.144109
Train Epoch: 2 [1127424/1547064 (72.9%)]	Loss: 0.149048
Train Epoch: 2 [1178624/1547064 (76.2%)]	Loss: 0.146121
Train Epoch: 2 [1229824/1547064 (79.5%)]	Loss: 0.149467
Train Epoch: 2 [1281024/1547064 (82.8%)]	Loss: 0.154023
Train Epoch: 2 [1332224/1547064 (86.1%)]	Loss: 0.161687
Train Epoch: 2 [1383424/1547064 (89.4%)]	Loss: 0.146292
Train Epoch: 2 [1434624/1547064 (92.7%)]	Loss: 0.149800
Train Epoch: 2 [1485824/1547064 (96.0%)]	Loss: 0.139888
Train Epoch: 2 [1537024/1547064 (99.4%)]	Loss: 0.150582

ACC in fold#3 was 0.840


Balanced ACC in fold#3 was 0.834


MCC in fold#3 was 0.659


Confusion Matrix in fold#3: 
           nonRipple   Ripple
nonRipple     652365   151570
Ripple        204453  1216604


Classification Report in fold#3: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.761        0.889  ...        0.825         0.843
recall            0.811        0.856  ...        0.834         0.840
f1-score          0.786        0.872  ...        0.829         0.841
sample size  803935.000  1421057.000  ...  2224992.000   2224992.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/1541032 (0.1%)]	Loss: 0.366215
Train Epoch: 1 [52224/1541032 (3.4%)]	Loss: 0.207071
Train Epoch: 1 [103424/1541032 (6.7%)]	Loss: 0.177566
Train Epoch: 1 [154624/1541032 (10.0%)]	Loss: 0.164347
Train Epoch: 1 [205824/1541032 (13.4%)]	Loss: 0.159801
Train Epoch: 1 [257024/1541032 (16.7%)]	Loss: 0.167699
Train Epoch: 1 [308224/1541032 (20.0%)]	Loss: 0.143570
Train Epoch: 1 [359424/1541032 (23.3%)]	Loss: 0.171641
Train Epoch: 1 [410624/1541032 (26.6%)]	Loss: 0.165024
Train Epoch: 1 [461824/1541032 (30.0%)]	Loss: 0.164913
Train Epoch: 1 [513024/1541032 (33.3%)]	Loss: 0.153840
Train Epoch: 1 [564224/1541032 (36.6%)]	Loss: 0.146079
Train Epoch: 1 [615424/1541032 (39.9%)]	Loss: 0.140396
Train Epoch: 1 [666624/1541032 (43.3%)]	Loss: 0.161011
Train Epoch: 1 [717824/1541032 (46.6%)]	Loss: 0.149937
Train Epoch: 1 [769024/1541032 (49.9%)]	Loss: 0.153992
Train Epoch: 1 [820224/1541032 (53.2%)]	Loss: 0.151979
Train Epoch: 1 [871424/1541032 (56.5%)]	Loss: 0.173039
Train Epoch: 1 [922624/1541032 (59.9%)]	Loss: 0.172468
Train Epoch: 1 [973824/1541032 (63.2%)]	Loss: 0.156724
Train Epoch: 1 [1025024/1541032 (66.5%)]	Loss: 0.145553
Train Epoch: 1 [1076224/1541032 (69.8%)]	Loss: 0.161936
Train Epoch: 1 [1127424/1541032 (73.2%)]	Loss: 0.155910
Train Epoch: 1 [1178624/1541032 (76.5%)]	Loss: 0.153344
Train Epoch: 1 [1229824/1541032 (79.8%)]	Loss: 0.137100
Train Epoch: 1 [1281024/1541032 (83.1%)]	Loss: 0.159729
Train Epoch: 1 [1332224/1541032 (86.5%)]	Loss: 0.163922
Train Epoch: 1 [1383424/1541032 (89.8%)]	Loss: 0.151634
Train Epoch: 1 [1434624/1541032 (93.1%)]	Loss: 0.148380
Train Epoch: 1 [1485824/1541032 (96.4%)]	Loss: 0.161027
Train Epoch: 1 [1537024/1541032 (99.7%)]	Loss: 0.154171
Train Epoch: 2 [1024/1541032 (0.1%)]	Loss: 0.154853
Train Epoch: 2 [52224/1541032 (3.4%)]	Loss: 0.148413
Train Epoch: 2 [103424/1541032 (6.7%)]	Loss: 0.166685
Train Epoch: 2 [154624/1541032 (10.0%)]	Loss: 0.141963
Train Epoch: 2 [205824/1541032 (13.4%)]	Loss: 0.146945
Train Epoch: 2 [257024/1541032 (16.7%)]	Loss: 0.146074
Train Epoch: 2 [308224/1541032 (20.0%)]	Loss: 0.155907
Train Epoch: 2 [359424/1541032 (23.3%)]	Loss: 0.131428
Train Epoch: 2 [410624/1541032 (26.6%)]	Loss: 0.146480
Train Epoch: 2 [461824/1541032 (30.0%)]	Loss: 0.153117
Train Epoch: 2 [513024/1541032 (33.3%)]	Loss: 0.148852
Train Epoch: 2 [564224/1541032 (36.6%)]	Loss: 0.145467
Train Epoch: 2 [615424/1541032 (39.9%)]	Loss: 0.143640
Train Epoch: 2 [666624/1541032 (43.3%)]	Loss: 0.157897
Train Epoch: 2 [717824/1541032 (46.6%)]	Loss: 0.144980
Train Epoch: 2 [769024/1541032 (49.9%)]	Loss: 0.147595
Train Epoch: 2 [820224/1541032 (53.2%)]	Loss: 0.152698
Train Epoch: 2 [871424/1541032 (56.5%)]	Loss: 0.135427
Train Epoch: 2 [922624/1541032 (59.9%)]	Loss: 0.158973
Train Epoch: 2 [973824/1541032 (63.2%)]	Loss: 0.152532
Train Epoch: 2 [1025024/1541032 (66.5%)]	Loss: 0.158373
Train Epoch: 2 [1076224/1541032 (69.8%)]	Loss: 0.149756
Train Epoch: 2 [1127424/1541032 (73.2%)]	Loss: 0.156590
Train Epoch: 2 [1178624/1541032 (76.5%)]	Loss: 0.150830
Train Epoch: 2 [1229824/1541032 (79.8%)]	Loss: 0.151159
Train Epoch: 2 [1281024/1541032 (83.1%)]	Loss: 0.135339
Train Epoch: 2 [1332224/1541032 (86.5%)]	Loss: 0.143058
Train Epoch: 2 [1383424/1541032 (89.8%)]	Loss: 0.155789
Train Epoch: 2 [1434624/1541032 (93.1%)]	Loss: 0.137724
Train Epoch: 2 [1485824/1541032 (96.4%)]	Loss: 0.156567
Train Epoch: 2 [1537024/1541032 (99.7%)]	Loss: 0.147203

ACC in fold#4 was 0.787


Balanced ACC in fold#4 was 0.797


MCC in fold#4 was 0.578


Confusion Matrix in fold#4: 
           nonRipple   Ripple
nonRipple     708513   138691
Ripple        334499  1043289


Classification Report in fold#4: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.679        0.883  ...        0.781         0.805
recall            0.836        0.757  ...        0.797         0.787
f1-score          0.750        0.815  ...        0.782         0.790
sample size  847204.000  1377788.000  ...  2224992.000   2224992.000

[4 rows x 5 columns]


Label Errors Rate:
0.078


 --- 5-fold CV overall metrics --- 


The Mattews correlation coefficient: 0.639 +/- 0.036 (mean +/- std.; n=5)


Balanced Accuracy Score: 0.822 +/- 0.016 (mean +/- std.; n=5)


Confusion Matrix (Test; sum; num. folds=5)
           nonRipple   Ripple
nonRipple    3471260   858712
Ripple       1081137  5713853


Classification Report (Test; mean; num. folds=5)
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.765        0.870  ...        0.817         0.830
recall            0.802        0.841  ...        0.822         0.825
f1-score          0.782        0.854  ...        0.818         0.826
sample size  865994.400  1358998.000  ...  2224992.400   2224992.400

[4 rows x 5 columns]


Classification Report (Test; std; num. folds=5)
             nonRipple     Ripple  balanced accuracy  macro avg  weighted avg
precision        0.045      0.018              0.016      0.020         0.016
recall           0.027      0.043              0.016      0.016         0.021
f1-score         0.020      0.021              0.016      0.020         0.020
sample size  40441.168  40440.809              0.016      0.490         0.490


ROC AUC micro Score: 0.916 +/- 0.014 (mean +/- std.; n=5)


ROC AUC macro Score: 0.91 +/- 0.012 (mean +/- std.; n=5)


Precision-Recall AUC micro Score: 0.918 +/- 0.013 (mean +/- std.; n=5)


Precision-Recall AUC macro Score: 0.909 +/- 0.012 (mean +/- std.; n=5)


Saved to: ./data/okada/cleanlab_results/D03-/are_errors.npy


Saved to: ./data/okada/cleanlab_results/D03-/are_ripple_GMM.npy


Saved to: ./data/okada/cleanlab_results/D03-/psx_ripple.npy


Saved to: ./data/okada/cleanlab_results/D03-/mccs.csv


Saved to: ./data/okada/cleanlab_results/D03-/balanced_accs.csv


Saved to: ./data/okada/cleanlab_results/D03-/conf_mat/fold#0.png


Saved to: ./data/okada/cleanlab_results/D03-/conf_mat/fold#1.png


Saved to: ./data/okada/cleanlab_results/D03-/conf_mat/fold#2.png


Saved to: ./data/okada/cleanlab_results/D03-/conf_mat/fold#3.png


Saved to: ./data/okada/cleanlab_results/D03-/conf_mat/fold#4.png


Saved to: ./data/okada/cleanlab_results/D03-/conf_mat/conf_mats.csv


Saved to: ./data/okada/cleanlab_results/D03-/conf_mat/overall_sum.png


Saved to: ./data/okada/cleanlab_results/D03-/clf_reports.csv


Saved to: ./data/okada/cleanlab_results/D03-/aucs.csv


Saved to: ./data/okada/cleanlab_results/D03-/roc_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D03-/roc_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D03-/roc_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D03-/roc_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D03-/roc_curves/fold#4.png


Saved to: ./data/okada/cleanlab_results/D03-/pr_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D03-/pr_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D03-/pr_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D03-/pr_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D03-/pr_curves/fold#4.png


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/01/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt7-4_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D03-/tt6-4_fp16.pkl

