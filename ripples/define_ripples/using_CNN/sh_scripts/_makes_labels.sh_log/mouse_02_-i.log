
Random seeds have been fixed as 42


Random seeds have been fixed as 42

Indice of mice to load: ['02']
48
Time (id:0): tot 00:00:00, prev 00:00:00 [hh:mm:ss]: Reporter has been initialized.

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/3084170 (0.0%)]	Loss: 0.351748
Train Epoch: 1 [52224/3084170 (1.7%)]	Loss: 0.197817
Train Epoch: 1 [103424/3084170 (3.4%)]	Loss: 0.148103
Train Epoch: 1 [154624/3084170 (5.0%)]	Loss: 0.133241
Train Epoch: 1 [205824/3084170 (6.7%)]	Loss: 0.135669
Train Epoch: 1 [257024/3084170 (8.3%)]	Loss: 0.135104
Train Epoch: 1 [308224/3084170 (10.0%)]	Loss: 0.131599
Train Epoch: 1 [359424/3084170 (11.7%)]	Loss: 0.125824
Train Epoch: 1 [410624/3084170 (13.3%)]	Loss: 0.123511
Train Epoch: 1 [461824/3084170 (15.0%)]	Loss: 0.137061
Train Epoch: 1 [513024/3084170 (16.6%)]	Loss: 0.141711
Train Epoch: 1 [564224/3084170 (18.3%)]	Loss: 0.137632
Train Epoch: 1 [615424/3084170 (20.0%)]	Loss: 0.136751
Train Epoch: 1 [666624/3084170 (21.6%)]	Loss: 0.135967
Train Epoch: 1 [717824/3084170 (23.3%)]	Loss: 0.145262
Train Epoch: 1 [769024/3084170 (24.9%)]	Loss: 0.137553
Train Epoch: 1 [820224/3084170 (26.6%)]	Loss: 0.130159
Train Epoch: 1 [871424/3084170 (28.3%)]	Loss: 0.129751
Train Epoch: 1 [922624/3084170 (29.9%)]	Loss: 0.137592
Train Epoch: 1 [973824/3084170 (31.6%)]	Loss: 0.119828
Train Epoch: 1 [1025024/3084170 (33.2%)]	Loss: 0.137619
Train Epoch: 1 [1076224/3084170 (34.9%)]	Loss: 0.128510
Train Epoch: 1 [1127424/3084170 (36.6%)]	Loss: 0.116602
Train Epoch: 1 [1178624/3084170 (38.2%)]	Loss: 0.120095
Train Epoch: 1 [1229824/3084170 (39.9%)]	Loss: 0.129215
Train Epoch: 1 [1281024/3084170 (41.5%)]	Loss: 0.125312
Train Epoch: 1 [1332224/3084170 (43.2%)]	Loss: 0.119917
Train Epoch: 1 [1383424/3084170 (44.9%)]	Loss: 0.134750
Train Epoch: 1 [1434624/3084170 (46.5%)]	Loss: 0.110803
Train Epoch: 1 [1485824/3084170 (48.2%)]	Loss: 0.139682
Train Epoch: 1 [1537024/3084170 (49.8%)]	Loss: 0.122720
Train Epoch: 1 [1588224/3084170 (51.5%)]	Loss: 0.142702
Train Epoch: 1 [1639424/3084170 (53.2%)]	Loss: 0.135159
Train Epoch: 1 [1690624/3084170 (54.8%)]	Loss: 0.147603
Train Epoch: 1 [1741824/3084170 (56.5%)]	Loss: 0.119467
Train Epoch: 1 [1793024/3084170 (58.1%)]	Loss: 0.133425
Train Epoch: 1 [1844224/3084170 (59.8%)]	Loss: 0.131152
Train Epoch: 1 [1895424/3084170 (61.5%)]	Loss: 0.140676
Train Epoch: 1 [1946624/3084170 (63.1%)]	Loss: 0.136539
Train Epoch: 1 [1997824/3084170 (64.8%)]	Loss: 0.146892
Train Epoch: 1 [2049024/3084170 (66.4%)]	Loss: 0.120033
Train Epoch: 1 [2100224/3084170 (68.1%)]	Loss: 0.140425
Train Epoch: 1 [2151424/3084170 (69.8%)]	Loss: 0.125030
Train Epoch: 1 [2202624/3084170 (71.4%)]	Loss: 0.123772
Train Epoch: 1 [2253824/3084170 (73.1%)]	Loss: 0.125793
Train Epoch: 1 [2305024/3084170 (74.7%)]	Loss: 0.128216
Train Epoch: 1 [2356224/3084170 (76.4%)]	Loss: 0.131471
Train Epoch: 1 [2407424/3084170 (78.1%)]	Loss: 0.128136
Train Epoch: 1 [2458624/3084170 (79.7%)]	Loss: 0.112557
Train Epoch: 1 [2509824/3084170 (81.4%)]	Loss: 0.140034
Train Epoch: 1 [2561024/3084170 (83.0%)]	Loss: 0.123691
Train Epoch: 1 [2612224/3084170 (84.7%)]	Loss: 0.145868
Train Epoch: 1 [2663424/3084170 (86.4%)]	Loss: 0.124110
Train Epoch: 1 [2714624/3084170 (88.0%)]	Loss: 0.132788
Train Epoch: 1 [2765824/3084170 (89.7%)]	Loss: 0.120997
Train Epoch: 1 [2817024/3084170 (91.3%)]	Loss: 0.113996
Train Epoch: 1 [2868224/3084170 (93.0%)]	Loss: 0.126841
Train Epoch: 1 [2919424/3084170 (94.7%)]	Loss: 0.118645
Train Epoch: 1 [2970624/3084170 (96.3%)]	Loss: 0.141065
Train Epoch: 1 [3021824/3084170 (98.0%)]	Loss: 0.121623
Train Epoch: 1 [3073024/3084170 (99.6%)]	Loss: 0.120704
Train Epoch: 2 [1024/3084170 (0.0%)]	Loss: 0.105553
Train Epoch: 2 [52224/3084170 (1.7%)]	Loss: 0.123602
Train Epoch: 2 [103424/3084170 (3.4%)]	Loss: 0.133150
Train Epoch: 2 [154624/3084170 (5.0%)]	Loss: 0.118357
Train Epoch: 2 [205824/3084170 (6.7%)]	Loss: 0.134259
Train Epoch: 2 [257024/3084170 (8.3%)]	Loss: 0.141021
Train Epoch: 2 [308224/3084170 (10.0%)]	Loss: 0.135390
Train Epoch: 2 [359424/3084170 (11.7%)]	Loss: 0.121478
Train Epoch: 2 [410624/3084170 (13.3%)]	Loss: 0.144713
Train Epoch: 2 [461824/3084170 (15.0%)]	Loss: 0.139365
Train Epoch: 2 [513024/3084170 (16.6%)]	Loss: 0.128153
Train Epoch: 2 [564224/3084170 (18.3%)]	Loss: 0.128934
Train Epoch: 2 [615424/3084170 (20.0%)]	Loss: 0.130322
Train Epoch: 2 [666624/3084170 (21.6%)]	Loss: 0.147874
Train Epoch: 2 [717824/3084170 (23.3%)]	Loss: 0.130629
Train Epoch: 2 [769024/3084170 (24.9%)]	Loss: 0.132761
Train Epoch: 2 [820224/3084170 (26.6%)]	Loss: 0.130263
Train Epoch: 2 [871424/3084170 (28.3%)]	Loss: 0.120849
Train Epoch: 2 [922624/3084170 (29.9%)]	Loss: 0.126815
Train Epoch: 2 [973824/3084170 (31.6%)]	Loss: 0.137926
Train Epoch: 2 [1025024/3084170 (33.2%)]	Loss: 0.131143
Train Epoch: 2 [1076224/3084170 (34.9%)]	Loss: 0.126503
Train Epoch: 2 [1127424/3084170 (36.6%)]	Loss: 0.129374
Train Epoch: 2 [1178624/3084170 (38.2%)]	Loss: 0.115705
Train Epoch: 2 [1229824/3084170 (39.9%)]	Loss: 0.124810
Train Epoch: 2 [1281024/3084170 (41.5%)]	Loss: 0.114153
Train Epoch: 2 [1332224/3084170 (43.2%)]	Loss: 0.120432
Train Epoch: 2 [1383424/3084170 (44.9%)]	Loss: 0.115327
Train Epoch: 2 [1434624/3084170 (46.5%)]	Loss: 0.139457
Train Epoch: 2 [1485824/3084170 (48.2%)]	Loss: 0.139042
Train Epoch: 2 [1537024/3084170 (49.8%)]	Loss: 0.129874
Train Epoch: 2 [1588224/3084170 (51.5%)]	Loss: 0.125815
Train Epoch: 2 [1639424/3084170 (53.2%)]	Loss: 0.127944
Train Epoch: 2 [1690624/3084170 (54.8%)]	Loss: 0.114975
Train Epoch: 2 [1741824/3084170 (56.5%)]	Loss: 0.116377
Train Epoch: 2 [1793024/3084170 (58.1%)]	Loss: 0.119322
Train Epoch: 2 [1844224/3084170 (59.8%)]	Loss: 0.124605
Train Epoch: 2 [1895424/3084170 (61.5%)]	Loss: 0.143743
Train Epoch: 2 [1946624/3084170 (63.1%)]	Loss: 0.122574
Train Epoch: 2 [1997824/3084170 (64.8%)]	Loss: 0.134029
Train Epoch: 2 [2049024/3084170 (66.4%)]	Loss: 0.105901
Train Epoch: 2 [2100224/3084170 (68.1%)]	Loss: 0.116609
Train Epoch: 2 [2151424/3084170 (69.8%)]	Loss: 0.113691
Train Epoch: 2 [2202624/3084170 (71.4%)]	Loss: 0.129940
Train Epoch: 2 [2253824/3084170 (73.1%)]	Loss: 0.121958
Train Epoch: 2 [2305024/3084170 (74.7%)]	Loss: 0.134524
Train Epoch: 2 [2356224/3084170 (76.4%)]	Loss: 0.123817
Train Epoch: 2 [2407424/3084170 (78.1%)]	Loss: 0.111163
Train Epoch: 2 [2458624/3084170 (79.7%)]	Loss: 0.144857
Train Epoch: 2 [2509824/3084170 (81.4%)]	Loss: 0.131324
Train Epoch: 2 [2561024/3084170 (83.0%)]	Loss: 0.121525
Train Epoch: 2 [2612224/3084170 (84.7%)]	Loss: 0.134563
Train Epoch: 2 [2663424/3084170 (86.4%)]	Loss: 0.118849
Train Epoch: 2 [2714624/3084170 (88.0%)]	Loss: 0.111416
Train Epoch: 2 [2765824/3084170 (89.7%)]	Loss: 0.121170
Train Epoch: 2 [2817024/3084170 (91.3%)]	Loss: 0.111490
Train Epoch: 2 [2868224/3084170 (93.0%)]	Loss: 0.136500
Train Epoch: 2 [2919424/3084170 (94.7%)]	Loss: 0.135037
Train Epoch: 2 [2970624/3084170 (96.3%)]	Loss: 0.132889
Train Epoch: 2 [3021824/3084170 (98.0%)]	Loss: 0.116552
Train Epoch: 2 [3073024/3084170 (99.6%)]	Loss: 0.128134

 ---------------------------------------------------------------------- 


ACC in fold#0 was 0.899


Confusion Matrix in fold#0: 
           nonRipple  Ripple
nonRipple     247069   61742
Ripple         16366  445866


Classification Report in fold#0: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.938       0.878     0.899       0.908         0.902
recall            0.800       0.965     0.899       0.882         0.899
f1-score          0.864       0.919     0.899       0.891         0.897
sample size  308811.000  462232.000     0.899  771043.000    771043.000


PR_AUC in fold#0 was 0.959


ROC_AUC in fold#0 was 0.955

Time (id:1): tot 01:19:43, prev 01:19:43 [hh:mm:ss]: 
i_fold=0 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/3084170 (0.0%)]	Loss: 0.370321
Train Epoch: 1 [52224/3084170 (1.7%)]	Loss: 0.213662
Train Epoch: 1 [103424/3084170 (3.4%)]	Loss: 0.145616
Train Epoch: 1 [154624/3084170 (5.0%)]	Loss: 0.152813
Train Epoch: 1 [205824/3084170 (6.7%)]	Loss: 0.139446
Train Epoch: 1 [257024/3084170 (8.3%)]	Loss: 0.142766
Train Epoch: 1 [308224/3084170 (10.0%)]	Loss: 0.151772
Train Epoch: 1 [359424/3084170 (11.7%)]	Loss: 0.132935
Train Epoch: 1 [410624/3084170 (13.3%)]	Loss: 0.143316
Train Epoch: 1 [461824/3084170 (15.0%)]	Loss: 0.133248
Train Epoch: 1 [513024/3084170 (16.6%)]	Loss: 0.140027
Train Epoch: 1 [564224/3084170 (18.3%)]	Loss: 0.149638
Train Epoch: 1 [615424/3084170 (20.0%)]	Loss: 0.153661
Train Epoch: 1 [666624/3084170 (21.6%)]	Loss: 0.142038
Train Epoch: 1 [717824/3084170 (23.3%)]	Loss: 0.146103
Train Epoch: 1 [769024/3084170 (24.9%)]	Loss: 0.114825
Train Epoch: 1 [820224/3084170 (26.6%)]	Loss: 0.125247
Train Epoch: 1 [871424/3084170 (28.3%)]	Loss: 0.138581
Train Epoch: 1 [922624/3084170 (29.9%)]	Loss: 0.136617
Train Epoch: 1 [973824/3084170 (31.6%)]	Loss: 0.126695
Train Epoch: 1 [1025024/3084170 (33.2%)]	Loss: 0.140583
Train Epoch: 1 [1076224/3084170 (34.9%)]	Loss: 0.124564
Train Epoch: 1 [1127424/3084170 (36.6%)]	Loss: 0.138587
Train Epoch: 1 [1178624/3084170 (38.2%)]	Loss: 0.126952
Train Epoch: 1 [1229824/3084170 (39.9%)]	Loss: 0.136683
Train Epoch: 1 [1281024/3084170 (41.5%)]	Loss: 0.147890
Train Epoch: 1 [1332224/3084170 (43.2%)]	Loss: 0.132077
Train Epoch: 1 [1383424/3084170 (44.9%)]	Loss: 0.133277
Train Epoch: 1 [1434624/3084170 (46.5%)]	Loss: 0.121320
Train Epoch: 1 [1485824/3084170 (48.2%)]	Loss: 0.148347
Train Epoch: 1 [1537024/3084170 (49.8%)]	Loss: 0.138197
Train Epoch: 1 [1588224/3084170 (51.5%)]	Loss: 0.120253
Train Epoch: 1 [1639424/3084170 (53.2%)]	Loss: 0.134664
Train Epoch: 1 [1690624/3084170 (54.8%)]	Loss: 0.125941
Train Epoch: 1 [1741824/3084170 (56.5%)]	Loss: 0.136189
Train Epoch: 1 [1793024/3084170 (58.1%)]	Loss: 0.128859
Train Epoch: 1 [1844224/3084170 (59.8%)]	Loss: 0.133631
Train Epoch: 1 [1895424/3084170 (61.5%)]	Loss: 0.132267
Train Epoch: 1 [1946624/3084170 (63.1%)]	Loss: 0.128324
Train Epoch: 1 [1997824/3084170 (64.8%)]	Loss: 0.142844
Train Epoch: 1 [2049024/3084170 (66.4%)]	Loss: 0.131234
Train Epoch: 1 [2100224/3084170 (68.1%)]	Loss: 0.142700
Train Epoch: 1 [2151424/3084170 (69.8%)]	Loss: 0.128872
Train Epoch: 1 [2202624/3084170 (71.4%)]	Loss: 0.140489
Train Epoch: 1 [2253824/3084170 (73.1%)]	Loss: 0.128266
Train Epoch: 1 [2305024/3084170 (74.7%)]	Loss: 0.123671
Train Epoch: 1 [2356224/3084170 (76.4%)]	Loss: 0.132665
Train Epoch: 1 [2407424/3084170 (78.1%)]	Loss: 0.144628
Train Epoch: 1 [2458624/3084170 (79.7%)]	Loss: 0.125022
Train Epoch: 1 [2509824/3084170 (81.4%)]	Loss: 0.115304
Train Epoch: 1 [2561024/3084170 (83.0%)]	Loss: 0.141020
Train Epoch: 1 [2612224/3084170 (84.7%)]	Loss: 0.121852
Train Epoch: 1 [2663424/3084170 (86.4%)]	Loss: 0.128351
Train Epoch: 1 [2714624/3084170 (88.0%)]	Loss: 0.140603
Train Epoch: 1 [2765824/3084170 (89.7%)]	Loss: 0.141401
Train Epoch: 1 [2817024/3084170 (91.3%)]	Loss: 0.147061
Train Epoch: 1 [2868224/3084170 (93.0%)]	Loss: 0.138626
Train Epoch: 1 [2919424/3084170 (94.7%)]	Loss: 0.147407
Train Epoch: 1 [2970624/3084170 (96.3%)]	Loss: 0.142090
Train Epoch: 1 [3021824/3084170 (98.0%)]	Loss: 0.129959
Train Epoch: 1 [3073024/3084170 (99.6%)]	Loss: 0.136791
Train Epoch: 2 [1024/3084170 (0.0%)]	Loss: 0.130272
Train Epoch: 2 [52224/3084170 (1.7%)]	Loss: 0.127968
Train Epoch: 2 [103424/3084170 (3.4%)]	Loss: 0.154984
Train Epoch: 2 [154624/3084170 (5.0%)]	Loss: 0.126473
Train Epoch: 2 [205824/3084170 (6.7%)]	Loss: 0.135168
Train Epoch: 2 [257024/3084170 (8.3%)]	Loss: 0.126804
Train Epoch: 2 [308224/3084170 (10.0%)]	Loss: 0.123248
Train Epoch: 2 [359424/3084170 (11.7%)]	Loss: 0.142844
Train Epoch: 2 [410624/3084170 (13.3%)]	Loss: 0.128505
Train Epoch: 2 [461824/3084170 (15.0%)]	Loss: 0.126651
Train Epoch: 2 [513024/3084170 (16.6%)]	Loss: 0.113210
Train Epoch: 2 [564224/3084170 (18.3%)]	Loss: 0.138369
Train Epoch: 2 [615424/3084170 (20.0%)]	Loss: 0.155605
Train Epoch: 2 [666624/3084170 (21.6%)]	Loss: 0.123964
Train Epoch: 2 [717824/3084170 (23.3%)]	Loss: 0.121795
Train Epoch: 2 [769024/3084170 (24.9%)]	Loss: 0.126358
Train Epoch: 2 [820224/3084170 (26.6%)]	Loss: 0.117027
Train Epoch: 2 [871424/3084170 (28.3%)]	Loss: 0.114568
Train Epoch: 2 [922624/3084170 (29.9%)]	Loss: 0.145313
Train Epoch: 2 [973824/3084170 (31.6%)]	Loss: 0.138531
Train Epoch: 2 [1025024/3084170 (33.2%)]	Loss: 0.122465
Train Epoch: 2 [1076224/3084170 (34.9%)]	Loss: 0.133355
Train Epoch: 2 [1127424/3084170 (36.6%)]	Loss: 0.138717
Train Epoch: 2 [1178624/3084170 (38.2%)]	Loss: 0.134048
Train Epoch: 2 [1229824/3084170 (39.9%)]	Loss: 0.124381
Train Epoch: 2 [1281024/3084170 (41.5%)]	Loss: 0.134161
Train Epoch: 2 [1332224/3084170 (43.2%)]	Loss: 0.126637
Train Epoch: 2 [1383424/3084170 (44.9%)]	Loss: 0.120985
Train Epoch: 2 [1434624/3084170 (46.5%)]	Loss: 0.135426
Train Epoch: 2 [1485824/3084170 (48.2%)]	Loss: 0.138419
Train Epoch: 2 [1537024/3084170 (49.8%)]	Loss: 0.123103
Train Epoch: 2 [1588224/3084170 (51.5%)]	Loss: 0.109613
Train Epoch: 2 [1639424/3084170 (53.2%)]	Loss: 0.140919
Train Epoch: 2 [1690624/3084170 (54.8%)]	Loss: 0.126615
Train Epoch: 2 [1741824/3084170 (56.5%)]	Loss: 0.120839
Train Epoch: 2 [1793024/3084170 (58.1%)]	Loss: 0.124615
Train Epoch: 2 [1844224/3084170 (59.8%)]	Loss: 0.107661
Train Epoch: 2 [1895424/3084170 (61.5%)]	Loss: 0.131745
Train Epoch: 2 [1946624/3084170 (63.1%)]	Loss: 0.128527
Train Epoch: 2 [1997824/3084170 (64.8%)]	Loss: 0.117304
Train Epoch: 2 [2049024/3084170 (66.4%)]	Loss: 0.104852
Train Epoch: 2 [2100224/3084170 (68.1%)]	Loss: 0.131443
Train Epoch: 2 [2151424/3084170 (69.8%)]	Loss: 0.126584
Train Epoch: 2 [2202624/3084170 (71.4%)]	Loss: 0.119959
Train Epoch: 2 [2253824/3084170 (73.1%)]	Loss: 0.122044
Train Epoch: 2 [2305024/3084170 (74.7%)]	Loss: 0.127307
Train Epoch: 2 [2356224/3084170 (76.4%)]	Loss: 0.131382
Train Epoch: 2 [2407424/3084170 (78.1%)]	Loss: 0.112179
Train Epoch: 2 [2458624/3084170 (79.7%)]	Loss: 0.124188
Train Epoch: 2 [2509824/3084170 (81.4%)]	Loss: 0.131189
Train Epoch: 2 [2561024/3084170 (83.0%)]	Loss: 0.129636
Train Epoch: 2 [2612224/3084170 (84.7%)]	Loss: 0.125526
Train Epoch: 2 [2663424/3084170 (86.4%)]	Loss: 0.131612
Train Epoch: 2 [2714624/3084170 (88.0%)]	Loss: 0.122230
Train Epoch: 2 [2765824/3084170 (89.7%)]	Loss: 0.126342
Train Epoch: 2 [2817024/3084170 (91.3%)]	Loss: 0.129114
Train Epoch: 2 [2868224/3084170 (93.0%)]	Loss: 0.126479
Train Epoch: 2 [2919424/3084170 (94.7%)]	Loss: 0.115917
Train Epoch: 2 [2970624/3084170 (96.3%)]	Loss: 0.129605
Train Epoch: 2 [3021824/3084170 (98.0%)]	Loss: 0.130774
Train Epoch: 2 [3073024/3084170 (99.6%)]	Loss: 0.116672

 ---------------------------------------------------------------------- 


ACC in fold#1 was 0.915


Confusion Matrix in fold#1: 
           nonRipple  Ripple
nonRipple     262222   46590
Ripple         18935  443296


Classification Report in fold#1: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.933       0.905     0.915       0.919         0.916
recall            0.849       0.959     0.915       0.904         0.915
f1-score          0.889       0.931     0.915       0.910         0.914
sample size  308812.000  462231.000     0.915  771043.000    771043.000


PR_AUC in fold#1 was 0.959


ROC_AUC in fold#1 was 0.958

Time (id:2): tot 02:39:18, prev 01:19:34 [hh:mm:ss]: 
i_fold=1 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/3084170 (0.0%)]	Loss: 0.352864
Train Epoch: 1 [52224/3084170 (1.7%)]	Loss: 0.191403
Train Epoch: 1 [103424/3084170 (3.4%)]	Loss: 0.137843
Train Epoch: 1 [154624/3084170 (5.0%)]	Loss: 0.147170
Train Epoch: 1 [205824/3084170 (6.7%)]	Loss: 0.144517
Train Epoch: 1 [257024/3084170 (8.3%)]	Loss: 0.131419
Train Epoch: 1 [308224/3084170 (10.0%)]	Loss: 0.145254
Train Epoch: 1 [359424/3084170 (11.7%)]	Loss: 0.149916
Train Epoch: 1 [410624/3084170 (13.3%)]	Loss: 0.158563
Train Epoch: 1 [461824/3084170 (15.0%)]	Loss: 0.134501
Train Epoch: 1 [513024/3084170 (16.6%)]	Loss: 0.129626
Train Epoch: 1 [564224/3084170 (18.3%)]	Loss: 0.140338
Train Epoch: 1 [615424/3084170 (20.0%)]	Loss: 0.143321
Train Epoch: 1 [666624/3084170 (21.6%)]	Loss: 0.151794
Train Epoch: 1 [717824/3084170 (23.3%)]	Loss: 0.137015
Train Epoch: 1 [769024/3084170 (24.9%)]	Loss: 0.128884
Train Epoch: 1 [820224/3084170 (26.6%)]	Loss: 0.140993
Train Epoch: 1 [871424/3084170 (28.3%)]	Loss: 0.147210
Train Epoch: 1 [922624/3084170 (29.9%)]	Loss: 0.114604
Train Epoch: 1 [973824/3084170 (31.6%)]	Loss: 0.146122
Train Epoch: 1 [1025024/3084170 (33.2%)]	Loss: 0.148743
Train Epoch: 1 [1076224/3084170 (34.9%)]	Loss: 0.136318
Train Epoch: 1 [1127424/3084170 (36.6%)]	Loss: 0.136839
Train Epoch: 1 [1178624/3084170 (38.2%)]	Loss: 0.128265
Train Epoch: 1 [1229824/3084170 (39.9%)]	Loss: 0.108355
Train Epoch: 1 [1281024/3084170 (41.5%)]	Loss: 0.123759
Train Epoch: 1 [1332224/3084170 (43.2%)]	Loss: 0.138046
Train Epoch: 1 [1383424/3084170 (44.9%)]	Loss: 0.138886
Train Epoch: 1 [1434624/3084170 (46.5%)]	Loss: 0.148026
Train Epoch: 1 [1485824/3084170 (48.2%)]	Loss: 0.145460
Train Epoch: 1 [1537024/3084170 (49.8%)]	Loss: 0.125721
Train Epoch: 1 [1588224/3084170 (51.5%)]	Loss: 0.130416
Train Epoch: 1 [1639424/3084170 (53.2%)]	Loss: 0.135763
Train Epoch: 1 [1690624/3084170 (54.8%)]	Loss: 0.129141
Train Epoch: 1 [1741824/3084170 (56.5%)]	Loss: 0.125913
Train Epoch: 1 [1793024/3084170 (58.1%)]	Loss: 0.124142
Train Epoch: 1 [1844224/3084170 (59.8%)]	Loss: 0.132721
Train Epoch: 1 [1895424/3084170 (61.5%)]	Loss: 0.123263
Train Epoch: 1 [1946624/3084170 (63.1%)]	Loss: 0.137519
Train Epoch: 1 [1997824/3084170 (64.8%)]	Loss: 0.122370
Train Epoch: 1 [2049024/3084170 (66.4%)]	Loss: 0.113949
Train Epoch: 1 [2100224/3084170 (68.1%)]	Loss: 0.138485
Train Epoch: 1 [2151424/3084170 (69.8%)]	Loss: 0.119339
Train Epoch: 1 [2202624/3084170 (71.4%)]	Loss: 0.115820
Train Epoch: 1 [2253824/3084170 (73.1%)]	Loss: 0.117786
Train Epoch: 1 [2305024/3084170 (74.7%)]	Loss: 0.120852
Train Epoch: 1 [2356224/3084170 (76.4%)]	Loss: 0.160852
Train Epoch: 1 [2407424/3084170 (78.1%)]	Loss: 0.132377
Train Epoch: 1 [2458624/3084170 (79.7%)]	Loss: 0.127902
Train Epoch: 1 [2509824/3084170 (81.4%)]	Loss: 0.134078
Train Epoch: 1 [2561024/3084170 (83.0%)]	Loss: 0.120599
Train Epoch: 1 [2612224/3084170 (84.7%)]	Loss: 0.127908
Train Epoch: 1 [2663424/3084170 (86.4%)]	Loss: 0.121481
Train Epoch: 1 [2714624/3084170 (88.0%)]	Loss: 0.138434
Train Epoch: 1 [2765824/3084170 (89.7%)]	Loss: 0.137326
Train Epoch: 1 [2817024/3084170 (91.3%)]	Loss: 0.133329
Train Epoch: 1 [2868224/3084170 (93.0%)]	Loss: 0.145560
Train Epoch: 1 [2919424/3084170 (94.7%)]	Loss: 0.134069
Train Epoch: 1 [2970624/3084170 (96.3%)]	Loss: 0.138715
Train Epoch: 1 [3021824/3084170 (98.0%)]	Loss: 0.131243
Train Epoch: 1 [3073024/3084170 (99.6%)]	Loss: 0.134269
Train Epoch: 2 [1024/3084170 (0.0%)]	Loss: 0.143040
Train Epoch: 2 [52224/3084170 (1.7%)]	Loss: 0.125961
Train Epoch: 2 [103424/3084170 (3.4%)]	Loss: 0.127934
Train Epoch: 2 [154624/3084170 (5.0%)]	Loss: 0.134796
Train Epoch: 2 [205824/3084170 (6.7%)]	Loss: 0.120371
Train Epoch: 2 [257024/3084170 (8.3%)]	Loss: 0.122251
Train Epoch: 2 [308224/3084170 (10.0%)]	Loss: 0.124117
Train Epoch: 2 [359424/3084170 (11.7%)]	Loss: 0.124615
Train Epoch: 2 [410624/3084170 (13.3%)]	Loss: 0.123890
Train Epoch: 2 [461824/3084170 (15.0%)]	Loss: 0.118994
Train Epoch: 2 [513024/3084170 (16.6%)]	Loss: 0.128470
Train Epoch: 2 [564224/3084170 (18.3%)]	Loss: 0.129752
Train Epoch: 2 [615424/3084170 (20.0%)]	Loss: 0.113115
Train Epoch: 2 [666624/3084170 (21.6%)]	Loss: 0.127288
Train Epoch: 2 [717824/3084170 (23.3%)]	Loss: 0.120931
Train Epoch: 2 [769024/3084170 (24.9%)]	Loss: 0.128140
Train Epoch: 2 [820224/3084170 (26.6%)]	Loss: 0.113626
Train Epoch: 2 [871424/3084170 (28.3%)]	Loss: 0.128884
Train Epoch: 2 [922624/3084170 (29.9%)]	Loss: 0.133695
Train Epoch: 2 [973824/3084170 (31.6%)]	Loss: 0.136836
Train Epoch: 2 [1025024/3084170 (33.2%)]	Loss: 0.125365
Train Epoch: 2 [1076224/3084170 (34.9%)]	Loss: 0.134615
Train Epoch: 2 [1127424/3084170 (36.6%)]	Loss: 0.129101
Train Epoch: 2 [1178624/3084170 (38.2%)]	Loss: 0.122760
Train Epoch: 2 [1229824/3084170 (39.9%)]	Loss: 0.131410
Train Epoch: 2 [1281024/3084170 (41.5%)]	Loss: 0.109894
Train Epoch: 2 [1332224/3084170 (43.2%)]	Loss: 0.130355
Train Epoch: 2 [1383424/3084170 (44.9%)]	Loss: 0.128644
Train Epoch: 2 [1434624/3084170 (46.5%)]	Loss: 0.121016
Train Epoch: 2 [1485824/3084170 (48.2%)]	Loss: 0.109624
Train Epoch: 2 [1537024/3084170 (49.8%)]	Loss: 0.124916
Train Epoch: 2 [1588224/3084170 (51.5%)]	Loss: 0.113553
Train Epoch: 2 [1639424/3084170 (53.2%)]	Loss: 0.124731
Train Epoch: 2 [1690624/3084170 (54.8%)]	Loss: 0.136830
Train Epoch: 2 [1741824/3084170 (56.5%)]	Loss: 0.127454
Train Epoch: 2 [1793024/3084170 (58.1%)]	Loss: 0.130572
Train Epoch: 2 [1844224/3084170 (59.8%)]	Loss: 0.118351
Train Epoch: 2 [1895424/3084170 (61.5%)]	Loss: 0.125957
Train Epoch: 2 [1946624/3084170 (63.1%)]	Loss: 0.115236
Train Epoch: 2 [1997824/3084170 (64.8%)]	Loss: 0.120088
Train Epoch: 2 [2049024/3084170 (66.4%)]	Loss: 0.134253
Train Epoch: 2 [2100224/3084170 (68.1%)]	Loss: 0.133366
Train Epoch: 2 [2151424/3084170 (69.8%)]	Loss: 0.113991
Train Epoch: 2 [2202624/3084170 (71.4%)]	Loss: 0.123760
Train Epoch: 2 [2253824/3084170 (73.1%)]	Loss: 0.117027
Train Epoch: 2 [2305024/3084170 (74.7%)]	Loss: 0.122925
Train Epoch: 2 [2356224/3084170 (76.4%)]	Loss: 0.135684
Train Epoch: 2 [2407424/3084170 (78.1%)]	Loss: 0.123010
Train Epoch: 2 [2458624/3084170 (79.7%)]	Loss: 0.133840
Train Epoch: 2 [2509824/3084170 (81.4%)]	Loss: 0.123867
Train Epoch: 2 [2561024/3084170 (83.0%)]	Loss: 0.128042
Train Epoch: 2 [2612224/3084170 (84.7%)]	Loss: 0.110584
Train Epoch: 2 [2663424/3084170 (86.4%)]	Loss: 0.119572
Train Epoch: 2 [2714624/3084170 (88.0%)]	Loss: 0.126326
Train Epoch: 2 [2765824/3084170 (89.7%)]	Loss: 0.126600
Train Epoch: 2 [2817024/3084170 (91.3%)]	Loss: 0.130106
Train Epoch: 2 [2868224/3084170 (93.0%)]	Loss: 0.121935
Train Epoch: 2 [2919424/3084170 (94.7%)]	Loss: 0.132872
Train Epoch: 2 [2970624/3084170 (96.3%)]	Loss: 0.125443
Train Epoch: 2 [3021824/3084170 (98.0%)]	Loss: 0.127382
Train Epoch: 2 [3073024/3084170 (99.6%)]	Loss: 0.133478

 ---------------------------------------------------------------------- 


ACC in fold#2 was 0.912


Confusion Matrix in fold#2: 
           nonRipple  Ripple
nonRipple     262997   45815
Ripple         22319  439912


Classification Report in fold#2: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.922       0.906     0.912       0.914         0.912
recall            0.852       0.952     0.912       0.902         0.912
f1-score          0.885       0.928     0.912       0.907         0.911
sample size  308812.000  462231.000     0.912  771043.000    771043.000


PR_AUC in fold#2 was 0.959


ROC_AUC in fold#2 was 0.956

Time (id:3): tot 03:58:49, prev 01:19:30 [hh:mm:ss]: 
i_fold=2 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/3084171 (0.0%)]	Loss: 0.344135
Train Epoch: 1 [52224/3084171 (1.7%)]	Loss: 0.184260
Train Epoch: 1 [103424/3084171 (3.4%)]	Loss: 0.154749
Train Epoch: 1 [154624/3084171 (5.0%)]	Loss: 0.140572
Train Epoch: 1 [205824/3084171 (6.7%)]	Loss: 0.133841
Train Epoch: 1 [257024/3084171 (8.3%)]	Loss: 0.135500
Train Epoch: 1 [308224/3084171 (10.0%)]	Loss: 0.146943
Train Epoch: 1 [359424/3084171 (11.7%)]	Loss: 0.140803
Train Epoch: 1 [410624/3084171 (13.3%)]	Loss: 0.131726
Train Epoch: 1 [461824/3084171 (15.0%)]	Loss: 0.122715
Train Epoch: 1 [513024/3084171 (16.6%)]	Loss: 0.138530
Train Epoch: 1 [564224/3084171 (18.3%)]	Loss: 0.129573
Train Epoch: 1 [615424/3084171 (20.0%)]	Loss: 0.143189
Train Epoch: 1 [666624/3084171 (21.6%)]	Loss: 0.125523
Train Epoch: 1 [717824/3084171 (23.3%)]	Loss: 0.123846
Train Epoch: 1 [769024/3084171 (24.9%)]	Loss: 0.130075
Train Epoch: 1 [820224/3084171 (26.6%)]	Loss: 0.160571
Train Epoch: 1 [871424/3084171 (28.3%)]	Loss: 0.141221
Train Epoch: 1 [922624/3084171 (29.9%)]	Loss: 0.144569
Train Epoch: 1 [973824/3084171 (31.6%)]	Loss: 0.114676
Train Epoch: 1 [1025024/3084171 (33.2%)]	Loss: 0.138652
Train Epoch: 1 [1076224/3084171 (34.9%)]	Loss: 0.122146
Train Epoch: 1 [1127424/3084171 (36.6%)]	Loss: 0.163985
Train Epoch: 1 [1178624/3084171 (38.2%)]	Loss: 0.133597
Train Epoch: 1 [1229824/3084171 (39.9%)]	Loss: 0.129978
Train Epoch: 1 [1281024/3084171 (41.5%)]	Loss: 0.117363
Train Epoch: 1 [1332224/3084171 (43.2%)]	Loss: 0.123581
Train Epoch: 1 [1383424/3084171 (44.9%)]	Loss: 0.125114
Train Epoch: 1 [1434624/3084171 (46.5%)]	Loss: 0.114158
Train Epoch: 1 [1485824/3084171 (48.2%)]	Loss: 0.142215
Train Epoch: 1 [1537024/3084171 (49.8%)]	Loss: 0.122624
Train Epoch: 1 [1588224/3084171 (51.5%)]	Loss: 0.137621
Train Epoch: 1 [1639424/3084171 (53.2%)]	Loss: 0.127490
Train Epoch: 1 [1690624/3084171 (54.8%)]	Loss: 0.121368
Train Epoch: 1 [1741824/3084171 (56.5%)]	Loss: 0.133903
Train Epoch: 1 [1793024/3084171 (58.1%)]	Loss: 0.126706
Train Epoch: 1 [1844224/3084171 (59.8%)]	Loss: 0.117116
Train Epoch: 1 [1895424/3084171 (61.5%)]	Loss: 0.134149
Train Epoch: 1 [1946624/3084171 (63.1%)]	Loss: 0.128438
Train Epoch: 1 [1997824/3084171 (64.8%)]	Loss: 0.132616
Train Epoch: 1 [2049024/3084171 (66.4%)]	Loss: 0.130389
Train Epoch: 1 [2100224/3084171 (68.1%)]	Loss: 0.145160
Train Epoch: 1 [2151424/3084171 (69.8%)]	Loss: 0.131999
Train Epoch: 1 [2202624/3084171 (71.4%)]	Loss: 0.121944
Train Epoch: 1 [2253824/3084171 (73.1%)]	Loss: 0.143659
Train Epoch: 1 [2305024/3084171 (74.7%)]	Loss: 0.123099
Train Epoch: 1 [2356224/3084171 (76.4%)]	Loss: 0.136786
Train Epoch: 1 [2407424/3084171 (78.1%)]	Loss: 0.144062
Train Epoch: 1 [2458624/3084171 (79.7%)]	Loss: 0.122627
Train Epoch: 1 [2509824/3084171 (81.4%)]	Loss: 0.122361
Train Epoch: 1 [2561024/3084171 (83.0%)]	Loss: 0.136828
Train Epoch: 1 [2612224/3084171 (84.7%)]	Loss: 0.114812
Train Epoch: 1 [2663424/3084171 (86.4%)]	Loss: 0.129036
Train Epoch: 1 [2714624/3084171 (88.0%)]	Loss: 0.127222
Train Epoch: 1 [2765824/3084171 (89.7%)]	Loss: 0.141044
Train Epoch: 1 [2817024/3084171 (91.3%)]	Loss: 0.137590
Train Epoch: 1 [2868224/3084171 (93.0%)]	Loss: 0.135746
Train Epoch: 1 [2919424/3084171 (94.7%)]	Loss: 0.127581
Train Epoch: 1 [2970624/3084171 (96.3%)]	Loss: 0.118661
Train Epoch: 1 [3021824/3084171 (98.0%)]	Loss: 0.128323
Train Epoch: 1 [3073024/3084171 (99.6%)]	Loss: 0.123309
Train Epoch: 2 [1024/3084171 (0.0%)]	Loss: 0.115221
Train Epoch: 2 [52224/3084171 (1.7%)]	Loss: 0.131948
Train Epoch: 2 [103424/3084171 (3.4%)]	Loss: 0.133192
Train Epoch: 2 [154624/3084171 (5.0%)]	Loss: 0.134889
Train Epoch: 2 [205824/3084171 (6.7%)]	Loss: 0.118250
Train Epoch: 2 [257024/3084171 (8.3%)]	Loss: 0.125767
Train Epoch: 2 [308224/3084171 (10.0%)]	Loss: 0.136471
Train Epoch: 2 [359424/3084171 (11.7%)]	Loss: 0.128780
Train Epoch: 2 [410624/3084171 (13.3%)]	Loss: 0.130338
Train Epoch: 2 [461824/3084171 (15.0%)]	Loss: 0.136562
Train Epoch: 2 [513024/3084171 (16.6%)]	Loss: 0.134720
Train Epoch: 2 [564224/3084171 (18.3%)]	Loss: 0.132308
Train Epoch: 2 [615424/3084171 (20.0%)]	Loss: 0.150637
Train Epoch: 2 [666624/3084171 (21.6%)]	Loss: 0.123297
Train Epoch: 2 [717824/3084171 (23.3%)]	Loss: 0.121492
Train Epoch: 2 [769024/3084171 (24.9%)]	Loss: 0.113352
Train Epoch: 2 [820224/3084171 (26.6%)]	Loss: 0.131100
Train Epoch: 2 [871424/3084171 (28.3%)]	Loss: 0.133536
Train Epoch: 2 [922624/3084171 (29.9%)]	Loss: 0.121035
Train Epoch: 2 [973824/3084171 (31.6%)]	Loss: 0.104290
Train Epoch: 2 [1025024/3084171 (33.2%)]	Loss: 0.114708
Train Epoch: 2 [1076224/3084171 (34.9%)]	Loss: 0.128599
Train Epoch: 2 [1127424/3084171 (36.6%)]	Loss: 0.118933
Train Epoch: 2 [1178624/3084171 (38.2%)]	Loss: 0.114157
Train Epoch: 2 [1229824/3084171 (39.9%)]	Loss: 0.120941
Train Epoch: 2 [1281024/3084171 (41.5%)]	Loss: 0.128328
Train Epoch: 2 [1332224/3084171 (43.2%)]	Loss: 0.116472
Train Epoch: 2 [1383424/3084171 (44.9%)]	Loss: 0.129221
Train Epoch: 2 [1434624/3084171 (46.5%)]	Loss: 0.121743
Train Epoch: 2 [1485824/3084171 (48.2%)]	Loss: 0.130066
Train Epoch: 2 [1537024/3084171 (49.8%)]	Loss: 0.120003
Train Epoch: 2 [1588224/3084171 (51.5%)]	Loss: 0.124528
Train Epoch: 2 [1639424/3084171 (53.2%)]	Loss: 0.122551
Train Epoch: 2 [1690624/3084171 (54.8%)]	Loss: 0.120740
Train Epoch: 2 [1741824/3084171 (56.5%)]	Loss: 0.142947
Train Epoch: 2 [1793024/3084171 (58.1%)]	Loss: 0.125999
Train Epoch: 2 [1844224/3084171 (59.8%)]	Loss: 0.118861
Train Epoch: 2 [1895424/3084171 (61.5%)]	Loss: 0.137504
Train Epoch: 2 [1946624/3084171 (63.1%)]	Loss: 0.128549
Train Epoch: 2 [1997824/3084171 (64.8%)]	Loss: 0.130770
Train Epoch: 2 [2049024/3084171 (66.4%)]	Loss: 0.140569
Train Epoch: 2 [2100224/3084171 (68.1%)]	Loss: 0.143023
Train Epoch: 2 [2151424/3084171 (69.8%)]	Loss: 0.122445
Train Epoch: 2 [2202624/3084171 (71.4%)]	Loss: 0.125857
Train Epoch: 2 [2253824/3084171 (73.1%)]	Loss: 0.142470
Train Epoch: 2 [2305024/3084171 (74.7%)]	Loss: 0.119023
Train Epoch: 2 [2356224/3084171 (76.4%)]	Loss: 0.107348
Train Epoch: 2 [2407424/3084171 (78.1%)]	Loss: 0.127774
Train Epoch: 2 [2458624/3084171 (79.7%)]	Loss: 0.121945
Train Epoch: 2 [2509824/3084171 (81.4%)]	Loss: 0.117695
Train Epoch: 2 [2561024/3084171 (83.0%)]	Loss: 0.124228
Train Epoch: 2 [2612224/3084171 (84.7%)]	Loss: 0.129233
Train Epoch: 2 [2663424/3084171 (86.4%)]	Loss: 0.128602
Train Epoch: 2 [2714624/3084171 (88.0%)]	Loss: 0.126560
Train Epoch: 2 [2765824/3084171 (89.7%)]	Loss: 0.125864
Train Epoch: 2 [2817024/3084171 (91.3%)]	Loss: 0.120522
Train Epoch: 2 [2868224/3084171 (93.0%)]	Loss: 0.128580
Train Epoch: 2 [2919424/3084171 (94.7%)]	Loss: 0.128759
Train Epoch: 2 [2970624/3084171 (96.3%)]	Loss: 0.119173
Train Epoch: 2 [3021824/3084171 (98.0%)]	Loss: 0.118184
Train Epoch: 2 [3073024/3084171 (99.6%)]	Loss: 0.124438

 ---------------------------------------------------------------------- 


ACC in fold#3 was 0.895


Confusion Matrix in fold#3: 
           nonRipple  Ripple
nonRipple     269089   39722
Ripple         40964  421267


Classification Report in fold#3: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.868       0.914     0.895       0.891         0.895
recall            0.871       0.911     0.895       0.891         0.895
f1-score          0.870       0.913     0.895       0.891         0.895
sample size  308811.000  462231.000     0.895  771042.000    771042.000


PR_AUC in fold#3 was 0.957


ROC_AUC in fold#3 was 0.951

Time (id:4): tot 05:18:03, prev 01:19:13 [hh:mm:ss]: 
i_fold=3 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/3084171 (0.0%)]	Loss: 0.363197
Train Epoch: 1 [52224/3084171 (1.7%)]	Loss: 0.199150
Train Epoch: 1 [103424/3084171 (3.4%)]	Loss: 0.143414
Train Epoch: 1 [154624/3084171 (5.0%)]	Loss: 0.139885
Train Epoch: 1 [205824/3084171 (6.7%)]	Loss: 0.133966
Train Epoch: 1 [257024/3084171 (8.3%)]	Loss: 0.127869
Train Epoch: 1 [308224/3084171 (10.0%)]	Loss: 0.114584
Train Epoch: 1 [359424/3084171 (11.7%)]	Loss: 0.135814
Train Epoch: 1 [410624/3084171 (13.3%)]	Loss: 0.140491
Train Epoch: 1 [461824/3084171 (15.0%)]	Loss: 0.123683
Train Epoch: 1 [513024/3084171 (16.6%)]	Loss: 0.136930
Train Epoch: 1 [564224/3084171 (18.3%)]	Loss: 0.131229
Train Epoch: 1 [615424/3084171 (20.0%)]	Loss: 0.123760
Train Epoch: 1 [666624/3084171 (21.6%)]	Loss: 0.131802
Train Epoch: 1 [717824/3084171 (23.3%)]	Loss: 0.135984
Train Epoch: 1 [769024/3084171 (24.9%)]	Loss: 0.118747
Train Epoch: 1 [820224/3084171 (26.6%)]	Loss: 0.136228
Train Epoch: 1 [871424/3084171 (28.3%)]	Loss: 0.115656
Train Epoch: 1 [922624/3084171 (29.9%)]	Loss: 0.126488
Train Epoch: 1 [973824/3084171 (31.6%)]	Loss: 0.129143
Train Epoch: 1 [1025024/3084171 (33.2%)]	Loss: 0.134117
Train Epoch: 1 [1076224/3084171 (34.9%)]	Loss: 0.115099
Train Epoch: 1 [1127424/3084171 (36.6%)]	Loss: 0.134615
Train Epoch: 1 [1178624/3084171 (38.2%)]	Loss: 0.126348
Train Epoch: 1 [1229824/3084171 (39.9%)]	Loss: 0.135496
Train Epoch: 1 [1281024/3084171 (41.5%)]	Loss: 0.147963
Train Epoch: 1 [1332224/3084171 (43.2%)]	Loss: 0.116945
Train Epoch: 1 [1383424/3084171 (44.9%)]	Loss: 0.118079
Train Epoch: 1 [1434624/3084171 (46.5%)]	Loss: 0.128117
Train Epoch: 1 [1485824/3084171 (48.2%)]	Loss: 0.133214
Train Epoch: 1 [1537024/3084171 (49.8%)]	Loss: 0.135499
Train Epoch: 1 [1588224/3084171 (51.5%)]	Loss: 0.130995
Train Epoch: 1 [1639424/3084171 (53.2%)]	Loss: 0.122525
Train Epoch: 1 [1690624/3084171 (54.8%)]	Loss: 0.127421
Train Epoch: 1 [1741824/3084171 (56.5%)]	Loss: 0.130124
Train Epoch: 1 [1793024/3084171 (58.1%)]	Loss: 0.129162
Train Epoch: 1 [1844224/3084171 (59.8%)]	Loss: 0.120215
Train Epoch: 1 [1895424/3084171 (61.5%)]	Loss: 0.133554
Train Epoch: 1 [1946624/3084171 (63.1%)]	Loss: 0.126382
Train Epoch: 1 [1997824/3084171 (64.8%)]	Loss: 0.119523
Train Epoch: 1 [2049024/3084171 (66.4%)]	Loss: 0.129420
Train Epoch: 1 [2100224/3084171 (68.1%)]	Loss: 0.123225
Train Epoch: 1 [2151424/3084171 (69.8%)]	Loss: 0.126018
Train Epoch: 1 [2202624/3084171 (71.4%)]	Loss: 0.134690
Train Epoch: 1 [2253824/3084171 (73.1%)]	Loss: 0.119265
Train Epoch: 1 [2305024/3084171 (74.7%)]	Loss: 0.125914
Train Epoch: 1 [2356224/3084171 (76.4%)]	Loss: 0.113860
Train Epoch: 1 [2407424/3084171 (78.1%)]	Loss: 0.130461
Train Epoch: 1 [2458624/3084171 (79.7%)]	Loss: 0.145030
Train Epoch: 1 [2509824/3084171 (81.4%)]	Loss: 0.123474
Train Epoch: 1 [2561024/3084171 (83.0%)]	Loss: 0.137045
Train Epoch: 1 [2612224/3084171 (84.7%)]	Loss: 0.130381
Train Epoch: 1 [2663424/3084171 (86.4%)]	Loss: 0.127462
Train Epoch: 1 [2714624/3084171 (88.0%)]	Loss: 0.135789
Train Epoch: 1 [2765824/3084171 (89.7%)]	Loss: 0.129376
Train Epoch: 1 [2817024/3084171 (91.3%)]	Loss: 0.105440
Train Epoch: 1 [2868224/3084171 (93.0%)]	Loss: 0.125068
Train Epoch: 1 [2919424/3084171 (94.7%)]	Loss: 0.126864
Train Epoch: 1 [2970624/3084171 (96.3%)]	Loss: 0.140433
Train Epoch: 1 [3021824/3084171 (98.0%)]	Loss: 0.140329
Train Epoch: 1 [3073024/3084171 (99.6%)]	Loss: 0.118800
Train Epoch: 2 [1024/3084171 (0.0%)]	Loss: 0.142073
Train Epoch: 2 [52224/3084171 (1.7%)]	Loss: 0.124845
Train Epoch: 2 [103424/3084171 (3.4%)]	Loss: 0.133209
Train Epoch: 2 [154624/3084171 (5.0%)]	Loss: 0.137822
Train Epoch: 2 [205824/3084171 (6.7%)]	Loss: 0.116831
Train Epoch: 2 [257024/3084171 (8.3%)]	Loss: 0.111941
Train Epoch: 2 [308224/3084171 (10.0%)]	Loss: 0.131303
Train Epoch: 2 [359424/3084171 (11.7%)]	Loss: 0.116168
Train Epoch: 2 [410624/3084171 (13.3%)]	Loss: 0.122350
Train Epoch: 2 [461824/3084171 (15.0%)]	Loss: 0.134980
Train Epoch: 2 [513024/3084171 (16.6%)]	Loss: 0.129574
Train Epoch: 2 [564224/3084171 (18.3%)]	Loss: 0.117376
Train Epoch: 2 [615424/3084171 (20.0%)]	Loss: 0.119263
Train Epoch: 2 [666624/3084171 (21.6%)]	Loss: 0.128840
Train Epoch: 2 [717824/3084171 (23.3%)]	Loss: 0.126779
Train Epoch: 2 [769024/3084171 (24.9%)]	Loss: 0.117275
Train Epoch: 2 [820224/3084171 (26.6%)]	Loss: 0.102547
Train Epoch: 2 [871424/3084171 (28.3%)]	Loss: 0.119524
Train Epoch: 2 [922624/3084171 (29.9%)]	Loss: 0.141946
Train Epoch: 2 [973824/3084171 (31.6%)]	Loss: 0.122815
Train Epoch: 2 [1025024/3084171 (33.2%)]	Loss: 0.116371
Train Epoch: 2 [1076224/3084171 (34.9%)]	Loss: 0.131314
Train Epoch: 2 [1127424/3084171 (36.6%)]	Loss: 0.126389
Train Epoch: 2 [1178624/3084171 (38.2%)]	Loss: 0.116433
Train Epoch: 2 [1229824/3084171 (39.9%)]	Loss: 0.120875
Train Epoch: 2 [1281024/3084171 (41.5%)]	Loss: 0.120473
Train Epoch: 2 [1332224/3084171 (43.2%)]	Loss: 0.133809
Train Epoch: 2 [1383424/3084171 (44.9%)]	Loss: 0.129236
Train Epoch: 2 [1434624/3084171 (46.5%)]	Loss: 0.127998
Train Epoch: 2 [1485824/3084171 (48.2%)]	Loss: 0.113650
Train Epoch: 2 [1537024/3084171 (49.8%)]	Loss: 0.125720
Train Epoch: 2 [1588224/3084171 (51.5%)]	Loss: 0.120747
Train Epoch: 2 [1639424/3084171 (53.2%)]	Loss: 0.118593
Train Epoch: 2 [1690624/3084171 (54.8%)]	Loss: 0.128980
Train Epoch: 2 [1741824/3084171 (56.5%)]	Loss: 0.121807
Train Epoch: 2 [1793024/3084171 (58.1%)]	Loss: 0.126426
Train Epoch: 2 [1844224/3084171 (59.8%)]	Loss: 0.121691
Train Epoch: 2 [1895424/3084171 (61.5%)]	Loss: 0.120516
Train Epoch: 2 [1946624/3084171 (63.1%)]	Loss: 0.128562
Train Epoch: 2 [1997824/3084171 (64.8%)]	Loss: 0.112666
Train Epoch: 2 [2049024/3084171 (66.4%)]	Loss: 0.122135
Train Epoch: 2 [2100224/3084171 (68.1%)]	Loss: 0.124894
Train Epoch: 2 [2151424/3084171 (69.8%)]	Loss: 0.115844
Train Epoch: 2 [2202624/3084171 (71.4%)]	Loss: 0.119733
Train Epoch: 2 [2253824/3084171 (73.1%)]	Loss: 0.122063
Train Epoch: 2 [2305024/3084171 (74.7%)]	Loss: 0.121758
Train Epoch: 2 [2356224/3084171 (76.4%)]	Loss: 0.120699
Train Epoch: 2 [2407424/3084171 (78.1%)]	Loss: 0.119987
Train Epoch: 2 [2458624/3084171 (79.7%)]	Loss: 0.142291
Train Epoch: 2 [2509824/3084171 (81.4%)]	Loss: 0.127623
Train Epoch: 2 [2561024/3084171 (83.0%)]	Loss: 0.115478
Train Epoch: 2 [2612224/3084171 (84.7%)]	Loss: 0.117251
Train Epoch: 2 [2663424/3084171 (86.4%)]	Loss: 0.135365
Train Epoch: 2 [2714624/3084171 (88.0%)]	Loss: 0.116725
Train Epoch: 2 [2765824/3084171 (89.7%)]	Loss: 0.123381
Train Epoch: 2 [2817024/3084171 (91.3%)]	Loss: 0.106153
Train Epoch: 2 [2868224/3084171 (93.0%)]	Loss: 0.121366
Train Epoch: 2 [2919424/3084171 (94.7%)]	Loss: 0.118480
Train Epoch: 2 [2970624/3084171 (96.3%)]	Loss: 0.119203
Train Epoch: 2 [3021824/3084171 (98.0%)]	Loss: 0.132044
Train Epoch: 2 [3073024/3084171 (99.6%)]	Loss: 0.135199

 ---------------------------------------------------------------------- 


ACC in fold#4 was 0.883


Confusion Matrix in fold#4: 
           nonRipple  Ripple
nonRipple     278434   30377
Ripple         60103  402128


Classification Report in fold#4: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.822       0.930     0.883       0.876         0.887
recall            0.902       0.870     0.883       0.886         0.883
f1-score          0.860       0.899     0.883       0.880         0.883
sample size  308811.000  462231.000     0.883  771042.000    771042.000


PR_AUC in fold#4 was 0.953


ROC_AUC in fold#4 was 0.947

Time (id:5): tot 06:37:14, prev 01:19:10 [hh:mm:ss]: 
i_fold=4 ends.



 ---------------------------------------------------------------------- 


Label Errors Rate:
0.048


 --- 5-fold CV overall metrics --- 


Confusion Matrix (Test; sum; num. folds=5)
           nonRipple   Ripple
nonRipple    1319811   224246
Ripple        158687  2152469


Classification Report (Test; mean; num. folds=5)
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.897       0.907     0.901       0.902         0.902
recall            0.855       0.931     0.901       0.893         0.901
f1-score          0.874       0.918     0.901       0.896         0.900
sample size  308811.400  462231.200     0.901  771042.600    771042.600


Classification Report (Test; std; num. folds=5)
             nonRipple  Ripple  accuracy  macro avg  weighted avg
precision        0.045   0.017     0.012      0.016         0.011
recall           0.033   0.036     0.012      0.009         0.012
f1-score         0.011   0.011     0.012      0.011         0.011
sample size      0.490   0.400     0.012      0.490         0.490


PRE-REC AUC Score: 0.957 +/- 0.002 (mean +/- std.; n=5)


ROC AUC Score: 0.954 +/- 0.004 (mean +/- std.; n=5)


Saved to: ./data/okada/cleanlab_results/D02+/are_errors.npy


Saved to: ./data/okada/cleanlab_results/D02+/cleaned_labels.npy


Saved to: ./data/okada/cleanlab_results/D02+/pred_probas_ripples.npy

(Moved to: /tmp/2021-0512-2159-D02+-conf_mats.csv)
Saved to: ./data/okada/cleanlab_results/D02+/conf_mats.csv
Saved to: ./data/okada/cleanlab_results/D02+/conf_mat_overall_sum.png
(Moved to: /tmp/2021-0512-2159-D02+-clf_reports.csv)
Saved to: ./data/okada/cleanlab_results/D02+/clf_reports.csv
(Moved to: /tmp/2021-0512-2159-D02+-roc-auc.csv)
Saved to: ./data/okada/cleanlab_results/D02+/roc-auc.csv

Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D02+/tt7-4_fp16.pkl

