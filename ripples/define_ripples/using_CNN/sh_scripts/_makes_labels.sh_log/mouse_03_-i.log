
Random seeds have been fixed as 42


Random seeds have been fixed as 42

Indice of mice to load: ['03']
40
Time (id:0): tot 00:00:00, prev 00:00:00 [hh:mm:ss]: Reporter has been initialized.

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383430 (0.0%)]	Loss: 0.361139
Train Epoch: 1 [52224/2383430 (2.2%)]	Loss: 0.167672
Train Epoch: 1 [103424/2383430 (4.3%)]	Loss: 0.140555
Train Epoch: 1 [154624/2383430 (6.5%)]	Loss: 0.137659
Train Epoch: 1 [205824/2383430 (8.6%)]	Loss: 0.141090
Train Epoch: 1 [257024/2383430 (10.8%)]	Loss: 0.131483
Train Epoch: 1 [308224/2383430 (12.9%)]	Loss: 0.124342
Train Epoch: 1 [359424/2383430 (15.1%)]	Loss: 0.114481
Train Epoch: 1 [410624/2383430 (17.2%)]	Loss: 0.126300
Train Epoch: 1 [461824/2383430 (19.4%)]	Loss: 0.139183
Train Epoch: 1 [513024/2383430 (21.5%)]	Loss: 0.128102
Train Epoch: 1 [564224/2383430 (23.7%)]	Loss: 0.127095
Train Epoch: 1 [615424/2383430 (25.8%)]	Loss: 0.142488
Train Epoch: 1 [666624/2383430 (28.0%)]	Loss: 0.113451
Train Epoch: 1 [717824/2383430 (30.1%)]	Loss: 0.130736
Train Epoch: 1 [769024/2383430 (32.3%)]	Loss: 0.127406
Train Epoch: 1 [820224/2383430 (34.4%)]	Loss: 0.134938
Train Epoch: 1 [871424/2383430 (36.6%)]	Loss: 0.122973
Train Epoch: 1 [922624/2383430 (38.7%)]	Loss: 0.108852
Train Epoch: 1 [973824/2383430 (40.9%)]	Loss: 0.106111
Train Epoch: 1 [1025024/2383430 (43.0%)]	Loss: 0.120250
Train Epoch: 1 [1076224/2383430 (45.2%)]	Loss: 0.110927
Train Epoch: 1 [1127424/2383430 (47.3%)]	Loss: 0.112671
Train Epoch: 1 [1178624/2383430 (49.5%)]	Loss: 0.114327
Train Epoch: 1 [1229824/2383430 (51.6%)]	Loss: 0.110183
Train Epoch: 1 [1281024/2383430 (53.7%)]	Loss: 0.127390
Train Epoch: 1 [1332224/2383430 (55.9%)]	Loss: 0.120256
Train Epoch: 1 [1383424/2383430 (58.0%)]	Loss: 0.118558
Train Epoch: 1 [1434624/2383430 (60.2%)]	Loss: 0.153604
Train Epoch: 1 [1485824/2383430 (62.3%)]	Loss: 0.117959
Train Epoch: 1 [1537024/2383430 (64.5%)]	Loss: 0.101841
Train Epoch: 1 [1588224/2383430 (66.6%)]	Loss: 0.120879
Train Epoch: 1 [1639424/2383430 (68.8%)]	Loss: 0.121572
Train Epoch: 1 [1690624/2383430 (70.9%)]	Loss: 0.117127
Train Epoch: 1 [1741824/2383430 (73.1%)]	Loss: 0.124855
Train Epoch: 1 [1793024/2383430 (75.2%)]	Loss: 0.112885
Train Epoch: 1 [1844224/2383430 (77.4%)]	Loss: 0.112599
Train Epoch: 1 [1895424/2383430 (79.5%)]	Loss: 0.119020
Train Epoch: 1 [1946624/2383430 (81.7%)]	Loss: 0.113398
Train Epoch: 1 [1997824/2383430 (83.8%)]	Loss: 0.103225
Train Epoch: 1 [2049024/2383430 (86.0%)]	Loss: 0.119255
Train Epoch: 1 [2100224/2383430 (88.1%)]	Loss: 0.126488
Train Epoch: 1 [2151424/2383430 (90.3%)]	Loss: 0.112361
Train Epoch: 1 [2202624/2383430 (92.4%)]	Loss: 0.109787
Train Epoch: 1 [2253824/2383430 (94.6%)]	Loss: 0.118422
Train Epoch: 1 [2305024/2383430 (96.7%)]	Loss: 0.108356
Train Epoch: 1 [2356224/2383430 (98.9%)]	Loss: 0.119841
Train Epoch: 2 [1024/2383430 (0.0%)]	Loss: 0.118840
Train Epoch: 2 [52224/2383430 (2.2%)]	Loss: 0.130714
Train Epoch: 2 [103424/2383430 (4.3%)]	Loss: 0.108743
Train Epoch: 2 [154624/2383430 (6.5%)]	Loss: 0.129341
Train Epoch: 2 [205824/2383430 (8.6%)]	Loss: 0.110598
Train Epoch: 2 [257024/2383430 (10.8%)]	Loss: 0.133578
Train Epoch: 2 [308224/2383430 (12.9%)]	Loss: 0.110334
Train Epoch: 2 [359424/2383430 (15.1%)]	Loss: 0.106502
Train Epoch: 2 [410624/2383430 (17.2%)]	Loss: 0.105395
Train Epoch: 2 [461824/2383430 (19.4%)]	Loss: 0.119232
Train Epoch: 2 [513024/2383430 (21.5%)]	Loss: 0.115139
Train Epoch: 2 [564224/2383430 (23.7%)]	Loss: 0.103103
Train Epoch: 2 [615424/2383430 (25.8%)]	Loss: 0.123261
Train Epoch: 2 [666624/2383430 (28.0%)]	Loss: 0.114833
Train Epoch: 2 [717824/2383430 (30.1%)]	Loss: 0.135360
Train Epoch: 2 [769024/2383430 (32.3%)]	Loss: 0.123273
Train Epoch: 2 [820224/2383430 (34.4%)]	Loss: 0.114029
Train Epoch: 2 [871424/2383430 (36.6%)]	Loss: 0.113437
Train Epoch: 2 [922624/2383430 (38.7%)]	Loss: 0.129665
Train Epoch: 2 [973824/2383430 (40.9%)]	Loss: 0.106100
Train Epoch: 2 [1025024/2383430 (43.0%)]	Loss: 0.104628
Train Epoch: 2 [1076224/2383430 (45.2%)]	Loss: 0.115499
Train Epoch: 2 [1127424/2383430 (47.3%)]	Loss: 0.111306
Train Epoch: 2 [1178624/2383430 (49.5%)]	Loss: 0.114509
Train Epoch: 2 [1229824/2383430 (51.6%)]	Loss: 0.109243
Train Epoch: 2 [1281024/2383430 (53.7%)]	Loss: 0.114347
Train Epoch: 2 [1332224/2383430 (55.9%)]	Loss: 0.108711
Train Epoch: 2 [1383424/2383430 (58.0%)]	Loss: 0.121801
Train Epoch: 2 [1434624/2383430 (60.2%)]	Loss: 0.097066
Train Epoch: 2 [1485824/2383430 (62.3%)]	Loss: 0.112565
Train Epoch: 2 [1537024/2383430 (64.5%)]	Loss: 0.141167
Train Epoch: 2 [1588224/2383430 (66.6%)]	Loss: 0.110760
Train Epoch: 2 [1639424/2383430 (68.8%)]	Loss: 0.123494
Train Epoch: 2 [1690624/2383430 (70.9%)]	Loss: 0.110332
Train Epoch: 2 [1741824/2383430 (73.1%)]	Loss: 0.111933
Train Epoch: 2 [1793024/2383430 (75.2%)]	Loss: 0.123748
Train Epoch: 2 [1844224/2383430 (77.4%)]	Loss: 0.119436
Train Epoch: 2 [1895424/2383430 (79.5%)]	Loss: 0.113456
Train Epoch: 2 [1946624/2383430 (81.7%)]	Loss: 0.121907
Train Epoch: 2 [1997824/2383430 (83.8%)]	Loss: 0.113928
Train Epoch: 2 [2049024/2383430 (86.0%)]	Loss: 0.113554
Train Epoch: 2 [2100224/2383430 (88.1%)]	Loss: 0.130738
Train Epoch: 2 [2151424/2383430 (90.3%)]	Loss: 0.109088
Train Epoch: 2 [2202624/2383430 (92.4%)]	Loss: 0.109536
Train Epoch: 2 [2253824/2383430 (94.6%)]	Loss: 0.120914
Train Epoch: 2 [2305024/2383430 (96.7%)]	Loss: 0.124093
Train Epoch: 2 [2356224/2383430 (98.9%)]	Loss: 0.117798

 ---------------------------------------------------------------------- 


ACC in fold#0 was 0.905


Confusion Matrix in fold#0: 
           nonRipple  Ripple
nonRipple     202488   34348
Ripple         22290  336732


Classification Report in fold#0: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.901       0.907     0.905       0.904         0.905
recall            0.855       0.938     0.905       0.896         0.905
f1-score          0.877       0.922     0.905       0.900         0.904
sample size  236836.000  359022.000     0.905  595858.000    595858.000


PR_AUC in fold#0 was 0.970


ROC_AUC in fold#0 was 0.965

Time (id:1): tot 01:01:13, prev 01:01:13 [hh:mm:ss]: 
i_fold=0 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383430 (0.0%)]	Loss: 0.383836
Train Epoch: 1 [52224/2383430 (2.2%)]	Loss: 0.172822
Train Epoch: 1 [103424/2383430 (4.3%)]	Loss: 0.124164
Train Epoch: 1 [154624/2383430 (6.5%)]	Loss: 0.145473
Train Epoch: 1 [205824/2383430 (8.6%)]	Loss: 0.144038
Train Epoch: 1 [257024/2383430 (10.8%)]	Loss: 0.136379
Train Epoch: 1 [308224/2383430 (12.9%)]	Loss: 0.119445
Train Epoch: 1 [359424/2383430 (15.1%)]	Loss: 0.141051
Train Epoch: 1 [410624/2383430 (17.2%)]	Loss: 0.118664
Train Epoch: 1 [461824/2383430 (19.4%)]	Loss: 0.130318
Train Epoch: 1 [513024/2383430 (21.5%)]	Loss: 0.118385
Train Epoch: 1 [564224/2383430 (23.7%)]	Loss: 0.114087
Train Epoch: 1 [615424/2383430 (25.8%)]	Loss: 0.113382
Train Epoch: 1 [666624/2383430 (28.0%)]	Loss: 0.114810
Train Epoch: 1 [717824/2383430 (30.1%)]	Loss: 0.122165
Train Epoch: 1 [769024/2383430 (32.3%)]	Loss: 0.135310
Train Epoch: 1 [820224/2383430 (34.4%)]	Loss: 0.122665
Train Epoch: 1 [871424/2383430 (36.6%)]	Loss: 0.113420
Train Epoch: 1 [922624/2383430 (38.7%)]	Loss: 0.116360
Train Epoch: 1 [973824/2383430 (40.9%)]	Loss: 0.122709
Train Epoch: 1 [1025024/2383430 (43.0%)]	Loss: 0.111974
Train Epoch: 1 [1076224/2383430 (45.2%)]	Loss: 0.115403
Train Epoch: 1 [1127424/2383430 (47.3%)]	Loss: 0.129614
Train Epoch: 1 [1178624/2383430 (49.5%)]	Loss: 0.120941
Train Epoch: 1 [1229824/2383430 (51.6%)]	Loss: 0.126866
Train Epoch: 1 [1281024/2383430 (53.7%)]	Loss: 0.113936
Train Epoch: 1 [1332224/2383430 (55.9%)]	Loss: 0.112646
Train Epoch: 1 [1383424/2383430 (58.0%)]	Loss: 0.124284
Train Epoch: 1 [1434624/2383430 (60.2%)]	Loss: 0.116455
Train Epoch: 1 [1485824/2383430 (62.3%)]	Loss: 0.101718
Train Epoch: 1 [1537024/2383430 (64.5%)]	Loss: 0.121963
Train Epoch: 1 [1588224/2383430 (66.6%)]	Loss: 0.115030
Train Epoch: 1 [1639424/2383430 (68.8%)]	Loss: 0.128173
Train Epoch: 1 [1690624/2383430 (70.9%)]	Loss: 0.117937
Train Epoch: 1 [1741824/2383430 (73.1%)]	Loss: 0.124345
Train Epoch: 1 [1793024/2383430 (75.2%)]	Loss: 0.111740
Train Epoch: 1 [1844224/2383430 (77.4%)]	Loss: 0.109978
Train Epoch: 1 [1895424/2383430 (79.5%)]	Loss: 0.122317
Train Epoch: 1 [1946624/2383430 (81.7%)]	Loss: 0.123451
Train Epoch: 1 [1997824/2383430 (83.8%)]	Loss: 0.121793
Train Epoch: 1 [2049024/2383430 (86.0%)]	Loss: 0.114119
Train Epoch: 1 [2100224/2383430 (88.1%)]	Loss: 0.130809
Train Epoch: 1 [2151424/2383430 (90.3%)]	Loss: 0.112630
Train Epoch: 1 [2202624/2383430 (92.4%)]	Loss: 0.103727
Train Epoch: 1 [2253824/2383430 (94.6%)]	Loss: 0.125507
Train Epoch: 1 [2305024/2383430 (96.7%)]	Loss: 0.108037
Train Epoch: 1 [2356224/2383430 (98.9%)]	Loss: 0.123931
Train Epoch: 2 [1024/2383430 (0.0%)]	Loss: 0.122306
Train Epoch: 2 [52224/2383430 (2.2%)]	Loss: 0.127978
Train Epoch: 2 [103424/2383430 (4.3%)]	Loss: 0.119302
Train Epoch: 2 [154624/2383430 (6.5%)]	Loss: 0.114968
Train Epoch: 2 [205824/2383430 (8.6%)]	Loss: 0.095332
Train Epoch: 2 [257024/2383430 (10.8%)]	Loss: 0.129104
Train Epoch: 2 [308224/2383430 (12.9%)]	Loss: 0.129251
Train Epoch: 2 [359424/2383430 (15.1%)]	Loss: 0.117506
Train Epoch: 2 [410624/2383430 (17.2%)]	Loss: 0.120118
Train Epoch: 2 [461824/2383430 (19.4%)]	Loss: 0.129439
Train Epoch: 2 [513024/2383430 (21.5%)]	Loss: 0.129055
Train Epoch: 2 [564224/2383430 (23.7%)]	Loss: 0.123876
Train Epoch: 2 [615424/2383430 (25.8%)]	Loss: 0.122324
Train Epoch: 2 [666624/2383430 (28.0%)]	Loss: 0.109732
Train Epoch: 2 [717824/2383430 (30.1%)]	Loss: 0.117763
Train Epoch: 2 [769024/2383430 (32.3%)]	Loss: 0.122679
Train Epoch: 2 [820224/2383430 (34.4%)]	Loss: 0.116169
Train Epoch: 2 [871424/2383430 (36.6%)]	Loss: 0.116678
Train Epoch: 2 [922624/2383430 (38.7%)]	Loss: 0.120420
Train Epoch: 2 [973824/2383430 (40.9%)]	Loss: 0.113923
Train Epoch: 2 [1025024/2383430 (43.0%)]	Loss: 0.116129
Train Epoch: 2 [1076224/2383430 (45.2%)]	Loss: 0.106348
Train Epoch: 2 [1127424/2383430 (47.3%)]	Loss: 0.125476
Train Epoch: 2 [1178624/2383430 (49.5%)]	Loss: 0.124544
Train Epoch: 2 [1229824/2383430 (51.6%)]	Loss: 0.138512
Train Epoch: 2 [1281024/2383430 (53.7%)]	Loss: 0.125259
Train Epoch: 2 [1332224/2383430 (55.9%)]	Loss: 0.115601
Train Epoch: 2 [1383424/2383430 (58.0%)]	Loss: 0.109456
Train Epoch: 2 [1434624/2383430 (60.2%)]	Loss: 0.117076
Train Epoch: 2 [1485824/2383430 (62.3%)]	Loss: 0.117442
Train Epoch: 2 [1537024/2383430 (64.5%)]	Loss: 0.113265
Train Epoch: 2 [1588224/2383430 (66.6%)]	Loss: 0.117184
Train Epoch: 2 [1639424/2383430 (68.8%)]	Loss: 0.131167
Train Epoch: 2 [1690624/2383430 (70.9%)]	Loss: 0.112494
Train Epoch: 2 [1741824/2383430 (73.1%)]	Loss: 0.119125
Train Epoch: 2 [1793024/2383430 (75.2%)]	Loss: 0.114507
Train Epoch: 2 [1844224/2383430 (77.4%)]	Loss: 0.132579
Train Epoch: 2 [1895424/2383430 (79.5%)]	Loss: 0.118300
Train Epoch: 2 [1946624/2383430 (81.7%)]	Loss: 0.117507
Train Epoch: 2 [1997824/2383430 (83.8%)]	Loss: 0.119759
Train Epoch: 2 [2049024/2383430 (86.0%)]	Loss: 0.118111
Train Epoch: 2 [2100224/2383430 (88.1%)]	Loss: 0.116000
Train Epoch: 2 [2151424/2383430 (90.3%)]	Loss: 0.127646
Train Epoch: 2 [2202624/2383430 (92.4%)]	Loss: 0.110641
Train Epoch: 2 [2253824/2383430 (94.6%)]	Loss: 0.121962
Train Epoch: 2 [2305024/2383430 (96.7%)]	Loss: 0.118402
Train Epoch: 2 [2356224/2383430 (98.9%)]	Loss: 0.120579

 ---------------------------------------------------------------------- 


ACC in fold#1 was 0.909


Confusion Matrix in fold#1: 
           nonRipple  Ripple
nonRipple     213879   22957
Ripple         31173  327849


Classification Report in fold#1: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.873       0.935     0.909       0.904         0.910
recall            0.903       0.913     0.909       0.908         0.909
f1-score          0.888       0.924     0.909       0.906         0.909
sample size  236836.000  359022.000     0.909  595858.000    595858.000


PR_AUC in fold#1 was 0.971


ROC_AUC in fold#1 was 0.967

Time (id:2): tot 02:02:23, prev 01:01:09 [hh:mm:ss]: 
i_fold=1 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383430 (0.0%)]	Loss: 0.354783
Train Epoch: 1 [52224/2383430 (2.2%)]	Loss: 0.169027
Train Epoch: 1 [103424/2383430 (4.3%)]	Loss: 0.131242
Train Epoch: 1 [154624/2383430 (6.5%)]	Loss: 0.110978
Train Epoch: 1 [205824/2383430 (8.6%)]	Loss: 0.116334
Train Epoch: 1 [257024/2383430 (10.8%)]	Loss: 0.122560
Train Epoch: 1 [308224/2383430 (12.9%)]	Loss: 0.133264
Train Epoch: 1 [359424/2383430 (15.1%)]	Loss: 0.118332
Train Epoch: 1 [410624/2383430 (17.2%)]	Loss: 0.126297
Train Epoch: 1 [461824/2383430 (19.4%)]	Loss: 0.120234
Train Epoch: 1 [513024/2383430 (21.5%)]	Loss: 0.128468
Train Epoch: 1 [564224/2383430 (23.7%)]	Loss: 0.112697
Train Epoch: 1 [615424/2383430 (25.8%)]	Loss: 0.128200
Train Epoch: 1 [666624/2383430 (28.0%)]	Loss: 0.120271
Train Epoch: 1 [717824/2383430 (30.1%)]	Loss: 0.138027
Train Epoch: 1 [769024/2383430 (32.3%)]	Loss: 0.121642
Train Epoch: 1 [820224/2383430 (34.4%)]	Loss: 0.130086
Train Epoch: 1 [871424/2383430 (36.6%)]	Loss: 0.125415
Train Epoch: 1 [922624/2383430 (38.7%)]	Loss: 0.104553
Train Epoch: 1 [973824/2383430 (40.9%)]	Loss: 0.118789
Train Epoch: 1 [1025024/2383430 (43.0%)]	Loss: 0.119381
Train Epoch: 1 [1076224/2383430 (45.2%)]	Loss: 0.123287
Train Epoch: 1 [1127424/2383430 (47.3%)]	Loss: 0.101636
Train Epoch: 1 [1178624/2383430 (49.5%)]	Loss: 0.135774
Train Epoch: 1 [1229824/2383430 (51.6%)]	Loss: 0.120056
Train Epoch: 1 [1281024/2383430 (53.7%)]	Loss: 0.109206
Train Epoch: 1 [1332224/2383430 (55.9%)]	Loss: 0.123863
Train Epoch: 1 [1383424/2383430 (58.0%)]	Loss: 0.118735
Train Epoch: 1 [1434624/2383430 (60.2%)]	Loss: 0.103359
Train Epoch: 1 [1485824/2383430 (62.3%)]	Loss: 0.125962
Train Epoch: 1 [1537024/2383430 (64.5%)]	Loss: 0.109351
Train Epoch: 1 [1588224/2383430 (66.6%)]	Loss: 0.127027
Train Epoch: 1 [1639424/2383430 (68.8%)]	Loss: 0.118917
Train Epoch: 1 [1690624/2383430 (70.9%)]	Loss: 0.120244
Train Epoch: 1 [1741824/2383430 (73.1%)]	Loss: 0.122708
Train Epoch: 1 [1793024/2383430 (75.2%)]	Loss: 0.121793
Train Epoch: 1 [1844224/2383430 (77.4%)]	Loss: 0.121651
Train Epoch: 1 [1895424/2383430 (79.5%)]	Loss: 0.125310
Train Epoch: 1 [1946624/2383430 (81.7%)]	Loss: 0.129189
Train Epoch: 1 [1997824/2383430 (83.8%)]	Loss: 0.122664
Train Epoch: 1 [2049024/2383430 (86.0%)]	Loss: 0.089845
Train Epoch: 1 [2100224/2383430 (88.1%)]	Loss: 0.103271
Train Epoch: 1 [2151424/2383430 (90.3%)]	Loss: 0.133946
Train Epoch: 1 [2202624/2383430 (92.4%)]	Loss: 0.127193
Train Epoch: 1 [2253824/2383430 (94.6%)]	Loss: 0.120645
Train Epoch: 1 [2305024/2383430 (96.7%)]	Loss: 0.120970
Train Epoch: 1 [2356224/2383430 (98.9%)]	Loss: 0.123594
Train Epoch: 2 [1024/2383430 (0.0%)]	Loss: 0.112147
Train Epoch: 2 [52224/2383430 (2.2%)]	Loss: 0.112716
Train Epoch: 2 [103424/2383430 (4.3%)]	Loss: 0.124898
Train Epoch: 2 [154624/2383430 (6.5%)]	Loss: 0.103965
Train Epoch: 2 [205824/2383430 (8.6%)]	Loss: 0.113235
Train Epoch: 2 [257024/2383430 (10.8%)]	Loss: 0.115923
Train Epoch: 2 [308224/2383430 (12.9%)]	Loss: 0.122134
Train Epoch: 2 [359424/2383430 (15.1%)]	Loss: 0.133981
Train Epoch: 2 [410624/2383430 (17.2%)]	Loss: 0.122382
Train Epoch: 2 [461824/2383430 (19.4%)]	Loss: 0.103361
Train Epoch: 2 [513024/2383430 (21.5%)]	Loss: 0.140858
Train Epoch: 2 [564224/2383430 (23.7%)]	Loss: 0.124296
Train Epoch: 2 [615424/2383430 (25.8%)]	Loss: 0.114294
Train Epoch: 2 [666624/2383430 (28.0%)]	Loss: 0.119446
Train Epoch: 2 [717824/2383430 (30.1%)]	Loss: 0.107497
Train Epoch: 2 [769024/2383430 (32.3%)]	Loss: 0.115833
Train Epoch: 2 [820224/2383430 (34.4%)]	Loss: 0.108961
Train Epoch: 2 [871424/2383430 (36.6%)]	Loss: 0.116944
Train Epoch: 2 [922624/2383430 (38.7%)]	Loss: 0.130072
Train Epoch: 2 [973824/2383430 (40.9%)]	Loss: 0.130849
Train Epoch: 2 [1025024/2383430 (43.0%)]	Loss: 0.116296
Train Epoch: 2 [1076224/2383430 (45.2%)]	Loss: 0.125035
Train Epoch: 2 [1127424/2383430 (47.3%)]	Loss: 0.125173
Train Epoch: 2 [1178624/2383430 (49.5%)]	Loss: 0.117012
Train Epoch: 2 [1229824/2383430 (51.6%)]	Loss: 0.117270
Train Epoch: 2 [1281024/2383430 (53.7%)]	Loss: 0.114406
Train Epoch: 2 [1332224/2383430 (55.9%)]	Loss: 0.119022
Train Epoch: 2 [1383424/2383430 (58.0%)]	Loss: 0.131717
Train Epoch: 2 [1434624/2383430 (60.2%)]	Loss: 0.113359
Train Epoch: 2 [1485824/2383430 (62.3%)]	Loss: 0.108682
Train Epoch: 2 [1537024/2383430 (64.5%)]	Loss: 0.121557
Train Epoch: 2 [1588224/2383430 (66.6%)]	Loss: 0.109072
Train Epoch: 2 [1639424/2383430 (68.8%)]	Loss: 0.107445
Train Epoch: 2 [1690624/2383430 (70.9%)]	Loss: 0.119711
Train Epoch: 2 [1741824/2383430 (73.1%)]	Loss: 0.098828
Train Epoch: 2 [1793024/2383430 (75.2%)]	Loss: 0.131690
Train Epoch: 2 [1844224/2383430 (77.4%)]	Loss: 0.115134
Train Epoch: 2 [1895424/2383430 (79.5%)]	Loss: 0.111638
Train Epoch: 2 [1946624/2383430 (81.7%)]	Loss: 0.116625
Train Epoch: 2 [1997824/2383430 (83.8%)]	Loss: 0.119720
Train Epoch: 2 [2049024/2383430 (86.0%)]	Loss: 0.108317
Train Epoch: 2 [2100224/2383430 (88.1%)]	Loss: 0.120875
Train Epoch: 2 [2151424/2383430 (90.3%)]	Loss: 0.122478
Train Epoch: 2 [2202624/2383430 (92.4%)]	Loss: 0.114344
Train Epoch: 2 [2253824/2383430 (94.6%)]	Loss: 0.110546
Train Epoch: 2 [2305024/2383430 (96.7%)]	Loss: 0.106831
Train Epoch: 2 [2356224/2383430 (98.9%)]	Loss: 0.115438

 ---------------------------------------------------------------------- 


ACC in fold#2 was 0.906


Confusion Matrix in fold#2: 
           nonRipple  Ripple
nonRipple     200013   36823
Ripple         19384  339638


Classification Report in fold#2: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.912       0.902     0.906       0.907         0.906
recall            0.845       0.946     0.906       0.895         0.906
f1-score          0.877       0.924     0.906       0.900         0.905
sample size  236836.000  359022.000     0.906  595858.000    595858.000


PR_AUC in fold#2 was 0.973


ROC_AUC in fold#2 was 0.967

Time (id:3): tot 03:03:53, prev 01:01:30 [hh:mm:ss]: 
i_fold=2 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383431 (0.0%)]	Loss: 0.343213
Train Epoch: 1 [52224/2383431 (2.2%)]	Loss: 0.179433
Train Epoch: 1 [103424/2383431 (4.3%)]	Loss: 0.142535
Train Epoch: 1 [154624/2383431 (6.5%)]	Loss: 0.126944
Train Epoch: 1 [205824/2383431 (8.6%)]	Loss: 0.134543
Train Epoch: 1 [257024/2383431 (10.8%)]	Loss: 0.129120
Train Epoch: 1 [308224/2383431 (12.9%)]	Loss: 0.116336
Train Epoch: 1 [359424/2383431 (15.1%)]	Loss: 0.128619
Train Epoch: 1 [410624/2383431 (17.2%)]	Loss: 0.135047
Train Epoch: 1 [461824/2383431 (19.4%)]	Loss: 0.123500
Train Epoch: 1 [513024/2383431 (21.5%)]	Loss: 0.128028
Train Epoch: 1 [564224/2383431 (23.7%)]	Loss: 0.140352
Train Epoch: 1 [615424/2383431 (25.8%)]	Loss: 0.110102
Train Epoch: 1 [666624/2383431 (28.0%)]	Loss: 0.114039
Train Epoch: 1 [717824/2383431 (30.1%)]	Loss: 0.138853
Train Epoch: 1 [769024/2383431 (32.3%)]	Loss: 0.105137
Train Epoch: 1 [820224/2383431 (34.4%)]	Loss: 0.124203
Train Epoch: 1 [871424/2383431 (36.6%)]	Loss: 0.113228
Train Epoch: 1 [922624/2383431 (38.7%)]	Loss: 0.120798
Train Epoch: 1 [973824/2383431 (40.9%)]	Loss: 0.115137
Train Epoch: 1 [1025024/2383431 (43.0%)]	Loss: 0.109159
Train Epoch: 1 [1076224/2383431 (45.2%)]	Loss: 0.120759
Train Epoch: 1 [1127424/2383431 (47.3%)]	Loss: 0.128056
Train Epoch: 1 [1178624/2383431 (49.5%)]	Loss: 0.138749
Train Epoch: 1 [1229824/2383431 (51.6%)]	Loss: 0.103553
Train Epoch: 1 [1281024/2383431 (53.7%)]	Loss: 0.123096
Train Epoch: 1 [1332224/2383431 (55.9%)]	Loss: 0.117516
Train Epoch: 1 [1383424/2383431 (58.0%)]	Loss: 0.119434
Train Epoch: 1 [1434624/2383431 (60.2%)]	Loss: 0.115705
Train Epoch: 1 [1485824/2383431 (62.3%)]	Loss: 0.117927
Train Epoch: 1 [1537024/2383431 (64.5%)]	Loss: 0.125349
Train Epoch: 1 [1588224/2383431 (66.6%)]	Loss: 0.120352
Train Epoch: 1 [1639424/2383431 (68.8%)]	Loss: 0.127451
Train Epoch: 1 [1690624/2383431 (70.9%)]	Loss: 0.124971
Train Epoch: 1 [1741824/2383431 (73.1%)]	Loss: 0.109879
Train Epoch: 1 [1793024/2383431 (75.2%)]	Loss: 0.119437
Train Epoch: 1 [1844224/2383431 (77.4%)]	Loss: 0.100604
Train Epoch: 1 [1895424/2383431 (79.5%)]	Loss: 0.107565
Train Epoch: 1 [1946624/2383431 (81.7%)]	Loss: 0.115407
Train Epoch: 1 [1997824/2383431 (83.8%)]	Loss: 0.116845
Train Epoch: 1 [2049024/2383431 (86.0%)]	Loss: 0.119943
Train Epoch: 1 [2100224/2383431 (88.1%)]	Loss: 0.141987
Train Epoch: 1 [2151424/2383431 (90.3%)]	Loss: 0.111987
Train Epoch: 1 [2202624/2383431 (92.4%)]	Loss: 0.130086
Train Epoch: 1 [2253824/2383431 (94.6%)]	Loss: 0.104591
Train Epoch: 1 [2305024/2383431 (96.7%)]	Loss: 0.131657
Train Epoch: 1 [2356224/2383431 (98.9%)]	Loss: 0.105168
Train Epoch: 2 [1024/2383431 (0.0%)]	Loss: 0.122179
Train Epoch: 2 [52224/2383431 (2.2%)]	Loss: 0.117150
Train Epoch: 2 [103424/2383431 (4.3%)]	Loss: 0.124205
Train Epoch: 2 [154624/2383431 (6.5%)]	Loss: 0.108582
Train Epoch: 2 [205824/2383431 (8.6%)]	Loss: 0.114934
Train Epoch: 2 [257024/2383431 (10.8%)]	Loss: 0.106767
Train Epoch: 2 [308224/2383431 (12.9%)]	Loss: 0.126013
Train Epoch: 2 [359424/2383431 (15.1%)]	Loss: 0.110257
Train Epoch: 2 [410624/2383431 (17.2%)]	Loss: 0.101813
Train Epoch: 2 [461824/2383431 (19.4%)]	Loss: 0.118838
Train Epoch: 2 [513024/2383431 (21.5%)]	Loss: 0.105071
Train Epoch: 2 [564224/2383431 (23.7%)]	Loss: 0.125483
Train Epoch: 2 [615424/2383431 (25.8%)]	Loss: 0.112150
Train Epoch: 2 [666624/2383431 (28.0%)]	Loss: 0.113726
Train Epoch: 2 [717824/2383431 (30.1%)]	Loss: 0.119807
Train Epoch: 2 [769024/2383431 (32.3%)]	Loss: 0.112279
Train Epoch: 2 [820224/2383431 (34.4%)]	Loss: 0.120454
Train Epoch: 2 [871424/2383431 (36.6%)]	Loss: 0.124005
Train Epoch: 2 [922624/2383431 (38.7%)]	Loss: 0.121587
Train Epoch: 2 [973824/2383431 (40.9%)]	Loss: 0.110298
Train Epoch: 2 [1025024/2383431 (43.0%)]	Loss: 0.120382
Train Epoch: 2 [1076224/2383431 (45.2%)]	Loss: 0.123297
Train Epoch: 2 [1127424/2383431 (47.3%)]	Loss: 0.110812
Train Epoch: 2 [1178624/2383431 (49.5%)]	Loss: 0.105780
Train Epoch: 2 [1229824/2383431 (51.6%)]	Loss: 0.118611
Train Epoch: 2 [1281024/2383431 (53.7%)]	Loss: 0.114251
Train Epoch: 2 [1332224/2383431 (55.9%)]	Loss: 0.118311
Train Epoch: 2 [1383424/2383431 (58.0%)]	Loss: 0.110452
Train Epoch: 2 [1434624/2383431 (60.2%)]	Loss: 0.125629
Train Epoch: 2 [1485824/2383431 (62.3%)]	Loss: 0.117838
Train Epoch: 2 [1537024/2383431 (64.5%)]	Loss: 0.135348
Train Epoch: 2 [1588224/2383431 (66.6%)]	Loss: 0.127630
Train Epoch: 2 [1639424/2383431 (68.8%)]	Loss: 0.105459
Train Epoch: 2 [1690624/2383431 (70.9%)]	Loss: 0.115728
Train Epoch: 2 [1741824/2383431 (73.1%)]	Loss: 0.110451
Train Epoch: 2 [1793024/2383431 (75.2%)]	Loss: 0.106137
Train Epoch: 2 [1844224/2383431 (77.4%)]	Loss: 0.094588
Train Epoch: 2 [1895424/2383431 (79.5%)]	Loss: 0.130311
Train Epoch: 2 [1946624/2383431 (81.7%)]	Loss: 0.115407
Train Epoch: 2 [1997824/2383431 (83.8%)]	Loss: 0.117080
Train Epoch: 2 [2049024/2383431 (86.0%)]	Loss: 0.105816
Train Epoch: 2 [2100224/2383431 (88.1%)]	Loss: 0.114278
Train Epoch: 2 [2151424/2383431 (90.3%)]	Loss: 0.117893
Train Epoch: 2 [2202624/2383431 (92.4%)]	Loss: 0.104381
Train Epoch: 2 [2253824/2383431 (94.6%)]	Loss: 0.108408
Train Epoch: 2 [2305024/2383431 (96.7%)]	Loss: 0.109688
Train Epoch: 2 [2356224/2383431 (98.9%)]	Loss: 0.115227

 ---------------------------------------------------------------------- 


ACC in fold#3 was 0.885


Confusion Matrix in fold#3: 
           nonRipple  Ripple
nonRipple     218641   18195
Ripple         50165  308856


Classification Report in fold#3: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.813       0.944     0.885       0.879         0.892
recall            0.923       0.860     0.885       0.892         0.885
f1-score          0.865       0.900     0.885       0.883         0.886
sample size  236836.000  359021.000     0.885  595857.000    595857.000


PR_AUC in fold#3 was 0.975


ROC_AUC in fold#3 was 0.965

Time (id:4): tot 04:05:32, prev 01:01:39 [hh:mm:ss]: 
i_fold=3 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383431 (0.0%)]	Loss: 0.343601
Train Epoch: 1 [52224/2383431 (2.2%)]	Loss: 0.164194
Train Epoch: 1 [103424/2383431 (4.3%)]	Loss: 0.141418
Train Epoch: 1 [154624/2383431 (6.5%)]	Loss: 0.126053
Train Epoch: 1 [205824/2383431 (8.6%)]	Loss: 0.123310
Train Epoch: 1 [257024/2383431 (10.8%)]	Loss: 0.123074
Train Epoch: 1 [308224/2383431 (12.9%)]	Loss: 0.121602
Train Epoch: 1 [359424/2383431 (15.1%)]	Loss: 0.118148
Train Epoch: 1 [410624/2383431 (17.2%)]	Loss: 0.112512
Train Epoch: 1 [461824/2383431 (19.4%)]	Loss: 0.101917
Train Epoch: 1 [513024/2383431 (21.5%)]	Loss: 0.111618
Train Epoch: 1 [564224/2383431 (23.7%)]	Loss: 0.109925
Train Epoch: 1 [615424/2383431 (25.8%)]	Loss: 0.120731
Train Epoch: 1 [666624/2383431 (28.0%)]	Loss: 0.124254
Train Epoch: 1 [717824/2383431 (30.1%)]	Loss: 0.121869
Train Epoch: 1 [769024/2383431 (32.3%)]	Loss: 0.117904
Train Epoch: 1 [820224/2383431 (34.4%)]	Loss: 0.107818
Train Epoch: 1 [871424/2383431 (36.6%)]	Loss: 0.123758
Train Epoch: 1 [922624/2383431 (38.7%)]	Loss: 0.123463
Train Epoch: 1 [973824/2383431 (40.9%)]	Loss: 0.118065
Train Epoch: 1 [1025024/2383431 (43.0%)]	Loss: 0.124475
Train Epoch: 1 [1076224/2383431 (45.2%)]	Loss: 0.118953
Train Epoch: 1 [1127424/2383431 (47.3%)]	Loss: 0.116181
Train Epoch: 1 [1178624/2383431 (49.5%)]	Loss: 0.129103
Train Epoch: 1 [1229824/2383431 (51.6%)]	Loss: 0.108579
Train Epoch: 1 [1281024/2383431 (53.7%)]	Loss: 0.120309
Train Epoch: 1 [1332224/2383431 (55.9%)]	Loss: 0.127800
Train Epoch: 1 [1383424/2383431 (58.0%)]	Loss: 0.110828
Train Epoch: 1 [1434624/2383431 (60.2%)]	Loss: 0.108549
Train Epoch: 1 [1485824/2383431 (62.3%)]	Loss: 0.111002
Train Epoch: 1 [1537024/2383431 (64.5%)]	Loss: 0.115492
Train Epoch: 1 [1588224/2383431 (66.6%)]	Loss: 0.131961
Train Epoch: 1 [1639424/2383431 (68.8%)]	Loss: 0.112039
Train Epoch: 1 [1690624/2383431 (70.9%)]	Loss: 0.115271
Train Epoch: 1 [1741824/2383431 (73.1%)]	Loss: 0.120412
Train Epoch: 1 [1793024/2383431 (75.2%)]	Loss: 0.110105
Train Epoch: 1 [1844224/2383431 (77.4%)]	Loss: 0.106017
Train Epoch: 1 [1895424/2383431 (79.5%)]	Loss: 0.124044
Train Epoch: 1 [1946624/2383431 (81.7%)]	Loss: 0.108553
Train Epoch: 1 [1997824/2383431 (83.8%)]	Loss: 0.101975
Train Epoch: 1 [2049024/2383431 (86.0%)]	Loss: 0.111581
Train Epoch: 1 [2100224/2383431 (88.1%)]	Loss: 0.104987
Train Epoch: 1 [2151424/2383431 (90.3%)]	Loss: 0.113928
Train Epoch: 1 [2202624/2383431 (92.4%)]	Loss: 0.126416
Train Epoch: 1 [2253824/2383431 (94.6%)]	Loss: 0.102429
Train Epoch: 1 [2305024/2383431 (96.7%)]	Loss: 0.129239
Train Epoch: 1 [2356224/2383431 (98.9%)]	Loss: 0.111733
Train Epoch: 2 [1024/2383431 (0.0%)]	Loss: 0.126428
Train Epoch: 2 [52224/2383431 (2.2%)]	Loss: 0.107173
Train Epoch: 2 [103424/2383431 (4.3%)]	Loss: 0.119208
Train Epoch: 2 [154624/2383431 (6.5%)]	Loss: 0.111654
Train Epoch: 2 [205824/2383431 (8.6%)]	Loss: 0.119391
Train Epoch: 2 [257024/2383431 (10.8%)]	Loss: 0.113515
Train Epoch: 2 [308224/2383431 (12.9%)]	Loss: 0.117580
Train Epoch: 2 [359424/2383431 (15.1%)]	Loss: 0.123090
Train Epoch: 2 [410624/2383431 (17.2%)]	Loss: 0.109613
Train Epoch: 2 [461824/2383431 (19.4%)]	Loss: 0.140014
Train Epoch: 2 [513024/2383431 (21.5%)]	Loss: 0.110882
Train Epoch: 2 [564224/2383431 (23.7%)]	Loss: 0.097337
Train Epoch: 2 [615424/2383431 (25.8%)]	Loss: 0.107294
Train Epoch: 2 [666624/2383431 (28.0%)]	Loss: 0.119936
Train Epoch: 2 [717824/2383431 (30.1%)]	Loss: 0.108574
Train Epoch: 2 [769024/2383431 (32.3%)]	Loss: 0.100221
Train Epoch: 2 [820224/2383431 (34.4%)]	Loss: 0.089837
Train Epoch: 2 [871424/2383431 (36.6%)]	Loss: 0.110841
Train Epoch: 2 [922624/2383431 (38.7%)]	Loss: 0.112119
Train Epoch: 2 [973824/2383431 (40.9%)]	Loss: 0.105136
Train Epoch: 2 [1025024/2383431 (43.0%)]	Loss: 0.122912
Train Epoch: 2 [1076224/2383431 (45.2%)]	Loss: 0.124339
Train Epoch: 2 [1127424/2383431 (47.3%)]	Loss: 0.100386
Train Epoch: 2 [1178624/2383431 (49.5%)]	Loss: 0.099914
Train Epoch: 2 [1229824/2383431 (51.6%)]	Loss: 0.102861
Train Epoch: 2 [1281024/2383431 (53.7%)]	Loss: 0.107444
Train Epoch: 2 [1332224/2383431 (55.9%)]	Loss: 0.117656
Train Epoch: 2 [1383424/2383431 (58.0%)]	Loss: 0.106743
Train Epoch: 2 [1434624/2383431 (60.2%)]	Loss: 0.118226
Train Epoch: 2 [1485824/2383431 (62.3%)]	Loss: 0.114165
Train Epoch: 2 [1537024/2383431 (64.5%)]	Loss: 0.113186
Train Epoch: 2 [1588224/2383431 (66.6%)]	Loss: 0.108542
Train Epoch: 2 [1639424/2383431 (68.8%)]	Loss: 0.106454
Train Epoch: 2 [1690624/2383431 (70.9%)]	Loss: 0.093681
Train Epoch: 2 [1741824/2383431 (73.1%)]	Loss: 0.109051
Train Epoch: 2 [1793024/2383431 (75.2%)]	Loss: 0.102432
Train Epoch: 2 [1844224/2383431 (77.4%)]	Loss: 0.130896
Train Epoch: 2 [1895424/2383431 (79.5%)]	Loss: 0.098623
Train Epoch: 2 [1946624/2383431 (81.7%)]	Loss: 0.101400
Train Epoch: 2 [1997824/2383431 (83.8%)]	Loss: 0.109981
Train Epoch: 2 [2049024/2383431 (86.0%)]	Loss: 0.091532
Train Epoch: 2 [2100224/2383431 (88.1%)]	Loss: 0.111510
Train Epoch: 2 [2151424/2383431 (90.3%)]	Loss: 0.112896
Train Epoch: 2 [2202624/2383431 (92.4%)]	Loss: 0.122773
Train Epoch: 2 [2253824/2383431 (94.6%)]	Loss: 0.114984
Train Epoch: 2 [2305024/2383431 (96.7%)]	Loss: 0.103082
Train Epoch: 2 [2356224/2383431 (98.9%)]	Loss: 0.104252

 ---------------------------------------------------------------------- 


ACC in fold#4 was 0.865


Confusion Matrix in fold#4: 
           nonRipple  Ripple
nonRipple     207492   29343
Ripple         51390  307632


Classification Report in fold#4: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.801       0.913     0.865       0.857         0.869
recall            0.876       0.857     0.865       0.866         0.865
f1-score          0.837       0.884     0.865       0.861         0.865
sample size  236835.000  359022.000     0.865  595857.000    595857.000


PR_AUC in fold#4 was 0.961


ROC_AUC in fold#4 was 0.948

Time (id:5): tot 05:07:08, prev 01:01:35 [hh:mm:ss]: 
i_fold=4 ends.



 ---------------------------------------------------------------------- 


Label Errors Rate:
0.030


 --- 5-fold CV overall metrics --- 


Confusion Matrix (Test; sum; num. folds=5)
           nonRipple   Ripple
nonRipple    1042513   141666
Ripple        174402  1620707


Classification Report (Test; mean; num. folds=5)
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.860       0.920     0.894       0.890         0.896
recall            0.880       0.903     0.894       0.891         0.894
f1-score          0.869       0.911     0.894       0.890         0.894
sample size  236835.800  359021.800     0.894  595857.600    595857.600


Classification Report (Test; std; num. folds=5)
             nonRipple  Ripple  accuracy  macro avg  weighted avg
precision        0.045   0.016     0.017      0.019         0.015
recall           0.029   0.038     0.017      0.014         0.017
f1-score         0.017   0.016     0.017      0.016         0.016
sample size      0.400   0.400     0.017      0.490         0.490


PRE-REC AUC Score: 0.97 +/- 0.005 (mean +/- std.; n=5)


ROC AUC Score: 0.963 +/- 0.007 (mean +/- std.; n=5)


Saved to: ./data/okada/cleanlab_results/D03+/are_errors.npy


Saved to: ./data/okada/cleanlab_results/D03+/cleaned_labels.npy


Saved to: ./data/okada/cleanlab_results/D03+/pred_probas_ripples.npy

(Moved to: /tmp/2021-0513-2039-D03+-conf_mats.csv)
Saved to: ./data/okada/cleanlab_results/D03+/conf_mats.csv
Saved to: ./data/okada/cleanlab_results/D03+/conf_mat_overall_sum.png
(Moved to: /tmp/2021-0513-2039-D03+-clf_reports.csv)
Saved to: ./data/okada/cleanlab_results/D03+/clf_reports.csv
(Moved to: /tmp/2021-0513-2039-D03+-roc-auc.csv)
Saved to: ./data/okada/cleanlab_results/D03+/roc-auc.csv

Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-4_fp16.pkl

