
Random seeds have been fixed as 42


Random seeds have been fixed as 42

Indice of mice to load: ['04']
48
Time (id:0): tot 00:00:00, prev 00:00:00 [hh:mm:ss]: Reporter has been initialized.

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2777098 (0.0%)]	Loss: 0.364221
Train Epoch: 1 [52224/2777098 (1.9%)]	Loss: 0.086474
Train Epoch: 1 [103424/2777098 (3.7%)]	Loss: 0.047233
Train Epoch: 1 [154624/2777098 (5.6%)]	Loss: 0.049368
Train Epoch: 1 [205824/2777098 (7.4%)]	Loss: 0.064029
Train Epoch: 1 [257024/2777098 (9.3%)]	Loss: 0.044601
Train Epoch: 1 [308224/2777098 (11.1%)]	Loss: 0.052545
Train Epoch: 1 [359424/2777098 (12.9%)]	Loss: 0.045601
Train Epoch: 1 [410624/2777098 (14.8%)]	Loss: 0.055721
Train Epoch: 1 [461824/2777098 (16.6%)]	Loss: 0.047614
Train Epoch: 1 [513024/2777098 (18.5%)]	Loss: 0.054780
Train Epoch: 1 [564224/2777098 (20.3%)]	Loss: 0.055107
Train Epoch: 1 [615424/2777098 (22.2%)]	Loss: 0.038668
Train Epoch: 1 [666624/2777098 (24.0%)]	Loss: 0.045696
Train Epoch: 1 [717824/2777098 (25.8%)]	Loss: 0.059809
Train Epoch: 1 [769024/2777098 (27.7%)]	Loss: 0.041244
Train Epoch: 1 [820224/2777098 (29.5%)]	Loss: 0.052454
Train Epoch: 1 [871424/2777098 (31.4%)]	Loss: 0.052774
Train Epoch: 1 [922624/2777098 (33.2%)]	Loss: 0.043522
Train Epoch: 1 [973824/2777098 (35.1%)]	Loss: 0.040620
Train Epoch: 1 [1025024/2777098 (36.9%)]	Loss: 0.058544
Train Epoch: 1 [1076224/2777098 (38.8%)]	Loss: 0.037326
Train Epoch: 1 [1127424/2777098 (40.6%)]	Loss: 0.041096
Train Epoch: 1 [1178624/2777098 (42.4%)]	Loss: 0.036874
Train Epoch: 1 [1229824/2777098 (44.3%)]	Loss: 0.049953
Train Epoch: 1 [1281024/2777098 (46.1%)]	Loss: 0.046055
Train Epoch: 1 [1332224/2777098 (48.0%)]	Loss: 0.035900
Train Epoch: 1 [1383424/2777098 (49.8%)]	Loss: 0.040242
Train Epoch: 1 [1434624/2777098 (51.7%)]	Loss: 0.036710
Train Epoch: 1 [1485824/2777098 (53.5%)]	Loss: 0.047943
Train Epoch: 1 [1537024/2777098 (55.3%)]	Loss: 0.041028
Train Epoch: 1 [1588224/2777098 (57.2%)]	Loss: 0.050867
Train Epoch: 1 [1639424/2777098 (59.0%)]	Loss: 0.043619
Train Epoch: 1 [1690624/2777098 (60.9%)]	Loss: 0.041313
Train Epoch: 1 [1741824/2777098 (62.7%)]	Loss: 0.043877
Train Epoch: 1 [1793024/2777098 (64.6%)]	Loss: 0.040024
Train Epoch: 1 [1844224/2777098 (66.4%)]	Loss: 0.034483
Train Epoch: 1 [1895424/2777098 (68.3%)]	Loss: 0.028174
Train Epoch: 1 [1946624/2777098 (70.1%)]	Loss: 0.046546
Train Epoch: 1 [1997824/2777098 (71.9%)]	Loss: 0.046222
Train Epoch: 1 [2049024/2777098 (73.8%)]	Loss: 0.047167
Train Epoch: 1 [2100224/2777098 (75.6%)]	Loss: 0.042556
Train Epoch: 1 [2151424/2777098 (77.5%)]	Loss: 0.038959
Train Epoch: 1 [2202624/2777098 (79.3%)]	Loss: 0.035722
Train Epoch: 1 [2253824/2777098 (81.2%)]	Loss: 0.040940
Train Epoch: 1 [2305024/2777098 (83.0%)]	Loss: 0.037833
Train Epoch: 1 [2356224/2777098 (84.8%)]	Loss: 0.065212
Train Epoch: 1 [2407424/2777098 (86.7%)]	Loss: 0.042802
Train Epoch: 1 [2458624/2777098 (88.5%)]	Loss: 0.034737
Train Epoch: 1 [2509824/2777098 (90.4%)]	Loss: 0.042846
Train Epoch: 1 [2561024/2777098 (92.2%)]	Loss: 0.037323
Train Epoch: 1 [2612224/2777098 (94.1%)]	Loss: 0.035048
Train Epoch: 1 [2663424/2777098 (95.9%)]	Loss: 0.028771
Train Epoch: 1 [2714624/2777098 (97.8%)]	Loss: 0.037115
Train Epoch: 1 [2765824/2777098 (99.6%)]	Loss: 0.037889
Train Epoch: 2 [1024/2777098 (0.0%)]	Loss: 0.046309
Train Epoch: 2 [52224/2777098 (1.9%)]	Loss: 0.043130
Train Epoch: 2 [103424/2777098 (3.7%)]	Loss: 0.039190
Train Epoch: 2 [154624/2777098 (5.6%)]	Loss: 0.036225
Train Epoch: 2 [205824/2777098 (7.4%)]	Loss: 0.042158
Train Epoch: 2 [257024/2777098 (9.3%)]	Loss: 0.050929
Train Epoch: 2 [308224/2777098 (11.1%)]	Loss: 0.036702
Train Epoch: 2 [359424/2777098 (12.9%)]	Loss: 0.030342
Train Epoch: 2 [410624/2777098 (14.8%)]	Loss: 0.039316
Train Epoch: 2 [461824/2777098 (16.6%)]	Loss: 0.039144
Train Epoch: 2 [513024/2777098 (18.5%)]	Loss: 0.032989
Train Epoch: 2 [564224/2777098 (20.3%)]	Loss: 0.035212
Train Epoch: 2 [615424/2777098 (22.2%)]	Loss: 0.047110
Train Epoch: 2 [666624/2777098 (24.0%)]	Loss: 0.036347
Train Epoch: 2 [717824/2777098 (25.8%)]	Loss: 0.043051
Train Epoch: 2 [769024/2777098 (27.7%)]	Loss: 0.047190
Train Epoch: 2 [820224/2777098 (29.5%)]	Loss: 0.043388
Train Epoch: 2 [871424/2777098 (31.4%)]	Loss: 0.045079
Train Epoch: 2 [922624/2777098 (33.2%)]	Loss: 0.028384
Train Epoch: 2 [973824/2777098 (35.1%)]	Loss: 0.039483
Train Epoch: 2 [1025024/2777098 (36.9%)]	Loss: 0.040619
Train Epoch: 2 [1076224/2777098 (38.8%)]	Loss: 0.041871
Train Epoch: 2 [1127424/2777098 (40.6%)]	Loss: 0.036041
Train Epoch: 2 [1178624/2777098 (42.4%)]	Loss: 0.032886
Train Epoch: 2 [1229824/2777098 (44.3%)]	Loss: 0.045780
Train Epoch: 2 [1281024/2777098 (46.1%)]	Loss: 0.039470
Train Epoch: 2 [1332224/2777098 (48.0%)]	Loss: 0.042457
Train Epoch: 2 [1383424/2777098 (49.8%)]	Loss: 0.035698
Train Epoch: 2 [1434624/2777098 (51.7%)]	Loss: 0.040054
Train Epoch: 2 [1485824/2777098 (53.5%)]	Loss: 0.043054
Train Epoch: 2 [1537024/2777098 (55.3%)]	Loss: 0.038272
Train Epoch: 2 [1588224/2777098 (57.2%)]	Loss: 0.039092
Train Epoch: 2 [1639424/2777098 (59.0%)]	Loss: 0.038391
Train Epoch: 2 [1690624/2777098 (60.9%)]	Loss: 0.037773
Train Epoch: 2 [1741824/2777098 (62.7%)]	Loss: 0.037318
Train Epoch: 2 [1793024/2777098 (64.6%)]	Loss: 0.039466
Train Epoch: 2 [1844224/2777098 (66.4%)]	Loss: 0.041755
Train Epoch: 2 [1895424/2777098 (68.3%)]	Loss: 0.037079
Train Epoch: 2 [1946624/2777098 (70.1%)]	Loss: 0.022691
Train Epoch: 2 [1997824/2777098 (71.9%)]	Loss: 0.042915
Train Epoch: 2 [2049024/2777098 (73.8%)]	Loss: 0.036465
Train Epoch: 2 [2100224/2777098 (75.6%)]	Loss: 0.038618
Train Epoch: 2 [2151424/2777098 (77.5%)]	Loss: 0.042848
Train Epoch: 2 [2202624/2777098 (79.3%)]	Loss: 0.039327
Train Epoch: 2 [2253824/2777098 (81.2%)]	Loss: 0.038231
Train Epoch: 2 [2305024/2777098 (83.0%)]	Loss: 0.031024
Train Epoch: 2 [2356224/2777098 (84.8%)]	Loss: 0.039676
Train Epoch: 2 [2407424/2777098 (86.7%)]	Loss: 0.035976
Train Epoch: 2 [2458624/2777098 (88.5%)]	Loss: 0.042642
Train Epoch: 2 [2509824/2777098 (90.4%)]	Loss: 0.038173
Train Epoch: 2 [2561024/2777098 (92.2%)]	Loss: 0.038904
Train Epoch: 2 [2612224/2777098 (94.1%)]	Loss: 0.046681
Train Epoch: 2 [2663424/2777098 (95.9%)]	Loss: 0.044783
Train Epoch: 2 [2714624/2777098 (97.8%)]	Loss: 0.043742
Train Epoch: 2 [2765824/2777098 (99.6%)]	Loss: 0.030666

 ---------------------------------------------------------------------- 


ACC in fold#0 was 0.968


Confusion Matrix in fold#0: 
           nonRipple  Ripple
nonRipple     124110   20484
Ripple          1634  548047


Classification Report in fold#0: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.987       0.964     0.968       0.975         0.969
recall            0.858       0.997     0.968       0.928         0.968
f1-score          0.918       0.980     0.968       0.949         0.967
sample size  144594.000  549681.000     0.968  694275.000    694275.000


PR_AUC in fold#0 was 0.999


ROC_AUC in fold#0 was 0.997

Time (id:1): tot 01:13:43, prev 01:13:43 [hh:mm:ss]: 
i_fold=0 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2777098 (0.0%)]	Loss: 0.359009
Train Epoch: 1 [52224/2777098 (1.9%)]	Loss: 0.088505
Train Epoch: 1 [103424/2777098 (3.7%)]	Loss: 0.055572
Train Epoch: 1 [154624/2777098 (5.6%)]	Loss: 0.041667
Train Epoch: 1 [205824/2777098 (7.4%)]	Loss: 0.033098
Train Epoch: 1 [257024/2777098 (9.3%)]	Loss: 0.048745
Train Epoch: 1 [308224/2777098 (11.1%)]	Loss: 0.042622
Train Epoch: 1 [359424/2777098 (12.9%)]	Loss: 0.040474
Train Epoch: 1 [410624/2777098 (14.8%)]	Loss: 0.046543
Train Epoch: 1 [461824/2777098 (16.6%)]	Loss: 0.043195
Train Epoch: 1 [513024/2777098 (18.5%)]	Loss: 0.044263
Train Epoch: 1 [564224/2777098 (20.3%)]	Loss: 0.040246
Train Epoch: 1 [615424/2777098 (22.2%)]	Loss: 0.047592
Train Epoch: 1 [666624/2777098 (24.0%)]	Loss: 0.051402
Train Epoch: 1 [717824/2777098 (25.8%)]	Loss: 0.045539
Train Epoch: 1 [769024/2777098 (27.7%)]	Loss: 0.046664
Train Epoch: 1 [820224/2777098 (29.5%)]	Loss: 0.046001
Train Epoch: 1 [871424/2777098 (31.4%)]	Loss: 0.040684
Train Epoch: 1 [922624/2777098 (33.2%)]	Loss: 0.045173
Train Epoch: 1 [973824/2777098 (35.1%)]	Loss: 0.050695
Train Epoch: 1 [1025024/2777098 (36.9%)]	Loss: 0.042104
Train Epoch: 1 [1076224/2777098 (38.8%)]	Loss: 0.047314
Train Epoch: 1 [1127424/2777098 (40.6%)]	Loss: 0.048764
Train Epoch: 1 [1178624/2777098 (42.4%)]	Loss: 0.037127
Train Epoch: 1 [1229824/2777098 (44.3%)]	Loss: 0.040664
Train Epoch: 1 [1281024/2777098 (46.1%)]	Loss: 0.042883
Train Epoch: 1 [1332224/2777098 (48.0%)]	Loss: 0.030630
Train Epoch: 1 [1383424/2777098 (49.8%)]	Loss: 0.032751
Train Epoch: 1 [1434624/2777098 (51.7%)]	Loss: 0.038319
Train Epoch: 1 [1485824/2777098 (53.5%)]	Loss: 0.052460
Train Epoch: 1 [1537024/2777098 (55.3%)]	Loss: 0.041342
Train Epoch: 1 [1588224/2777098 (57.2%)]	Loss: 0.037666
Train Epoch: 1 [1639424/2777098 (59.0%)]	Loss: 0.036861
Train Epoch: 1 [1690624/2777098 (60.9%)]	Loss: 0.035997
Train Epoch: 1 [1741824/2777098 (62.7%)]	Loss: 0.052415
Train Epoch: 1 [1793024/2777098 (64.6%)]	Loss: 0.035119
Train Epoch: 1 [1844224/2777098 (66.4%)]	Loss: 0.026843
Train Epoch: 1 [1895424/2777098 (68.3%)]	Loss: 0.039833
Train Epoch: 1 [1946624/2777098 (70.1%)]	Loss: 0.029564
Train Epoch: 1 [1997824/2777098 (71.9%)]	Loss: 0.037102
Train Epoch: 1 [2049024/2777098 (73.8%)]	Loss: 0.031470
Train Epoch: 1 [2100224/2777098 (75.6%)]	Loss: 0.033614
Train Epoch: 1 [2151424/2777098 (77.5%)]	Loss: 0.035855
Train Epoch: 1 [2202624/2777098 (79.3%)]	Loss: 0.041456
Train Epoch: 1 [2253824/2777098 (81.2%)]	Loss: 0.042551
Train Epoch: 1 [2305024/2777098 (83.0%)]	Loss: 0.037627
Train Epoch: 1 [2356224/2777098 (84.8%)]	Loss: 0.032639
Train Epoch: 1 [2407424/2777098 (86.7%)]	Loss: 0.037771
Train Epoch: 1 [2458624/2777098 (88.5%)]	Loss: 0.030056
Train Epoch: 1 [2509824/2777098 (90.4%)]	Loss: 0.026536
Train Epoch: 1 [2561024/2777098 (92.2%)]	Loss: 0.039466
Train Epoch: 1 [2612224/2777098 (94.1%)]	Loss: 0.027767
Train Epoch: 1 [2663424/2777098 (95.9%)]	Loss: 0.040418
Train Epoch: 1 [2714624/2777098 (97.8%)]	Loss: 0.034665
Train Epoch: 1 [2765824/2777098 (99.6%)]	Loss: 0.036366
Train Epoch: 2 [1024/2777098 (0.0%)]	Loss: 0.040711
Train Epoch: 2 [52224/2777098 (1.9%)]	Loss: 0.032264
Train Epoch: 2 [103424/2777098 (3.7%)]	Loss: 0.035750
Train Epoch: 2 [154624/2777098 (5.6%)]	Loss: 0.031729
Train Epoch: 2 [205824/2777098 (7.4%)]	Loss: 0.029750
Train Epoch: 2 [257024/2777098 (9.3%)]	Loss: 0.031260
Train Epoch: 2 [308224/2777098 (11.1%)]	Loss: 0.028086
Train Epoch: 2 [359424/2777098 (12.9%)]	Loss: 0.034506
Train Epoch: 2 [410624/2777098 (14.8%)]	Loss: 0.033251
Train Epoch: 2 [461824/2777098 (16.6%)]	Loss: 0.036823
Train Epoch: 2 [513024/2777098 (18.5%)]	Loss: 0.042407
Train Epoch: 2 [564224/2777098 (20.3%)]	Loss: 0.033467
Train Epoch: 2 [615424/2777098 (22.2%)]	Loss: 0.037944
Train Epoch: 2 [666624/2777098 (24.0%)]	Loss: 0.026667
Train Epoch: 2 [717824/2777098 (25.8%)]	Loss: 0.035644
Train Epoch: 2 [769024/2777098 (27.7%)]	Loss: 0.029313
Train Epoch: 2 [820224/2777098 (29.5%)]	Loss: 0.026955
Train Epoch: 2 [871424/2777098 (31.4%)]	Loss: 0.034773
Train Epoch: 2 [922624/2777098 (33.2%)]	Loss: 0.022988
Train Epoch: 2 [973824/2777098 (35.1%)]	Loss: 0.034485
Train Epoch: 2 [1025024/2777098 (36.9%)]	Loss: 0.045177
Train Epoch: 2 [1076224/2777098 (38.8%)]	Loss: 0.028604
Train Epoch: 2 [1127424/2777098 (40.6%)]	Loss: 0.027714
Train Epoch: 2 [1178624/2777098 (42.4%)]	Loss: 0.028847
Train Epoch: 2 [1229824/2777098 (44.3%)]	Loss: 0.040191
Train Epoch: 2 [1281024/2777098 (46.1%)]	Loss: 0.037847
Train Epoch: 2 [1332224/2777098 (48.0%)]	Loss: 0.033327
Train Epoch: 2 [1383424/2777098 (49.8%)]	Loss: 0.035785
Train Epoch: 2 [1434624/2777098 (51.7%)]	Loss: 0.042804
Train Epoch: 2 [1485824/2777098 (53.5%)]	Loss: 0.032184
Train Epoch: 2 [1537024/2777098 (55.3%)]	Loss: 0.029110
Train Epoch: 2 [1588224/2777098 (57.2%)]	Loss: 0.026784
Train Epoch: 2 [1639424/2777098 (59.0%)]	Loss: 0.026261
Train Epoch: 2 [1690624/2777098 (60.9%)]	Loss: 0.035930
Train Epoch: 2 [1741824/2777098 (62.7%)]	Loss: 0.032802
Train Epoch: 2 [1793024/2777098 (64.6%)]	Loss: 0.030929
Train Epoch: 2 [1844224/2777098 (66.4%)]	Loss: 0.037169
Train Epoch: 2 [1895424/2777098 (68.3%)]	Loss: 0.027637
Train Epoch: 2 [1946624/2777098 (70.1%)]	Loss: 0.038057
Train Epoch: 2 [1997824/2777098 (71.9%)]	Loss: 0.033956
Train Epoch: 2 [2049024/2777098 (73.8%)]	Loss: 0.027249
Train Epoch: 2 [2100224/2777098 (75.6%)]	Loss: 0.030623
Train Epoch: 2 [2151424/2777098 (77.5%)]	Loss: 0.040353
Train Epoch: 2 [2202624/2777098 (79.3%)]	Loss: 0.041143
Train Epoch: 2 [2253824/2777098 (81.2%)]	Loss: 0.037264
Train Epoch: 2 [2305024/2777098 (83.0%)]	Loss: 0.037206
Train Epoch: 2 [2356224/2777098 (84.8%)]	Loss: 0.034245
Train Epoch: 2 [2407424/2777098 (86.7%)]	Loss: 0.024312
Train Epoch: 2 [2458624/2777098 (88.5%)]	Loss: 0.028520
Train Epoch: 2 [2509824/2777098 (90.4%)]	Loss: 0.026914
Train Epoch: 2 [2561024/2777098 (92.2%)]	Loss: 0.025633
Train Epoch: 2 [2612224/2777098 (94.1%)]	Loss: 0.023705
Train Epoch: 2 [2663424/2777098 (95.9%)]	Loss: 0.033282
Train Epoch: 2 [2714624/2777098 (97.8%)]	Loss: 0.041868
Train Epoch: 2 [2765824/2777098 (99.6%)]	Loss: 0.034386

 ---------------------------------------------------------------------- 


ACC in fold#1 was 0.963


Confusion Matrix in fold#1: 
           nonRipple  Ripple
nonRipple     123984   20611
Ripple          5080  544600


Classification Report in fold#1: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.961       0.964     0.963       0.962         0.963
recall            0.857       0.991     0.963       0.924         0.963
f1-score          0.906       0.977     0.963       0.942         0.962
sample size  144595.000  549680.000     0.963  694275.000    694275.000


PR_AUC in fold#1 was 0.997


ROC_AUC in fold#1 was 0.992

Time (id:2): tot 02:26:55, prev 01:13:11 [hh:mm:ss]: 
i_fold=1 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2777098 (0.0%)]	Loss: 0.340709
Train Epoch: 1 [52224/2777098 (1.9%)]	Loss: 0.093674
Train Epoch: 1 [103424/2777098 (3.7%)]	Loss: 0.059659
Train Epoch: 1 [154624/2777098 (5.6%)]	Loss: 0.044850
Train Epoch: 1 [205824/2777098 (7.4%)]	Loss: 0.052956
Train Epoch: 1 [257024/2777098 (9.3%)]	Loss: 0.044401
Train Epoch: 1 [308224/2777098 (11.1%)]	Loss: 0.045381
Train Epoch: 1 [359424/2777098 (12.9%)]	Loss: 0.044780
Train Epoch: 1 [410624/2777098 (14.8%)]	Loss: 0.045252
Train Epoch: 1 [461824/2777098 (16.6%)]	Loss: 0.046670
Train Epoch: 1 [513024/2777098 (18.5%)]	Loss: 0.046107
Train Epoch: 1 [564224/2777098 (20.3%)]	Loss: 0.044965
Train Epoch: 1 [615424/2777098 (22.2%)]	Loss: 0.040679
Train Epoch: 1 [666624/2777098 (24.0%)]	Loss: 0.042007
Train Epoch: 1 [717824/2777098 (25.8%)]	Loss: 0.040789
Train Epoch: 1 [769024/2777098 (27.7%)]	Loss: 0.041421
Train Epoch: 1 [820224/2777098 (29.5%)]	Loss: 0.036580
Train Epoch: 1 [871424/2777098 (31.4%)]	Loss: 0.040023
Train Epoch: 1 [922624/2777098 (33.2%)]	Loss: 0.043113
Train Epoch: 1 [973824/2777098 (35.1%)]	Loss: 0.040079
Train Epoch: 1 [1025024/2777098 (36.9%)]	Loss: 0.041059
Train Epoch: 1 [1076224/2777098 (38.8%)]	Loss: 0.031674
Train Epoch: 1 [1127424/2777098 (40.6%)]	Loss: 0.039142
Train Epoch: 1 [1178624/2777098 (42.4%)]	Loss: 0.038530
Train Epoch: 1 [1229824/2777098 (44.3%)]	Loss: 0.044907
Train Epoch: 1 [1281024/2777098 (46.1%)]	Loss: 0.047293
Train Epoch: 1 [1332224/2777098 (48.0%)]	Loss: 0.032018
Train Epoch: 1 [1383424/2777098 (49.8%)]	Loss: 0.037005
Train Epoch: 1 [1434624/2777098 (51.7%)]	Loss: 0.046700
Train Epoch: 1 [1485824/2777098 (53.5%)]	Loss: 0.039686
Train Epoch: 1 [1537024/2777098 (55.3%)]	Loss: 0.033996
Train Epoch: 1 [1588224/2777098 (57.2%)]	Loss: 0.030002
Train Epoch: 1 [1639424/2777098 (59.0%)]	Loss: 0.041558
Train Epoch: 1 [1690624/2777098 (60.9%)]	Loss: 0.041526
Train Epoch: 1 [1741824/2777098 (62.7%)]	Loss: 0.043451
Train Epoch: 1 [1793024/2777098 (64.6%)]	Loss: 0.031596
Train Epoch: 1 [1844224/2777098 (66.4%)]	Loss: 0.038087
Train Epoch: 1 [1895424/2777098 (68.3%)]	Loss: 0.035464
Train Epoch: 1 [1946624/2777098 (70.1%)]	Loss: 0.041266
Train Epoch: 1 [1997824/2777098 (71.9%)]	Loss: 0.041150
Train Epoch: 1 [2049024/2777098 (73.8%)]	Loss: 0.049205
Train Epoch: 1 [2100224/2777098 (75.6%)]	Loss: 0.046478
Train Epoch: 1 [2151424/2777098 (77.5%)]	Loss: 0.037041
Train Epoch: 1 [2202624/2777098 (79.3%)]	Loss: 0.041527
Train Epoch: 1 [2253824/2777098 (81.2%)]	Loss: 0.039896
Train Epoch: 1 [2305024/2777098 (83.0%)]	Loss: 0.027830
Train Epoch: 1 [2356224/2777098 (84.8%)]	Loss: 0.039389
Train Epoch: 1 [2407424/2777098 (86.7%)]	Loss: 0.040962
Train Epoch: 1 [2458624/2777098 (88.5%)]	Loss: 0.038411
Train Epoch: 1 [2509824/2777098 (90.4%)]	Loss: 0.034122
Train Epoch: 1 [2561024/2777098 (92.2%)]	Loss: 0.040696
Train Epoch: 1 [2612224/2777098 (94.1%)]	Loss: 0.032759
Train Epoch: 1 [2663424/2777098 (95.9%)]	Loss: 0.048272
Train Epoch: 1 [2714624/2777098 (97.8%)]	Loss: 0.028747
Train Epoch: 1 [2765824/2777098 (99.6%)]	Loss: 0.036713
Train Epoch: 2 [1024/2777098 (0.0%)]	Loss: 0.041409
Train Epoch: 2 [52224/2777098 (1.9%)]	Loss: 0.044736
Train Epoch: 2 [103424/2777098 (3.7%)]	Loss: 0.032656
Train Epoch: 2 [154624/2777098 (5.6%)]	Loss: 0.037359
Train Epoch: 2 [205824/2777098 (7.4%)]	Loss: 0.033279
Train Epoch: 2 [257024/2777098 (9.3%)]	Loss: 0.042365
Train Epoch: 2 [308224/2777098 (11.1%)]	Loss: 0.028932
Train Epoch: 2 [359424/2777098 (12.9%)]	Loss: 0.036463
Train Epoch: 2 [410624/2777098 (14.8%)]	Loss: 0.026454
Train Epoch: 2 [461824/2777098 (16.6%)]	Loss: 0.039687
Train Epoch: 2 [513024/2777098 (18.5%)]	Loss: 0.035655
Train Epoch: 2 [564224/2777098 (20.3%)]	Loss: 0.036701
Train Epoch: 2 [615424/2777098 (22.2%)]	Loss: 0.040480
Train Epoch: 2 [666624/2777098 (24.0%)]	Loss: 0.040479
Train Epoch: 2 [717824/2777098 (25.8%)]	Loss: 0.032140
Train Epoch: 2 [769024/2777098 (27.7%)]	Loss: 0.038355
Train Epoch: 2 [820224/2777098 (29.5%)]	Loss: 0.037339
Train Epoch: 2 [871424/2777098 (31.4%)]	Loss: 0.031894
Train Epoch: 2 [922624/2777098 (33.2%)]	Loss: 0.029820
Train Epoch: 2 [973824/2777098 (35.1%)]	Loss: 0.039961
Train Epoch: 2 [1025024/2777098 (36.9%)]	Loss: 0.041950
Train Epoch: 2 [1076224/2777098 (38.8%)]	Loss: 0.035498
Train Epoch: 2 [1127424/2777098 (40.6%)]	Loss: 0.032390
Train Epoch: 2 [1178624/2777098 (42.4%)]	Loss: 0.029982
Train Epoch: 2 [1229824/2777098 (44.3%)]	Loss: 0.031759
Train Epoch: 2 [1281024/2777098 (46.1%)]	Loss: 0.045131
Train Epoch: 2 [1332224/2777098 (48.0%)]	Loss: 0.035181
Train Epoch: 2 [1383424/2777098 (49.8%)]	Loss: 0.035582
Train Epoch: 2 [1434624/2777098 (51.7%)]	Loss: 0.038575
Train Epoch: 2 [1485824/2777098 (53.5%)]	Loss: 0.048501
Train Epoch: 2 [1537024/2777098 (55.3%)]	Loss: 0.038258
Train Epoch: 2 [1588224/2777098 (57.2%)]	Loss: 0.035485
Train Epoch: 2 [1639424/2777098 (59.0%)]	Loss: 0.031361
Train Epoch: 2 [1690624/2777098 (60.9%)]	Loss: 0.039022
Train Epoch: 2 [1741824/2777098 (62.7%)]	Loss: 0.047671
Train Epoch: 2 [1793024/2777098 (64.6%)]	Loss: 0.032916
Train Epoch: 2 [1844224/2777098 (66.4%)]	Loss: 0.028866
Train Epoch: 2 [1895424/2777098 (68.3%)]	Loss: 0.034588
Train Epoch: 2 [1946624/2777098 (70.1%)]	Loss: 0.041856
Train Epoch: 2 [1997824/2777098 (71.9%)]	Loss: 0.033726
Train Epoch: 2 [2049024/2777098 (73.8%)]	Loss: 0.032533
Train Epoch: 2 [2100224/2777098 (75.6%)]	Loss: 0.045595
Train Epoch: 2 [2151424/2777098 (77.5%)]	Loss: 0.037203
Train Epoch: 2 [2202624/2777098 (79.3%)]	Loss: 0.040065
Train Epoch: 2 [2253824/2777098 (81.2%)]	Loss: 0.040139
Train Epoch: 2 [2305024/2777098 (83.0%)]	Loss: 0.036066
Train Epoch: 2 [2356224/2777098 (84.8%)]	Loss: 0.039420
Train Epoch: 2 [2407424/2777098 (86.7%)]	Loss: 0.044704
Train Epoch: 2 [2458624/2777098 (88.5%)]	Loss: 0.037567
Train Epoch: 2 [2509824/2777098 (90.4%)]	Loss: 0.026641
Train Epoch: 2 [2561024/2777098 (92.2%)]	Loss: 0.031782
Train Epoch: 2 [2612224/2777098 (94.1%)]	Loss: 0.039346
Train Epoch: 2 [2663424/2777098 (95.9%)]	Loss: 0.052281
Train Epoch: 2 [2714624/2777098 (97.8%)]	Loss: 0.033988
Train Epoch: 2 [2765824/2777098 (99.6%)]	Loss: 0.030592

 ---------------------------------------------------------------------- 


ACC in fold#2 was 0.966


Confusion Matrix in fold#2: 
           nonRipple  Ripple
nonRipple     123353   21242
Ripple          2709  546971


Classification Report in fold#2: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.979       0.963     0.966       0.971         0.966
recall            0.853       0.995     0.966       0.924         0.966
f1-score          0.912       0.979     0.966       0.945         0.965
sample size  144595.000  549680.000     0.966  694275.000    694275.000


PR_AUC in fold#2 was 0.998


ROC_AUC in fold#2 was 0.994

Time (id:3): tot 03:39:51, prev 01:12:55 [hh:mm:ss]: 
i_fold=2 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2777099 (0.0%)]	Loss: 0.367216
Train Epoch: 1 [52224/2777099 (1.9%)]	Loss: 0.091635
Train Epoch: 1 [103424/2777099 (3.7%)]	Loss: 0.064329
Train Epoch: 1 [154624/2777099 (5.6%)]	Loss: 0.054977
Train Epoch: 1 [205824/2777099 (7.4%)]	Loss: 0.049859
Train Epoch: 1 [257024/2777099 (9.3%)]	Loss: 0.043140
Train Epoch: 1 [308224/2777099 (11.1%)]	Loss: 0.045640
Train Epoch: 1 [359424/2777099 (12.9%)]	Loss: 0.036286
Train Epoch: 1 [410624/2777099 (14.8%)]	Loss: 0.034081
Train Epoch: 1 [461824/2777099 (16.6%)]	Loss: 0.035077
Train Epoch: 1 [513024/2777099 (18.5%)]	Loss: 0.036525
Train Epoch: 1 [564224/2777099 (20.3%)]	Loss: 0.040542
Train Epoch: 1 [615424/2777099 (22.2%)]	Loss: 0.046432
Train Epoch: 1 [666624/2777099 (24.0%)]	Loss: 0.043116
Train Epoch: 1 [717824/2777099 (25.8%)]	Loss: 0.039843
Train Epoch: 1 [769024/2777099 (27.7%)]	Loss: 0.036077
Train Epoch: 1 [820224/2777099 (29.5%)]	Loss: 0.034040
Train Epoch: 1 [871424/2777099 (31.4%)]	Loss: 0.049067
Train Epoch: 1 [922624/2777099 (33.2%)]	Loss: 0.034942
Train Epoch: 1 [973824/2777099 (35.1%)]	Loss: 0.043839
Train Epoch: 1 [1025024/2777099 (36.9%)]	Loss: 0.055735
Train Epoch: 1 [1076224/2777099 (38.8%)]	Loss: 0.034482
Train Epoch: 1 [1127424/2777099 (40.6%)]	Loss: 0.038085
Train Epoch: 1 [1178624/2777099 (42.4%)]	Loss: 0.043690
Train Epoch: 1 [1229824/2777099 (44.3%)]	Loss: 0.033174
Train Epoch: 1 [1281024/2777099 (46.1%)]	Loss: 0.038601
Train Epoch: 1 [1332224/2777099 (48.0%)]	Loss: 0.042791
Train Epoch: 1 [1383424/2777099 (49.8%)]	Loss: 0.046487
Train Epoch: 1 [1434624/2777099 (51.7%)]	Loss: 0.041233
Train Epoch: 1 [1485824/2777099 (53.5%)]	Loss: 0.043721
Train Epoch: 1 [1537024/2777099 (55.3%)]	Loss: 0.048547
Train Epoch: 1 [1588224/2777099 (57.2%)]	Loss: 0.044038
Train Epoch: 1 [1639424/2777099 (59.0%)]	Loss: 0.033249
Train Epoch: 1 [1690624/2777099 (60.9%)]	Loss: 0.042446
Train Epoch: 1 [1741824/2777099 (62.7%)]	Loss: 0.039849
Train Epoch: 1 [1793024/2777099 (64.6%)]	Loss: 0.033570
Train Epoch: 1 [1844224/2777099 (66.4%)]	Loss: 0.043798
Train Epoch: 1 [1895424/2777099 (68.3%)]	Loss: 0.044648
Train Epoch: 1 [1946624/2777099 (70.1%)]	Loss: 0.028680
Train Epoch: 1 [1997824/2777099 (71.9%)]	Loss: 0.038300
Train Epoch: 1 [2049024/2777099 (73.8%)]	Loss: 0.049651
Train Epoch: 1 [2100224/2777099 (75.6%)]	Loss: 0.044459
Train Epoch: 1 [2151424/2777099 (77.5%)]	Loss: 0.037274
Train Epoch: 1 [2202624/2777099 (79.3%)]	Loss: 0.041243
Train Epoch: 1 [2253824/2777099 (81.2%)]	Loss: 0.037695
Train Epoch: 1 [2305024/2777099 (83.0%)]	Loss: 0.045683
Train Epoch: 1 [2356224/2777099 (84.8%)]	Loss: 0.035867
Train Epoch: 1 [2407424/2777099 (86.7%)]	Loss: 0.049893
Train Epoch: 1 [2458624/2777099 (88.5%)]	Loss: 0.031847
Train Epoch: 1 [2509824/2777099 (90.4%)]	Loss: 0.044260
Train Epoch: 1 [2561024/2777099 (92.2%)]	Loss: 0.039273
Train Epoch: 1 [2612224/2777099 (94.1%)]	Loss: 0.036549
Train Epoch: 1 [2663424/2777099 (95.9%)]	Loss: 0.047688
Train Epoch: 1 [2714624/2777099 (97.8%)]	Loss: 0.030460
Train Epoch: 1 [2765824/2777099 (99.6%)]	Loss: 0.041752
Train Epoch: 2 [1024/2777099 (0.0%)]	Loss: 0.041862
Train Epoch: 2 [52224/2777099 (1.9%)]	Loss: 0.044371
Train Epoch: 2 [103424/2777099 (3.7%)]	Loss: 0.031442
Train Epoch: 2 [154624/2777099 (5.6%)]	Loss: 0.033585
Train Epoch: 2 [205824/2777099 (7.4%)]	Loss: 0.039312
Train Epoch: 2 [257024/2777099 (9.3%)]	Loss: 0.030091
Train Epoch: 2 [308224/2777099 (11.1%)]	Loss: 0.032361
Train Epoch: 2 [359424/2777099 (12.9%)]	Loss: 0.041182
Train Epoch: 2 [410624/2777099 (14.8%)]	Loss: 0.044126
Train Epoch: 2 [461824/2777099 (16.6%)]	Loss: 0.039515
Train Epoch: 2 [513024/2777099 (18.5%)]	Loss: 0.042857
Train Epoch: 2 [564224/2777099 (20.3%)]	Loss: 0.042541
Train Epoch: 2 [615424/2777099 (22.2%)]	Loss: 0.035979
Train Epoch: 2 [666624/2777099 (24.0%)]	Loss: 0.030394
Train Epoch: 2 [717824/2777099 (25.8%)]	Loss: 0.032894
Train Epoch: 2 [769024/2777099 (27.7%)]	Loss: 0.042216
Train Epoch: 2 [820224/2777099 (29.5%)]	Loss: 0.039517
Train Epoch: 2 [871424/2777099 (31.4%)]	Loss: 0.030289
Train Epoch: 2 [922624/2777099 (33.2%)]	Loss: 0.039803
Train Epoch: 2 [973824/2777099 (35.1%)]	Loss: 0.032280
Train Epoch: 2 [1025024/2777099 (36.9%)]	Loss: 0.031720
Train Epoch: 2 [1076224/2777099 (38.8%)]	Loss: 0.040398
Train Epoch: 2 [1127424/2777099 (40.6%)]	Loss: 0.043886
Train Epoch: 2 [1178624/2777099 (42.4%)]	Loss: 0.041133
Train Epoch: 2 [1229824/2777099 (44.3%)]	Loss: 0.037763
Train Epoch: 2 [1281024/2777099 (46.1%)]	Loss: 0.036724
Train Epoch: 2 [1332224/2777099 (48.0%)]	Loss: 0.029366
Train Epoch: 2 [1383424/2777099 (49.8%)]	Loss: 0.038084
Train Epoch: 2 [1434624/2777099 (51.7%)]	Loss: 0.032648
Train Epoch: 2 [1485824/2777099 (53.5%)]	Loss: 0.050053
Train Epoch: 2 [1537024/2777099 (55.3%)]	Loss: 0.033911
Train Epoch: 2 [1588224/2777099 (57.2%)]	Loss: 0.038980
Train Epoch: 2 [1639424/2777099 (59.0%)]	Loss: 0.041961
Train Epoch: 2 [1690624/2777099 (60.9%)]	Loss: 0.033172
Train Epoch: 2 [1741824/2777099 (62.7%)]	Loss: 0.036473
Train Epoch: 2 [1793024/2777099 (64.6%)]	Loss: 0.045651
Train Epoch: 2 [1844224/2777099 (66.4%)]	Loss: 0.031973
Train Epoch: 2 [1895424/2777099 (68.3%)]	Loss: 0.043207
Train Epoch: 2 [1946624/2777099 (70.1%)]	Loss: 0.038051
Train Epoch: 2 [1997824/2777099 (71.9%)]	Loss: 0.028448
Train Epoch: 2 [2049024/2777099 (73.8%)]	Loss: 0.026317
Train Epoch: 2 [2100224/2777099 (75.6%)]	Loss: 0.032854
Train Epoch: 2 [2151424/2777099 (77.5%)]	Loss: 0.031241
Train Epoch: 2 [2202624/2777099 (79.3%)]	Loss: 0.032815
Train Epoch: 2 [2253824/2777099 (81.2%)]	Loss: 0.038700
Train Epoch: 2 [2305024/2777099 (83.0%)]	Loss: 0.035653
Train Epoch: 2 [2356224/2777099 (84.8%)]	Loss: 0.034538
Train Epoch: 2 [2407424/2777099 (86.7%)]	Loss: 0.025597
Train Epoch: 2 [2458624/2777099 (88.5%)]	Loss: 0.036324
Train Epoch: 2 [2509824/2777099 (90.4%)]	Loss: 0.030642
Train Epoch: 2 [2561024/2777099 (92.2%)]	Loss: 0.034514
Train Epoch: 2 [2612224/2777099 (94.1%)]	Loss: 0.043044
Train Epoch: 2 [2663424/2777099 (95.9%)]	Loss: 0.038922
Train Epoch: 2 [2714624/2777099 (97.8%)]	Loss: 0.045323
Train Epoch: 2 [2765824/2777099 (99.6%)]	Loss: 0.036012

 ---------------------------------------------------------------------- 


ACC in fold#3 was 0.974


Confusion Matrix in fold#3: 
           nonRipple  Ripple
nonRipple     131912   12682
Ripple          5259  544421


Classification Report in fold#3: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.962       0.977     0.974       0.969         0.974
recall            0.912       0.990     0.974       0.951         0.974
f1-score          0.936       0.984     0.974       0.960         0.974
sample size  144594.000  549680.000     0.974  694274.000    694274.000


PR_AUC in fold#3 was 0.998


ROC_AUC in fold#3 was 0.995

Time (id:4): tot 04:53:08, prev 01:13:17 [hh:mm:ss]: 
i_fold=3 ends.



 ---------------------------------------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2777099 (0.0%)]	Loss: 0.370821
Train Epoch: 1 [52224/2777099 (1.9%)]	Loss: 0.077197
Train Epoch: 1 [103424/2777099 (3.7%)]	Loss: 0.040323
Train Epoch: 1 [154624/2777099 (5.6%)]	Loss: 0.044940
Train Epoch: 1 [205824/2777099 (7.4%)]	Loss: 0.044213
Train Epoch: 1 [257024/2777099 (9.3%)]	Loss: 0.044564
Train Epoch: 1 [308224/2777099 (11.1%)]	Loss: 0.049112
Train Epoch: 1 [359424/2777099 (12.9%)]	Loss: 0.046459
Train Epoch: 1 [410624/2777099 (14.8%)]	Loss: 0.048263
Train Epoch: 1 [461824/2777099 (16.6%)]	Loss: 0.035199
Train Epoch: 1 [513024/2777099 (18.5%)]	Loss: 0.039237
Train Epoch: 1 [564224/2777099 (20.3%)]	Loss: 0.032710
Train Epoch: 1 [615424/2777099 (22.2%)]	Loss: 0.033482
Train Epoch: 1 [666624/2777099 (24.0%)]	Loss: 0.048855
Train Epoch: 1 [717824/2777099 (25.8%)]	Loss: 0.039620
Train Epoch: 1 [769024/2777099 (27.7%)]	Loss: 0.054678
Train Epoch: 1 [820224/2777099 (29.5%)]	Loss: 0.037306
Train Epoch: 1 [871424/2777099 (31.4%)]	Loss: 0.042152
Train Epoch: 1 [922624/2777099 (33.2%)]	Loss: 0.042026
Train Epoch: 1 [973824/2777099 (35.1%)]	Loss: 0.040871
Train Epoch: 1 [1025024/2777099 (36.9%)]	Loss: 0.046458
Train Epoch: 1 [1076224/2777099 (38.8%)]	Loss: 0.036728
Train Epoch: 1 [1127424/2777099 (40.6%)]	Loss: 0.043935
Train Epoch: 1 [1178624/2777099 (42.4%)]	Loss: 0.046176
Train Epoch: 1 [1229824/2777099 (44.3%)]	Loss: 0.040830
Train Epoch: 1 [1281024/2777099 (46.1%)]	Loss: 0.038127
Train Epoch: 1 [1332224/2777099 (48.0%)]	Loss: 0.036237
Train Epoch: 1 [1383424/2777099 (49.8%)]	Loss: 0.031117
Train Epoch: 1 [1434624/2777099 (51.7%)]	Loss: 0.045392
Train Epoch: 1 [1485824/2777099 (53.5%)]	Loss: 0.035220
Train Epoch: 1 [1537024/2777099 (55.3%)]	Loss: 0.030132
Train Epoch: 1 [1588224/2777099 (57.2%)]	Loss: 0.028763
Train Epoch: 1 [1639424/2777099 (59.0%)]	Loss: 0.052092
Train Epoch: 1 [1690624/2777099 (60.9%)]	Loss: 0.027140
Train Epoch: 1 [1741824/2777099 (62.7%)]	Loss: 0.036627
Train Epoch: 1 [1793024/2777099 (64.6%)]	Loss: 0.032321
Train Epoch: 1 [1844224/2777099 (66.4%)]	Loss: 0.037996
Train Epoch: 1 [1895424/2777099 (68.3%)]	Loss: 0.033384
Train Epoch: 1 [1946624/2777099 (70.1%)]	Loss: 0.039702
Train Epoch: 1 [1997824/2777099 (71.9%)]	Loss: 0.035771
Train Epoch: 1 [2049024/2777099 (73.8%)]	Loss: 0.038888
Train Epoch: 1 [2100224/2777099 (75.6%)]	Loss: 0.045509
Train Epoch: 1 [2151424/2777099 (77.5%)]	Loss: 0.045581
Train Epoch: 1 [2202624/2777099 (79.3%)]	Loss: 0.044582
Train Epoch: 1 [2253824/2777099 (81.2%)]	Loss: 0.036175
Train Epoch: 1 [2305024/2777099 (83.0%)]	Loss: 0.043171
Train Epoch: 1 [2356224/2777099 (84.8%)]	Loss: 0.041708
Train Epoch: 1 [2407424/2777099 (86.7%)]	Loss: 0.035915
Train Epoch: 1 [2458624/2777099 (88.5%)]	Loss: 0.036102
Train Epoch: 1 [2509824/2777099 (90.4%)]	Loss: 0.039322
Train Epoch: 1 [2561024/2777099 (92.2%)]	Loss: 0.044995
Train Epoch: 1 [2612224/2777099 (94.1%)]	Loss: 0.045781
Train Epoch: 1 [2663424/2777099 (95.9%)]	Loss: 0.034743
Train Epoch: 1 [2714624/2777099 (97.8%)]	Loss: 0.039152
Train Epoch: 1 [2765824/2777099 (99.6%)]	Loss: 0.036542
Train Epoch: 2 [1024/2777099 (0.0%)]	Loss: 0.032035
Train Epoch: 2 [52224/2777099 (1.9%)]	Loss: 0.045357
Train Epoch: 2 [103424/2777099 (3.7%)]	Loss: 0.040926
Train Epoch: 2 [154624/2777099 (5.6%)]	Loss: 0.041724
Train Epoch: 2 [205824/2777099 (7.4%)]	Loss: 0.026624
Train Epoch: 2 [257024/2777099 (9.3%)]	Loss: 0.045209
Train Epoch: 2 [308224/2777099 (11.1%)]	Loss: 0.031215
Train Epoch: 2 [359424/2777099 (12.9%)]	Loss: 0.046021
Train Epoch: 2 [410624/2777099 (14.8%)]	Loss: 0.030648
Train Epoch: 2 [461824/2777099 (16.6%)]	Loss: 0.040856
Train Epoch: 2 [513024/2777099 (18.5%)]	Loss: 0.050681
Train Epoch: 2 [564224/2777099 (20.3%)]	Loss: 0.048676
Train Epoch: 2 [615424/2777099 (22.2%)]	Loss: 0.036544
Train Epoch: 2 [666624/2777099 (24.0%)]	Loss: 0.038455
Train Epoch: 2 [717824/2777099 (25.8%)]	Loss: 0.034176
Train Epoch: 2 [769024/2777099 (27.7%)]	Loss: 0.032009
Train Epoch: 2 [820224/2777099 (29.5%)]	Loss: 0.029361
Train Epoch: 2 [871424/2777099 (31.4%)]	Loss: 0.031688
Train Epoch: 2 [922624/2777099 (33.2%)]	Loss: 0.039457
Train Epoch: 2 [973824/2777099 (35.1%)]	Loss: 0.045059
Train Epoch: 2 [1025024/2777099 (36.9%)]	Loss: 0.039688
Train Epoch: 2 [1076224/2777099 (38.8%)]	Loss: 0.028473
Train Epoch: 2 [1127424/2777099 (40.6%)]	Loss: 0.030122
Train Epoch: 2 [1178624/2777099 (42.4%)]	Loss: 0.035729
Train Epoch: 2 [1229824/2777099 (44.3%)]	Loss: 0.039185
Train Epoch: 2 [1281024/2777099 (46.1%)]	Loss: 0.034294
Train Epoch: 2 [1332224/2777099 (48.0%)]	Loss: 0.038423
Train Epoch: 2 [1383424/2777099 (49.8%)]	Loss: 0.036061
Train Epoch: 2 [1434624/2777099 (51.7%)]	Loss: 0.035176
Train Epoch: 2 [1485824/2777099 (53.5%)]	Loss: 0.030805
Train Epoch: 2 [1537024/2777099 (55.3%)]	Loss: 0.042214
Train Epoch: 2 [1588224/2777099 (57.2%)]	Loss: 0.032039
Train Epoch: 2 [1639424/2777099 (59.0%)]	Loss: 0.035952
Train Epoch: 2 [1690624/2777099 (60.9%)]	Loss: 0.031361
Train Epoch: 2 [1741824/2777099 (62.7%)]	Loss: 0.034729
Train Epoch: 2 [1793024/2777099 (64.6%)]	Loss: 0.033099
Train Epoch: 2 [1844224/2777099 (66.4%)]	Loss: 0.026502
Train Epoch: 2 [1895424/2777099 (68.3%)]	Loss: 0.034074
Train Epoch: 2 [1946624/2777099 (70.1%)]	Loss: 0.029750
Train Epoch: 2 [1997824/2777099 (71.9%)]	Loss: 0.040540
Train Epoch: 2 [2049024/2777099 (73.8%)]	Loss: 0.030886
Train Epoch: 2 [2100224/2777099 (75.6%)]	Loss: 0.027291
Train Epoch: 2 [2151424/2777099 (77.5%)]	Loss: 0.036054
Train Epoch: 2 [2202624/2777099 (79.3%)]	Loss: 0.038661
Train Epoch: 2 [2253824/2777099 (81.2%)]	Loss: 0.041087
Train Epoch: 2 [2305024/2777099 (83.0%)]	Loss: 0.034141
Train Epoch: 2 [2356224/2777099 (84.8%)]	Loss: 0.040432
Train Epoch: 2 [2407424/2777099 (86.7%)]	Loss: 0.031840
Train Epoch: 2 [2458624/2777099 (88.5%)]	Loss: 0.029338
Train Epoch: 2 [2509824/2777099 (90.4%)]	Loss: 0.036747
Train Epoch: 2 [2561024/2777099 (92.2%)]	Loss: 0.027126
Train Epoch: 2 [2612224/2777099 (94.1%)]	Loss: 0.035781
Train Epoch: 2 [2663424/2777099 (95.9%)]	Loss: 0.029038
Train Epoch: 2 [2714624/2777099 (97.8%)]	Loss: 0.049505
Train Epoch: 2 [2765824/2777099 (99.6%)]	Loss: 0.029066

 ---------------------------------------------------------------------- 


ACC in fold#4 was 0.963


Confusion Matrix in fold#4: 
           nonRipple  Ripple
nonRipple     124465   20129
Ripple          5496  544184


Classification Report in fold#4: 
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.958       0.964     0.963       0.961         0.963
recall            0.861       0.990     0.963       0.925         0.963
f1-score          0.907       0.977     0.963       0.942         0.962
sample size  144594.000  549680.000     0.963  694274.000    694274.000


PR_AUC in fold#4 was 0.998


ROC_AUC in fold#4 was 0.993

Time (id:5): tot 06:06:23, prev 01:13:14 [hh:mm:ss]: 
i_fold=4 ends.



 ---------------------------------------------------------------------- 


Label Errors Rate:
0.005


 --- 5-fold CV overall metrics --- 


Confusion Matrix (Test; sum; num. folds=5)
           nonRipple   Ripple
nonRipple     627824    95148
Ripple         20178  2728223


Classification Report (Test; mean; num. folds=5)
              nonRipple      Ripple  accuracy   macro avg  weighted avg
precision         0.969       0.966     0.967       0.968         0.967
recall            0.868       0.993     0.967       0.930         0.967
f1-score          0.916       0.979     0.967       0.948         0.966
sample size  144594.400  549680.200     0.967  694274.600    694274.600


Classification Report (Test; std; num. folds=5)
             nonRipple  Ripple  accuracy  macro avg  weighted avg
precision        0.011   0.005     0.004      0.005         0.004
recall           0.022   0.003     0.004      0.010         0.004
f1-score         0.011   0.003     0.004      0.007         0.004
sample size      0.490   0.400     0.004      0.490         0.490


PRE-REC AUC Score: 0.998 +/- 0.001 (mean +/- std.; n=5)


ROC AUC Score: 0.994 +/- 0.002 (mean +/- std.; n=5)


Saved to: ./data/okada/cleanlab_results/D04+/are_errors.npy


Saved to: ./data/okada/cleanlab_results/D04+/cleaned_labels.npy


Saved to: ./data/okada/cleanlab_results/D04+/pred_probas_ripples.npy

(Moved to: /tmp/2021-0514-2213-D04+-conf_mats.csv)
Saved to: ./data/okada/cleanlab_results/D04+/conf_mats.csv
Saved to: ./data/okada/cleanlab_results/D04+/conf_mat_overall_sum.png
(Moved to: /tmp/2021-0514-2213-D04+-clf_reports.csv)
Saved to: ./data/okada/cleanlab_results/D04+/clf_reports.csv
(Moved to: /tmp/2021-0514-2213-D04+-roc-auc.csv)
Saved to: ./data/okada/cleanlab_results/D04+/roc-auc.csv

Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D04+/tt7-4_fp16.pkl

