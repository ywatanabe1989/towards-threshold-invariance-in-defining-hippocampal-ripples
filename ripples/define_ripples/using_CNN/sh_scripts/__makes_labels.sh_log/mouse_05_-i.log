
Random seeds have been fixed as 42


Random seeds have been fixed as 42


Standard Output/Error are going to be logged in the followings: 
  - ./ripples/define_ripples/using_CNN/makes_labels/log/stdout.log
  - ./ripples/define_ripples/using_CNN/makes_labels/log/stderr.log


Random seeds have been fixed as 42

D05+
['./data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy']

2021-0711-0458

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2541349 (0.0%)]	Loss: 0.363966
Train Epoch: 1 [52224/2541349 (2.1%)]	Loss: 0.109851
Train Epoch: 1 [103424/2541349 (4.1%)]	Loss: 0.088201
Train Epoch: 1 [154624/2541349 (6.1%)]	Loss: 0.082741
Train Epoch: 1 [205824/2541349 (8.1%)]	Loss: 0.080444
Train Epoch: 1 [257024/2541349 (10.1%)]	Loss: 0.086912
Train Epoch: 1 [308224/2541349 (12.1%)]	Loss: 0.086642
Train Epoch: 1 [359424/2541349 (14.1%)]	Loss: 0.071888
Train Epoch: 1 [410624/2541349 (16.2%)]	Loss: 0.076714
Train Epoch: 1 [461824/2541349 (18.2%)]	Loss: 0.087459
Train Epoch: 1 [513024/2541349 (20.2%)]	Loss: 0.075126
Train Epoch: 1 [564224/2541349 (22.2%)]	Loss: 0.078714
Train Epoch: 1 [615424/2541349 (24.2%)]	Loss: 0.080584
Train Epoch: 1 [666624/2541349 (26.2%)]	Loss: 0.084211
Train Epoch: 1 [717824/2541349 (28.2%)]	Loss: 0.082829
Train Epoch: 1 [769024/2541349 (30.3%)]	Loss: 0.069862
Train Epoch: 1 [820224/2541349 (32.3%)]	Loss: 0.078512
Train Epoch: 1 [871424/2541349 (34.3%)]	Loss: 0.083160
Train Epoch: 1 [922624/2541349 (36.3%)]	Loss: 0.063052
Train Epoch: 1 [973824/2541349 (38.3%)]	Loss: 0.069471
Train Epoch: 1 [1025024/2541349 (40.3%)]	Loss: 0.074549
Train Epoch: 1 [1076224/2541349 (42.3%)]	Loss: 0.078540
Train Epoch: 1 [1127424/2541349 (44.4%)]	Loss: 0.064588
Train Epoch: 1 [1178624/2541349 (46.4%)]	Loss: 0.080243
Train Epoch: 1 [1229824/2541349 (48.4%)]	Loss: 0.087443
Train Epoch: 1 [1281024/2541349 (50.4%)]	Loss: 0.074054
Train Epoch: 1 [1332224/2541349 (52.4%)]	Loss: 0.068423
Train Epoch: 1 [1383424/2541349 (54.4%)]	Loss: 0.073602
Train Epoch: 1 [1434624/2541349 (56.5%)]	Loss: 0.066059
Train Epoch: 1 [1485824/2541349 (58.5%)]	Loss: 0.080326
Train Epoch: 1 [1537024/2541349 (60.5%)]	Loss: 0.070279
Train Epoch: 1 [1588224/2541349 (62.5%)]	Loss: 0.062386
Train Epoch: 1 [1639424/2541349 (64.5%)]	Loss: 0.067579
Train Epoch: 1 [1690624/2541349 (66.5%)]	Loss: 0.058212
Train Epoch: 1 [1741824/2541349 (68.5%)]	Loss: 0.066868
Train Epoch: 1 [1793024/2541349 (70.6%)]	Loss: 0.051367
Train Epoch: 1 [1844224/2541349 (72.6%)]	Loss: 0.058407
Train Epoch: 1 [1895424/2541349 (74.6%)]	Loss: 0.068674
Train Epoch: 1 [1946624/2541349 (76.6%)]	Loss: 0.052955
Train Epoch: 1 [1997824/2541349 (78.6%)]	Loss: 0.056816
Train Epoch: 1 [2049024/2541349 (80.6%)]	Loss: 0.070012
Train Epoch: 1 [2100224/2541349 (82.6%)]	Loss: 0.055661
Train Epoch: 1 [2151424/2541349 (84.7%)]	Loss: 0.062133
Train Epoch: 1 [2202624/2541349 (86.7%)]	Loss: 0.063195
Train Epoch: 1 [2253824/2541349 (88.7%)]	Loss: 0.068844
Train Epoch: 1 [2305024/2541349 (90.7%)]	Loss: 0.062376
Train Epoch: 1 [2356224/2541349 (92.7%)]	Loss: 0.066598
Train Epoch: 1 [2407424/2541349 (94.7%)]	Loss: 0.073694
Train Epoch: 1 [2458624/2541349 (96.7%)]	Loss: 0.078995
Train Epoch: 1 [2509824/2541349 (98.8%)]	Loss: 0.096518
Train Epoch: 2 [1024/2541349 (0.0%)]	Loss: 0.049045
Train Epoch: 2 [52224/2541349 (2.1%)]	Loss: 0.071443
Train Epoch: 2 [103424/2541349 (4.1%)]	Loss: 0.053708
Train Epoch: 2 [154624/2541349 (6.1%)]	Loss: 0.069487
Train Epoch: 2 [205824/2541349 (8.1%)]	Loss: 0.058517
Train Epoch: 2 [257024/2541349 (10.1%)]	Loss: 0.064221
Train Epoch: 2 [308224/2541349 (12.1%)]	Loss: 0.070923
Train Epoch: 2 [359424/2541349 (14.1%)]	Loss: 0.065779
Train Epoch: 2 [410624/2541349 (16.2%)]	Loss: 0.074232
Train Epoch: 2 [461824/2541349 (18.2%)]	Loss: 0.073712
Train Epoch: 2 [513024/2541349 (20.2%)]	Loss: 0.075654
Train Epoch: 2 [564224/2541349 (22.2%)]	Loss: 0.077416
Train Epoch: 2 [615424/2541349 (24.2%)]	Loss: 0.066878
Train Epoch: 2 [666624/2541349 (26.2%)]	Loss: 0.058596
Train Epoch: 2 [717824/2541349 (28.2%)]	Loss: 0.065888
Train Epoch: 2 [769024/2541349 (30.3%)]	Loss: 0.067367
Train Epoch: 2 [820224/2541349 (32.3%)]	Loss: 0.067914
Train Epoch: 2 [871424/2541349 (34.3%)]	Loss: 0.064373
Train Epoch: 2 [922624/2541349 (36.3%)]	Loss: 0.058299
Train Epoch: 2 [973824/2541349 (38.3%)]	Loss: 0.070768
Train Epoch: 2 [1025024/2541349 (40.3%)]	Loss: 0.053854
Train Epoch: 2 [1076224/2541349 (42.3%)]	Loss: 0.067139
Train Epoch: 2 [1127424/2541349 (44.4%)]	Loss: 0.068381
Train Epoch: 2 [1178624/2541349 (46.4%)]	Loss: 0.068396
Train Epoch: 2 [1229824/2541349 (48.4%)]	Loss: 0.074477
Train Epoch: 2 [1281024/2541349 (50.4%)]	Loss: 0.072001
Train Epoch: 2 [1332224/2541349 (52.4%)]	Loss: 0.069515
Train Epoch: 2 [1383424/2541349 (54.4%)]	Loss: 0.065737
Train Epoch: 2 [1434624/2541349 (56.5%)]	Loss: 0.062137
Train Epoch: 2 [1485824/2541349 (58.5%)]	Loss: 0.061924
Train Epoch: 2 [1537024/2541349 (60.5%)]	Loss: 0.076119
Train Epoch: 2 [1588224/2541349 (62.5%)]	Loss: 0.066528
Train Epoch: 2 [1639424/2541349 (64.5%)]	Loss: 0.073022
Train Epoch: 2 [1690624/2541349 (66.5%)]	Loss: 0.071105
Train Epoch: 2 [1741824/2541349 (68.5%)]	Loss: 0.065250
Train Epoch: 2 [1793024/2541349 (70.6%)]	Loss: 0.062537
Train Epoch: 2 [1844224/2541349 (72.6%)]	Loss: 0.071118
Train Epoch: 2 [1895424/2541349 (74.6%)]	Loss: 0.067840
Train Epoch: 2 [1946624/2541349 (76.6%)]	Loss: 0.051968
Train Epoch: 2 [1997824/2541349 (78.6%)]	Loss: 0.058464
Train Epoch: 2 [2049024/2541349 (80.6%)]	Loss: 0.072364
Train Epoch: 2 [2100224/2541349 (82.6%)]	Loss: 0.074524
Train Epoch: 2 [2151424/2541349 (84.7%)]	Loss: 0.057955
Train Epoch: 2 [2202624/2541349 (86.7%)]	Loss: 0.070279
Train Epoch: 2 [2253824/2541349 (88.7%)]	Loss: 0.067919
Train Epoch: 2 [2305024/2541349 (90.7%)]	Loss: 0.061946
Train Epoch: 2 [2356224/2541349 (92.7%)]	Loss: 0.064253
Train Epoch: 2 [2407424/2541349 (94.7%)]	Loss: 0.057252
Train Epoch: 2 [2458624/2541349 (96.7%)]	Loss: 0.064975
Train Epoch: 2 [2509824/2541349 (98.8%)]	Loss: 0.075417

ACC in fold#0 was 0.937


Balanced ACC in fold#0 was 0.898


MCC in fold#0 was 0.829


Confusion Matrix in fold#0: 
           nonRipple  Ripple
nonRipple     129587   28785
Ripple         10978  465988


Classification Report in fold#0: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.922       0.942  ...       0.932         0.937
recall            0.818       0.977  ...       0.898         0.937
f1-score          0.867       0.959  ...       0.913         0.936
sample size  158372.000  476966.000  ...  635338.000    635338.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2541349 (0.0%)]	Loss: 0.347139
Train Epoch: 1 [52224/2541349 (2.1%)]	Loss: 0.114241
Train Epoch: 1 [103424/2541349 (4.1%)]	Loss: 0.087726
Train Epoch: 1 [154624/2541349 (6.1%)]	Loss: 0.096160
Train Epoch: 1 [205824/2541349 (8.1%)]	Loss: 0.076286
Train Epoch: 1 [257024/2541349 (10.1%)]	Loss: 0.081217
Train Epoch: 1 [308224/2541349 (12.1%)]	Loss: 0.065630
Train Epoch: 1 [359424/2541349 (14.1%)]	Loss: 0.078144
Train Epoch: 1 [410624/2541349 (16.2%)]	Loss: 0.087283
Train Epoch: 1 [461824/2541349 (18.2%)]	Loss: 0.075970
Train Epoch: 1 [513024/2541349 (20.2%)]	Loss: 0.068383
Train Epoch: 1 [564224/2541349 (22.2%)]	Loss: 0.056636
Train Epoch: 1 [615424/2541349 (24.2%)]	Loss: 0.075688
Train Epoch: 1 [666624/2541349 (26.2%)]	Loss: 0.066054
Train Epoch: 1 [717824/2541349 (28.2%)]	Loss: 0.084985
Train Epoch: 1 [769024/2541349 (30.3%)]	Loss: 0.075000
Train Epoch: 1 [820224/2541349 (32.3%)]	Loss: 0.072586
Train Epoch: 1 [871424/2541349 (34.3%)]	Loss: 0.058751
Train Epoch: 1 [922624/2541349 (36.3%)]	Loss: 0.066812
Train Epoch: 1 [973824/2541349 (38.3%)]	Loss: 0.083001
Train Epoch: 1 [1025024/2541349 (40.3%)]	Loss: 0.066886
Train Epoch: 1 [1076224/2541349 (42.3%)]	Loss: 0.058419
Train Epoch: 1 [1127424/2541349 (44.4%)]	Loss: 0.061196
Train Epoch: 1 [1178624/2541349 (46.4%)]	Loss: 0.074310
Train Epoch: 1 [1229824/2541349 (48.4%)]	Loss: 0.075834
Train Epoch: 1 [1281024/2541349 (50.4%)]	Loss: 0.081762
Train Epoch: 1 [1332224/2541349 (52.4%)]	Loss: 0.068984
Train Epoch: 1 [1383424/2541349 (54.4%)]	Loss: 0.064390
Train Epoch: 1 [1434624/2541349 (56.5%)]	Loss: 0.055374
Train Epoch: 1 [1485824/2541349 (58.5%)]	Loss: 0.067302
Train Epoch: 1 [1537024/2541349 (60.5%)]	Loss: 0.066768
Train Epoch: 1 [1588224/2541349 (62.5%)]	Loss: 0.072674
Train Epoch: 1 [1639424/2541349 (64.5%)]	Loss: 0.079855
Train Epoch: 1 [1690624/2541349 (66.5%)]	Loss: 0.064085
Train Epoch: 1 [1741824/2541349 (68.5%)]	Loss: 0.068761
Train Epoch: 1 [1793024/2541349 (70.6%)]	Loss: 0.089822
Train Epoch: 1 [1844224/2541349 (72.6%)]	Loss: 0.069325
Train Epoch: 1 [1895424/2541349 (74.6%)]	Loss: 0.074135
Train Epoch: 1 [1946624/2541349 (76.6%)]	Loss: 0.069505
Train Epoch: 1 [1997824/2541349 (78.6%)]	Loss: 0.066292
Train Epoch: 1 [2049024/2541349 (80.6%)]	Loss: 0.068674
Train Epoch: 1 [2100224/2541349 (82.6%)]	Loss: 0.058034
Train Epoch: 1 [2151424/2541349 (84.7%)]	Loss: 0.082670
Train Epoch: 1 [2202624/2541349 (86.7%)]	Loss: 0.064021
Train Epoch: 1 [2253824/2541349 (88.7%)]	Loss: 0.081787
Train Epoch: 1 [2305024/2541349 (90.7%)]	Loss: 0.081160
Train Epoch: 1 [2356224/2541349 (92.7%)]	Loss: 0.071172
Train Epoch: 1 [2407424/2541349 (94.7%)]	Loss: 0.068411
Train Epoch: 1 [2458624/2541349 (96.7%)]	Loss: 0.073097
Train Epoch: 1 [2509824/2541349 (98.8%)]	Loss: 0.068797
Train Epoch: 2 [1024/2541349 (0.0%)]	Loss: 0.074612
Train Epoch: 2 [52224/2541349 (2.1%)]	Loss: 0.074234
Train Epoch: 2 [103424/2541349 (4.1%)]	Loss: 0.071087
Train Epoch: 2 [154624/2541349 (6.1%)]	Loss: 0.075031
Train Epoch: 2 [205824/2541349 (8.1%)]	Loss: 0.077041
Train Epoch: 2 [257024/2541349 (10.1%)]	Loss: 0.059409
Train Epoch: 2 [308224/2541349 (12.1%)]	Loss: 0.068372
Train Epoch: 2 [359424/2541349 (14.1%)]	Loss: 0.063417
Train Epoch: 2 [410624/2541349 (16.2%)]	Loss: 0.060092
Train Epoch: 2 [461824/2541349 (18.2%)]	Loss: 0.056745
Train Epoch: 2 [513024/2541349 (20.2%)]	Loss: 0.067358
Train Epoch: 2 [564224/2541349 (22.2%)]	Loss: 0.068394
Train Epoch: 2 [615424/2541349 (24.2%)]	Loss: 0.062490
Train Epoch: 2 [666624/2541349 (26.2%)]	Loss: 0.063724
Train Epoch: 2 [717824/2541349 (28.2%)]	Loss: 0.062239
Train Epoch: 2 [769024/2541349 (30.3%)]	Loss: 0.066116
Train Epoch: 2 [820224/2541349 (32.3%)]	Loss: 0.069866
Train Epoch: 2 [871424/2541349 (34.3%)]	Loss: 0.069078
Train Epoch: 2 [922624/2541349 (36.3%)]	Loss: 0.059495
Train Epoch: 2 [973824/2541349 (38.3%)]	Loss: 0.076495
Train Epoch: 2 [1025024/2541349 (40.3%)]	Loss: 0.066938
Train Epoch: 2 [1076224/2541349 (42.3%)]	Loss: 0.065130
Train Epoch: 2 [1127424/2541349 (44.4%)]	Loss: 0.078152
Train Epoch: 2 [1178624/2541349 (46.4%)]	Loss: 0.065950
Train Epoch: 2 [1229824/2541349 (48.4%)]	Loss: 0.063481
Train Epoch: 2 [1281024/2541349 (50.4%)]	Loss: 0.059120
Train Epoch: 2 [1332224/2541349 (52.4%)]	Loss: 0.051104
Train Epoch: 2 [1383424/2541349 (54.4%)]	Loss: 0.070627
Train Epoch: 2 [1434624/2541349 (56.5%)]	Loss: 0.065734
Train Epoch: 2 [1485824/2541349 (58.5%)]	Loss: 0.075831
Train Epoch: 2 [1537024/2541349 (60.5%)]	Loss: 0.058110
Train Epoch: 2 [1588224/2541349 (62.5%)]	Loss: 0.065672
Train Epoch: 2 [1639424/2541349 (64.5%)]	Loss: 0.072289
Train Epoch: 2 [1690624/2541349 (66.5%)]	Loss: 0.076160
Train Epoch: 2 [1741824/2541349 (68.5%)]	Loss: 0.070915
Train Epoch: 2 [1793024/2541349 (70.6%)]	Loss: 0.053817
Train Epoch: 2 [1844224/2541349 (72.6%)]	Loss: 0.069799
Train Epoch: 2 [1895424/2541349 (74.6%)]	Loss: 0.065823
Train Epoch: 2 [1946624/2541349 (76.6%)]	Loss: 0.066912
Train Epoch: 2 [1997824/2541349 (78.6%)]	Loss: 0.065952
Train Epoch: 2 [2049024/2541349 (80.6%)]	Loss: 0.068462
Train Epoch: 2 [2100224/2541349 (82.6%)]	Loss: 0.061478
Train Epoch: 2 [2151424/2541349 (84.7%)]	Loss: 0.066373
Train Epoch: 2 [2202624/2541349 (86.7%)]	Loss: 0.064946
Train Epoch: 2 [2253824/2541349 (88.7%)]	Loss: 0.061970
Train Epoch: 2 [2305024/2541349 (90.7%)]	Loss: 0.063118
Train Epoch: 2 [2356224/2541349 (92.7%)]	Loss: 0.067452
Train Epoch: 2 [2407424/2541349 (94.7%)]	Loss: 0.066843
Train Epoch: 2 [2458624/2541349 (96.7%)]	Loss: 0.069058
Train Epoch: 2 [2509824/2541349 (98.8%)]	Loss: 0.075912

ACC in fold#1 was 0.942


Balanced ACC in fold#1 was 0.919


MCC in fold#1 was 0.845


Confusion Matrix in fold#1: 
           nonRipple  Ripple
nonRipple     138004   20368
Ripple         16243  460723


Classification Report in fold#1: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.895       0.958  ...       0.926         0.942
recall            0.871       0.966  ...       0.919         0.942
f1-score          0.883       0.962  ...       0.922         0.942
sample size  158372.000  476966.000  ...  635338.000    635338.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2541350 (0.0%)]	Loss: 0.343701
Train Epoch: 1 [52224/2541350 (2.1%)]	Loss: 0.125794
Train Epoch: 1 [103424/2541350 (4.1%)]	Loss: 0.088311
Train Epoch: 1 [154624/2541350 (6.1%)]	Loss: 0.078724
Train Epoch: 1 [205824/2541350 (8.1%)]	Loss: 0.077365
Train Epoch: 1 [257024/2541350 (10.1%)]	Loss: 0.076540
Train Epoch: 1 [308224/2541350 (12.1%)]	Loss: 0.075834
Train Epoch: 1 [359424/2541350 (14.1%)]	Loss: 0.077223
Train Epoch: 1 [410624/2541350 (16.2%)]	Loss: 0.082331
Train Epoch: 1 [461824/2541350 (18.2%)]	Loss: 0.071977
Train Epoch: 1 [513024/2541350 (20.2%)]	Loss: 0.073814
Train Epoch: 1 [564224/2541350 (22.2%)]	Loss: 0.075167
Train Epoch: 1 [615424/2541350 (24.2%)]	Loss: 0.075441
Train Epoch: 1 [666624/2541350 (26.2%)]	Loss: 0.065387
Train Epoch: 1 [717824/2541350 (28.2%)]	Loss: 0.078982
Train Epoch: 1 [769024/2541350 (30.3%)]	Loss: 0.069850
Train Epoch: 1 [820224/2541350 (32.3%)]	Loss: 0.085639
Train Epoch: 1 [871424/2541350 (34.3%)]	Loss: 0.074841
Train Epoch: 1 [922624/2541350 (36.3%)]	Loss: 0.068097
Train Epoch: 1 [973824/2541350 (38.3%)]	Loss: 0.054047
Train Epoch: 1 [1025024/2541350 (40.3%)]	Loss: 0.070392
Train Epoch: 1 [1076224/2541350 (42.3%)]	Loss: 0.073274
Train Epoch: 1 [1127424/2541350 (44.4%)]	Loss: 0.065079
Train Epoch: 1 [1178624/2541350 (46.4%)]	Loss: 0.071845
Train Epoch: 1 [1229824/2541350 (48.4%)]	Loss: 0.057719
Train Epoch: 1 [1281024/2541350 (50.4%)]	Loss: 0.068491
Train Epoch: 1 [1332224/2541350 (52.4%)]	Loss: 0.066348
Train Epoch: 1 [1383424/2541350 (54.4%)]	Loss: 0.066679
Train Epoch: 1 [1434624/2541350 (56.5%)]	Loss: 0.074361
Train Epoch: 1 [1485824/2541350 (58.5%)]	Loss: 0.082005
Train Epoch: 1 [1537024/2541350 (60.5%)]	Loss: 0.066767
Train Epoch: 1 [1588224/2541350 (62.5%)]	Loss: 0.078926
Train Epoch: 1 [1639424/2541350 (64.5%)]	Loss: 0.070129
Train Epoch: 1 [1690624/2541350 (66.5%)]	Loss: 0.074286
Train Epoch: 1 [1741824/2541350 (68.5%)]	Loss: 0.077357
Train Epoch: 1 [1793024/2541350 (70.6%)]	Loss: 0.068989
Train Epoch: 1 [1844224/2541350 (72.6%)]	Loss: 0.065427
Train Epoch: 1 [1895424/2541350 (74.6%)]	Loss: 0.076756
Train Epoch: 1 [1946624/2541350 (76.6%)]	Loss: 0.078688
Train Epoch: 1 [1997824/2541350 (78.6%)]	Loss: 0.060551
Train Epoch: 1 [2049024/2541350 (80.6%)]	Loss: 0.068779
Train Epoch: 1 [2100224/2541350 (82.6%)]	Loss: 0.069600
Train Epoch: 1 [2151424/2541350 (84.7%)]	Loss: 0.074690
Train Epoch: 1 [2202624/2541350 (86.7%)]	Loss: 0.086505
Train Epoch: 1 [2253824/2541350 (88.7%)]	Loss: 0.064715
Train Epoch: 1 [2305024/2541350 (90.7%)]	Loss: 0.072090
Train Epoch: 1 [2356224/2541350 (92.7%)]	Loss: 0.083388
Train Epoch: 1 [2407424/2541350 (94.7%)]	Loss: 0.067914
Train Epoch: 1 [2458624/2541350 (96.7%)]	Loss: 0.064746
Train Epoch: 1 [2509824/2541350 (98.8%)]	Loss: 0.072101
Train Epoch: 2 [1024/2541350 (0.0%)]	Loss: 0.068648
Train Epoch: 2 [52224/2541350 (2.1%)]	Loss: 0.074251
Train Epoch: 2 [103424/2541350 (4.1%)]	Loss: 0.078688
Train Epoch: 2 [154624/2541350 (6.1%)]	Loss: 0.084778
Train Epoch: 2 [205824/2541350 (8.1%)]	Loss: 0.067998
Train Epoch: 2 [257024/2541350 (10.1%)]	Loss: 0.071815
Train Epoch: 2 [308224/2541350 (12.1%)]	Loss: 0.070697
Train Epoch: 2 [359424/2541350 (14.1%)]	Loss: 0.076184
Train Epoch: 2 [410624/2541350 (16.2%)]	Loss: 0.086327
Train Epoch: 2 [461824/2541350 (18.2%)]	Loss: 0.074191
Train Epoch: 2 [513024/2541350 (20.2%)]	Loss: 0.067964
Train Epoch: 2 [564224/2541350 (22.2%)]	Loss: 0.070741
Train Epoch: 2 [615424/2541350 (24.2%)]	Loss: 0.058795
Train Epoch: 2 [666624/2541350 (26.2%)]	Loss: 0.079459
Train Epoch: 2 [717824/2541350 (28.2%)]	Loss: 0.066236
Train Epoch: 2 [769024/2541350 (30.3%)]	Loss: 0.071482
Train Epoch: 2 [820224/2541350 (32.3%)]	Loss: 0.069820
Train Epoch: 2 [871424/2541350 (34.3%)]	Loss: 0.065380
Train Epoch: 2 [922624/2541350 (36.3%)]	Loss: 0.082425
Train Epoch: 2 [973824/2541350 (38.3%)]	Loss: 0.083084
Train Epoch: 2 [1025024/2541350 (40.3%)]	Loss: 0.065651
Train Epoch: 2 [1076224/2541350 (42.3%)]	Loss: 0.062515
Train Epoch: 2 [1127424/2541350 (44.4%)]	Loss: 0.071871
Train Epoch: 2 [1178624/2541350 (46.4%)]	Loss: 0.057800
Train Epoch: 2 [1229824/2541350 (48.4%)]	Loss: 0.077536
Train Epoch: 2 [1281024/2541350 (50.4%)]	Loss: 0.070476
Train Epoch: 2 [1332224/2541350 (52.4%)]	Loss: 0.076092
Train Epoch: 2 [1383424/2541350 (54.4%)]	Loss: 0.074587
Train Epoch: 2 [1434624/2541350 (56.5%)]	Loss: 0.056160
Train Epoch: 2 [1485824/2541350 (58.5%)]	Loss: 0.060286
Train Epoch: 2 [1537024/2541350 (60.5%)]	Loss: 0.059888
Train Epoch: 2 [1588224/2541350 (62.5%)]	Loss: 0.070995
Train Epoch: 2 [1639424/2541350 (64.5%)]	Loss: 0.066644
Train Epoch: 2 [1690624/2541350 (66.5%)]	Loss: 0.065537
Train Epoch: 2 [1741824/2541350 (68.5%)]	Loss: 0.084068
Train Epoch: 2 [1793024/2541350 (70.6%)]	Loss: 0.061381
Train Epoch: 2 [1844224/2541350 (72.6%)]	Loss: 0.067072
Train Epoch: 2 [1895424/2541350 (74.6%)]	Loss: 0.081431
Train Epoch: 2 [1946624/2541350 (76.6%)]	Loss: 0.066442
Train Epoch: 2 [1997824/2541350 (78.6%)]	Loss: 0.061364
Train Epoch: 2 [2049024/2541350 (80.6%)]	Loss: 0.061808
Train Epoch: 2 [2100224/2541350 (82.6%)]	Loss: 0.067859
Train Epoch: 2 [2151424/2541350 (84.7%)]	Loss: 0.051090
Train Epoch: 2 [2202624/2541350 (86.7%)]	Loss: 0.080668
Train Epoch: 2 [2253824/2541350 (88.7%)]	Loss: 0.079500
Train Epoch: 2 [2305024/2541350 (90.7%)]	Loss: 0.074352
Train Epoch: 2 [2356224/2541350 (92.7%)]	Loss: 0.064472
Train Epoch: 2 [2407424/2541350 (94.7%)]	Loss: 0.066955
Train Epoch: 2 [2458624/2541350 (96.7%)]	Loss: 0.067360
Train Epoch: 2 [2509824/2541350 (98.8%)]	Loss: 0.061958

ACC in fold#2 was 0.944


Balanced ACC in fold#2 was 0.906


MCC in fold#2 was 0.846


Confusion Matrix in fold#2: 
           nonRipple  Ripple
nonRipple     131367   27004
Ripple          8716  468250


Classification Report in fold#2: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.938       0.945  ...       0.942         0.944
recall            0.829       0.982  ...       0.906         0.944
f1-score          0.880       0.963  ...       0.922         0.943
sample size  158371.000  476966.000  ...  635337.000    635337.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2541350 (0.0%)]	Loss: 0.336271
Train Epoch: 1 [52224/2541350 (2.1%)]	Loss: 0.104457
Train Epoch: 1 [103424/2541350 (4.1%)]	Loss: 0.074707
Train Epoch: 1 [154624/2541350 (6.1%)]	Loss: 0.091443
Train Epoch: 1 [205824/2541350 (8.1%)]	Loss: 0.075557
Train Epoch: 1 [257024/2541350 (10.1%)]	Loss: 0.093421
Train Epoch: 1 [308224/2541350 (12.1%)]	Loss: 0.066731
Train Epoch: 1 [359424/2541350 (14.1%)]	Loss: 0.073329
Train Epoch: 1 [410624/2541350 (16.2%)]	Loss: 0.080362
Train Epoch: 1 [461824/2541350 (18.2%)]	Loss: 0.084919
Train Epoch: 1 [513024/2541350 (20.2%)]	Loss: 0.079687
Train Epoch: 1 [564224/2541350 (22.2%)]	Loss: 0.068899
Train Epoch: 1 [615424/2541350 (24.2%)]	Loss: 0.069836
Train Epoch: 1 [666624/2541350 (26.2%)]	Loss: 0.067383
Train Epoch: 1 [717824/2541350 (28.2%)]	Loss: 0.075207
Train Epoch: 1 [769024/2541350 (30.3%)]	Loss: 0.071732
Train Epoch: 1 [820224/2541350 (32.3%)]	Loss: 0.079728
Train Epoch: 1 [871424/2541350 (34.3%)]	Loss: 0.070438
Train Epoch: 1 [922624/2541350 (36.3%)]	Loss: 0.076232
Train Epoch: 1 [973824/2541350 (38.3%)]	Loss: 0.073286
Train Epoch: 1 [1025024/2541350 (40.3%)]	Loss: 0.072439
Train Epoch: 1 [1076224/2541350 (42.3%)]	Loss: 0.080777
Train Epoch: 1 [1127424/2541350 (44.4%)]	Loss: 0.072683
Train Epoch: 1 [1178624/2541350 (46.4%)]	Loss: 0.072555
Train Epoch: 1 [1229824/2541350 (48.4%)]	Loss: 0.078402
Train Epoch: 1 [1281024/2541350 (50.4%)]	Loss: 0.065480
Train Epoch: 1 [1332224/2541350 (52.4%)]	Loss: 0.076379
Train Epoch: 1 [1383424/2541350 (54.4%)]	Loss: 0.063213
Train Epoch: 1 [1434624/2541350 (56.5%)]	Loss: 0.078337
Train Epoch: 1 [1485824/2541350 (58.5%)]	Loss: 0.080913
Train Epoch: 1 [1537024/2541350 (60.5%)]	Loss: 0.073631
Train Epoch: 1 [1588224/2541350 (62.5%)]	Loss: 0.071640
Train Epoch: 1 [1639424/2541350 (64.5%)]	Loss: 0.060791
Train Epoch: 1 [1690624/2541350 (66.5%)]	Loss: 0.076250
Train Epoch: 1 [1741824/2541350 (68.5%)]	Loss: 0.073813
Train Epoch: 1 [1793024/2541350 (70.6%)]	Loss: 0.071858
Train Epoch: 1 [1844224/2541350 (72.6%)]	Loss: 0.075933
Train Epoch: 1 [1895424/2541350 (74.6%)]	Loss: 0.086107
Train Epoch: 1 [1946624/2541350 (76.6%)]	Loss: 0.073590
Train Epoch: 1 [1997824/2541350 (78.6%)]	Loss: 0.074614
Train Epoch: 1 [2049024/2541350 (80.6%)]	Loss: 0.071817
Train Epoch: 1 [2100224/2541350 (82.6%)]	Loss: 0.056741
Train Epoch: 1 [2151424/2541350 (84.7%)]	Loss: 0.062246
Train Epoch: 1 [2202624/2541350 (86.7%)]	Loss: 0.072242
Train Epoch: 1 [2253824/2541350 (88.7%)]	Loss: 0.076002
Train Epoch: 1 [2305024/2541350 (90.7%)]	Loss: 0.068304
Train Epoch: 1 [2356224/2541350 (92.7%)]	Loss: 0.082805
Train Epoch: 1 [2407424/2541350 (94.7%)]	Loss: 0.066799
Train Epoch: 1 [2458624/2541350 (96.7%)]	Loss: 0.061371
Train Epoch: 1 [2509824/2541350 (98.8%)]	Loss: 0.063721
Train Epoch: 2 [1024/2541350 (0.0%)]	Loss: 0.073884
Train Epoch: 2 [52224/2541350 (2.1%)]	Loss: 0.076197
Train Epoch: 2 [103424/2541350 (4.1%)]	Loss: 0.067873
Train Epoch: 2 [154624/2541350 (6.1%)]	Loss: 0.071414
Train Epoch: 2 [205824/2541350 (8.1%)]	Loss: 0.082428
Train Epoch: 2 [257024/2541350 (10.1%)]	Loss: 0.065867
Train Epoch: 2 [308224/2541350 (12.1%)]	Loss: 0.069021
Train Epoch: 2 [359424/2541350 (14.1%)]	Loss: 0.080132
Train Epoch: 2 [410624/2541350 (16.2%)]	Loss: 0.070771
Train Epoch: 2 [461824/2541350 (18.2%)]	Loss: 0.064157
Train Epoch: 2 [513024/2541350 (20.2%)]	Loss: 0.077395
Train Epoch: 2 [564224/2541350 (22.2%)]	Loss: 0.065731
Train Epoch: 2 [615424/2541350 (24.2%)]	Loss: 0.076490
Train Epoch: 2 [666624/2541350 (26.2%)]	Loss: 0.082700
Train Epoch: 2 [717824/2541350 (28.2%)]	Loss: 0.069464
Train Epoch: 2 [769024/2541350 (30.3%)]	Loss: 0.061419
Train Epoch: 2 [820224/2541350 (32.3%)]	Loss: 0.070433
Train Epoch: 2 [871424/2541350 (34.3%)]	Loss: 0.060450
Train Epoch: 2 [922624/2541350 (36.3%)]	Loss: 0.063439
Train Epoch: 2 [973824/2541350 (38.3%)]	Loss: 0.063064
Train Epoch: 2 [1025024/2541350 (40.3%)]	Loss: 0.062971
Train Epoch: 2 [1076224/2541350 (42.3%)]	Loss: 0.072948
Train Epoch: 2 [1127424/2541350 (44.4%)]	Loss: 0.072478
Train Epoch: 2 [1178624/2541350 (46.4%)]	Loss: 0.067797
Train Epoch: 2 [1229824/2541350 (48.4%)]	Loss: 0.074386
Train Epoch: 2 [1281024/2541350 (50.4%)]	Loss: 0.067984
Train Epoch: 2 [1332224/2541350 (52.4%)]	Loss: 0.066154
Train Epoch: 2 [1383424/2541350 (54.4%)]	Loss: 0.067500
Train Epoch: 2 [1434624/2541350 (56.5%)]	Loss: 0.076900
Train Epoch: 2 [1485824/2541350 (58.5%)]	Loss: 0.066629
Train Epoch: 2 [1537024/2541350 (60.5%)]	Loss: 0.071389
Train Epoch: 2 [1588224/2541350 (62.5%)]	Loss: 0.065158
Train Epoch: 2 [1639424/2541350 (64.5%)]	Loss: 0.069461
Train Epoch: 2 [1690624/2541350 (66.5%)]	Loss: 0.051756
Train Epoch: 2 [1741824/2541350 (68.5%)]	Loss: 0.070505
Train Epoch: 2 [1793024/2541350 (70.6%)]	Loss: 0.074783
Train Epoch: 2 [1844224/2541350 (72.6%)]	Loss: 0.080825
Train Epoch: 2 [1895424/2541350 (74.6%)]	Loss: 0.074886
Train Epoch: 2 [1946624/2541350 (76.6%)]	Loss: 0.076712
Train Epoch: 2 [1997824/2541350 (78.6%)]	Loss: 0.064252
Train Epoch: 2 [2049024/2541350 (80.6%)]	Loss: 0.072113
Train Epoch: 2 [2100224/2541350 (82.6%)]	Loss: 0.066856
Train Epoch: 2 [2151424/2541350 (84.7%)]	Loss: 0.070932
Train Epoch: 2 [2202624/2541350 (86.7%)]	Loss: 0.076066
Train Epoch: 2 [2253824/2541350 (88.7%)]	Loss: 0.076294
Train Epoch: 2 [2305024/2541350 (90.7%)]	Loss: 0.058758
Train Epoch: 2 [2356224/2541350 (92.7%)]	Loss: 0.070478
Train Epoch: 2 [2407424/2541350 (94.7%)]	Loss: 0.073177
Train Epoch: 2 [2458624/2541350 (96.7%)]	Loss: 0.067281
Train Epoch: 2 [2509824/2541350 (98.8%)]	Loss: 0.068446

ACC in fold#3 was 0.945


Balanced ACC in fold#3 was 0.905


MCC in fold#3 was 0.849


Confusion Matrix in fold#3: 
           nonRipple  Ripple
nonRipple     130832   27540
Ripple          7508  469457


Classification Report in fold#3: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.946       0.945  ...       0.945         0.945
recall            0.826       0.984  ...       0.905         0.945
f1-score          0.882       0.964  ...       0.923         0.944
sample size  158372.000  476965.000  ...  635337.000    635337.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2541350 (0.0%)]	Loss: 0.365384
Train Epoch: 1 [52224/2541350 (2.1%)]	Loss: 0.123729
Train Epoch: 1 [103424/2541350 (4.1%)]	Loss: 0.091411
Train Epoch: 1 [154624/2541350 (6.1%)]	Loss: 0.082744
Train Epoch: 1 [205824/2541350 (8.1%)]	Loss: 0.096998
Train Epoch: 1 [257024/2541350 (10.1%)]	Loss: 0.082333
Train Epoch: 1 [308224/2541350 (12.1%)]	Loss: 0.094899
Train Epoch: 1 [359424/2541350 (14.1%)]	Loss: 0.067855
Train Epoch: 1 [410624/2541350 (16.2%)]	Loss: 0.079441
Train Epoch: 1 [461824/2541350 (18.2%)]	Loss: 0.077333
Train Epoch: 1 [513024/2541350 (20.2%)]	Loss: 0.072583
Train Epoch: 1 [564224/2541350 (22.2%)]	Loss: 0.067487
Train Epoch: 1 [615424/2541350 (24.2%)]	Loss: 0.064843
Train Epoch: 1 [666624/2541350 (26.2%)]	Loss: 0.085151
Train Epoch: 1 [717824/2541350 (28.2%)]	Loss: 0.074446
Train Epoch: 1 [769024/2541350 (30.3%)]	Loss: 0.066174
Train Epoch: 1 [820224/2541350 (32.3%)]	Loss: 0.071356
Train Epoch: 1 [871424/2541350 (34.3%)]	Loss: 0.067366
Train Epoch: 1 [922624/2541350 (36.3%)]	Loss: 0.072430
Train Epoch: 1 [973824/2541350 (38.3%)]	Loss: 0.066179
Train Epoch: 1 [1025024/2541350 (40.3%)]	Loss: 0.079946
Train Epoch: 1 [1076224/2541350 (42.3%)]	Loss: 0.069903
Train Epoch: 1 [1127424/2541350 (44.4%)]	Loss: 0.078397
Train Epoch: 1 [1178624/2541350 (46.4%)]	Loss: 0.081250
Train Epoch: 1 [1229824/2541350 (48.4%)]	Loss: 0.078815
Train Epoch: 1 [1281024/2541350 (50.4%)]	Loss: 0.082573
Train Epoch: 1 [1332224/2541350 (52.4%)]	Loss: 0.080884
Train Epoch: 1 [1383424/2541350 (54.4%)]	Loss: 0.070537
Train Epoch: 1 [1434624/2541350 (56.5%)]	Loss: 0.057350
Train Epoch: 1 [1485824/2541350 (58.5%)]	Loss: 0.078859
Train Epoch: 1 [1537024/2541350 (60.5%)]	Loss: 0.075160
Train Epoch: 1 [1588224/2541350 (62.5%)]	Loss: 0.065925
Train Epoch: 1 [1639424/2541350 (64.5%)]	Loss: 0.073281
Train Epoch: 1 [1690624/2541350 (66.5%)]	Loss: 0.067976
Train Epoch: 1 [1741824/2541350 (68.5%)]	Loss: 0.074658
Train Epoch: 1 [1793024/2541350 (70.6%)]	Loss: 0.074721
Train Epoch: 1 [1844224/2541350 (72.6%)]	Loss: 0.065173
Train Epoch: 1 [1895424/2541350 (74.6%)]	Loss: 0.060729
Train Epoch: 1 [1946624/2541350 (76.6%)]	Loss: 0.074791
Train Epoch: 1 [1997824/2541350 (78.6%)]	Loss: 0.067981
Train Epoch: 1 [2049024/2541350 (80.6%)]	Loss: 0.065236
Train Epoch: 1 [2100224/2541350 (82.6%)]	Loss: 0.081309
Train Epoch: 1 [2151424/2541350 (84.7%)]	Loss: 0.076444
Train Epoch: 1 [2202624/2541350 (86.7%)]	Loss: 0.058972
Train Epoch: 1 [2253824/2541350 (88.7%)]	Loss: 0.072022
Train Epoch: 1 [2305024/2541350 (90.7%)]	Loss: 0.077931
Train Epoch: 1 [2356224/2541350 (92.7%)]	Loss: 0.063506
Train Epoch: 1 [2407424/2541350 (94.7%)]	Loss: 0.065511
Train Epoch: 1 [2458624/2541350 (96.7%)]	Loss: 0.072526
Train Epoch: 1 [2509824/2541350 (98.8%)]	Loss: 0.072072
Train Epoch: 2 [1024/2541350 (0.0%)]	Loss: 0.079155
Train Epoch: 2 [52224/2541350 (2.1%)]	Loss: 0.065602
Train Epoch: 2 [103424/2541350 (4.1%)]	Loss: 0.064784
Train Epoch: 2 [154624/2541350 (6.1%)]	Loss: 0.074302
Train Epoch: 2 [205824/2541350 (8.1%)]	Loss: 0.082165
Train Epoch: 2 [257024/2541350 (10.1%)]	Loss: 0.078273
Train Epoch: 2 [308224/2541350 (12.1%)]	Loss: 0.062005
Train Epoch: 2 [359424/2541350 (14.1%)]	Loss: 0.059856
Train Epoch: 2 [410624/2541350 (16.2%)]	Loss: 0.054340
Train Epoch: 2 [461824/2541350 (18.2%)]	Loss: 0.076940
Train Epoch: 2 [513024/2541350 (20.2%)]	Loss: 0.063342
Train Epoch: 2 [564224/2541350 (22.2%)]	Loss: 0.073127
Train Epoch: 2 [615424/2541350 (24.2%)]	Loss: 0.069785
Train Epoch: 2 [666624/2541350 (26.2%)]	Loss: 0.071001
Train Epoch: 2 [717824/2541350 (28.2%)]	Loss: 0.079682
Train Epoch: 2 [769024/2541350 (30.3%)]	Loss: 0.060405
Train Epoch: 2 [820224/2541350 (32.3%)]	Loss: 0.075379
Train Epoch: 2 [871424/2541350 (34.3%)]	Loss: 0.065072
Train Epoch: 2 [922624/2541350 (36.3%)]	Loss: 0.083614
Train Epoch: 2 [973824/2541350 (38.3%)]	Loss: 0.058542
Train Epoch: 2 [1025024/2541350 (40.3%)]	Loss: 0.071824
Train Epoch: 2 [1076224/2541350 (42.3%)]	Loss: 0.063027
Train Epoch: 2 [1127424/2541350 (44.4%)]	Loss: 0.061698
Train Epoch: 2 [1178624/2541350 (46.4%)]	Loss: 0.071261
Train Epoch: 2 [1229824/2541350 (48.4%)]	Loss: 0.074704
Train Epoch: 2 [1281024/2541350 (50.4%)]	Loss: 0.072695
Train Epoch: 2 [1332224/2541350 (52.4%)]	Loss: 0.068288
Train Epoch: 2 [1383424/2541350 (54.4%)]	Loss: 0.074171
Train Epoch: 2 [1434624/2541350 (56.5%)]	Loss: 0.062592
Train Epoch: 2 [1485824/2541350 (58.5%)]	Loss: 0.069077
Train Epoch: 2 [1537024/2541350 (60.5%)]	Loss: 0.071452
Train Epoch: 2 [1588224/2541350 (62.5%)]	Loss: 0.076037
Train Epoch: 2 [1639424/2541350 (64.5%)]	Loss: 0.071998
Train Epoch: 2 [1690624/2541350 (66.5%)]	Loss: 0.073655
Train Epoch: 2 [1741824/2541350 (68.5%)]	Loss: 0.075382
Train Epoch: 2 [1793024/2541350 (70.6%)]	Loss: 0.055212
Train Epoch: 2 [1844224/2541350 (72.6%)]	Loss: 0.060104
Train Epoch: 2 [1895424/2541350 (74.6%)]	Loss: 0.069235
Train Epoch: 2 [1946624/2541350 (76.6%)]	Loss: 0.077363
Train Epoch: 2 [1997824/2541350 (78.6%)]	Loss: 0.074596
Train Epoch: 2 [2049024/2541350 (80.6%)]	Loss: 0.062256
Train Epoch: 2 [2100224/2541350 (82.6%)]	Loss: 0.069957
Train Epoch: 2 [2151424/2541350 (84.7%)]	Loss: 0.065439
Train Epoch: 2 [2202624/2541350 (86.7%)]	Loss: 0.076025
Train Epoch: 2 [2253824/2541350 (88.7%)]	Loss: 0.078425
Train Epoch: 2 [2305024/2541350 (90.7%)]	Loss: 0.079741
Train Epoch: 2 [2356224/2541350 (92.7%)]	Loss: 0.068367
Train Epoch: 2 [2407424/2541350 (94.7%)]	Loss: 0.064727
Train Epoch: 2 [2458624/2541350 (96.7%)]	Loss: 0.070957
Train Epoch: 2 [2509824/2541350 (98.8%)]	Loss: 0.058233

ACC in fold#4 was 0.940


Balanced ACC in fold#4 was 0.933


MCC in fold#4 was 0.846


Confusion Matrix in fold#4: 
           nonRipple  Ripple
nonRipple     145268   13104
Ripple         24775  452190


Classification Report in fold#4: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.854       0.972  ...       0.913         0.943
recall            0.917       0.948  ...       0.933         0.940
f1-score          0.885       0.960  ...       0.922         0.941
sample size  158372.000  476965.000  ...  635337.000    635337.000

[4 rows x 5 columns]


Label Errors Rate:
0.014


 --- 5-fold CV overall metrics --- 


The Mattews correlation coefficient: 0.843 +/- 0.007 (mean +/- std.; n=5)


Balanced Accuracy Score: 0.912 +/- 0.012 (mean +/- std.; n=5)


Confusion Matrix (Test; sum; num. folds=5)
           nonRipple   Ripple
nonRipple     675058   116801
Ripple         68220  2316608


Classification Report (Test; mean; num. folds=5)
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.911       0.952  ...       0.932         0.942
recall            0.852       0.971  ...       0.912         0.942
f1-score          0.879       0.962  ...       0.920         0.941
sample size  158371.800  476965.600  ...  635337.400    635337.400

[4 rows x 5 columns]


Classification Report (Test; std; num. folds=5)
             nonRipple  Ripple  balanced accuracy  macro avg  weighted avg
precision        0.033   0.011              0.012      0.012         0.003
recall           0.037   0.013              0.012      0.012         0.003
f1-score         0.006   0.002              0.012      0.004         0.003
sample size      0.400   0.490              0.012      0.490         0.490


ROC AUC micro Score: 0.988 +/- 0.001 (mean +/- std.; n=5)


ROC AUC macro Score: 0.985 +/- 0.002 (mean +/- std.; n=5)


Precision-Recall AUC micro Score: 0.988 +/- 0.001 (mean +/- std.; n=5)


Precision-Recall AUC macro Score: 0.978 +/- 0.002 (mean +/- std.; n=5)


Saved to: ./data/okada/cleanlab_results/D05+/are_errors.npy


Saved to: ./data/okada/cleanlab_results/D05+/are_ripple_GMM.npy


Saved to: ./data/okada/cleanlab_results/D05+/psx_ripple.npy


Saved to: ./data/okada/cleanlab_results/D05+/mccs.csv


Saved to: ./data/okada/cleanlab_results/D05+/balanced_accs.csv


Saved to: ./data/okada/cleanlab_results/D05+/conf_mat/fold#0.png


Saved to: ./data/okada/cleanlab_results/D05+/conf_mat/fold#1.png


Saved to: ./data/okada/cleanlab_results/D05+/conf_mat/fold#2.png


Saved to: ./data/okada/cleanlab_results/D05+/conf_mat/fold#3.png


Saved to: ./data/okada/cleanlab_results/D05+/conf_mat/fold#4.png


Saved to: ./data/okada/cleanlab_results/D05+/conf_mat/conf_mats.csv


Saved to: ./data/okada/cleanlab_results/D05+/conf_mat/overall_sum.png


Saved to: ./data/okada/cleanlab_results/D05+/clf_reports.csv


Saved to: ./data/okada/cleanlab_results/D05+/aucs.csv


Saved to: ./data/okada/cleanlab_results/D05+/roc_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D05+/roc_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D05+/roc_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D05+/roc_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D05+/roc_curves/fold#4.png


Saved to: ./data/okada/cleanlab_results/D05+/pr_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D05+/pr_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D05+/pr_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D05+/pr_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D05+/pr_curves/fold#4.png


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D05+/tt6-4_fp16.pkl

