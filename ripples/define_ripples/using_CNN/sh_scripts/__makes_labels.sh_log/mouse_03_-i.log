
Random seeds have been fixed as 42


Random seeds have been fixed as 42


Standard Output/Error are going to be logged in the followings: 
  - ./ripples/define_ripples/using_CNN/makes_labels/log/stdout.log
  - ./ripples/define_ripples/using_CNN/makes_labels/log/stderr.log


Random seeds have been fixed as 42

D03+
['./data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy']

2021-0710-1736

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383430 (0.0%)]	Loss: 0.361066
Train Epoch: 1 [52224/2383430 (2.2%)]	Loss: 0.164225
Train Epoch: 1 [103424/2383430 (4.3%)]	Loss: 0.131858
Train Epoch: 1 [154624/2383430 (6.5%)]	Loss: 0.137416
Train Epoch: 1 [205824/2383430 (8.6%)]	Loss: 0.138344
Train Epoch: 1 [257024/2383430 (10.8%)]	Loss: 0.132445
Train Epoch: 1 [308224/2383430 (12.9%)]	Loss: 0.126520
Train Epoch: 1 [359424/2383430 (15.1%)]	Loss: 0.115829
Train Epoch: 1 [410624/2383430 (17.2%)]	Loss: 0.122016
Train Epoch: 1 [461824/2383430 (19.4%)]	Loss: 0.136797
Train Epoch: 1 [513024/2383430 (21.5%)]	Loss: 0.127288
Train Epoch: 1 [564224/2383430 (23.7%)]	Loss: 0.124196
Train Epoch: 1 [615424/2383430 (25.8%)]	Loss: 0.140168
Train Epoch: 1 [666624/2383430 (28.0%)]	Loss: 0.116441
Train Epoch: 1 [717824/2383430 (30.1%)]	Loss: 0.131954
Train Epoch: 1 [769024/2383430 (32.3%)]	Loss: 0.126475
Train Epoch: 1 [820224/2383430 (34.4%)]	Loss: 0.132125
Train Epoch: 1 [871424/2383430 (36.6%)]	Loss: 0.120678
Train Epoch: 1 [922624/2383430 (38.7%)]	Loss: 0.109210
Train Epoch: 1 [973824/2383430 (40.9%)]	Loss: 0.107157
Train Epoch: 1 [1025024/2383430 (43.0%)]	Loss: 0.120799
Train Epoch: 1 [1076224/2383430 (45.2%)]	Loss: 0.114336
Train Epoch: 1 [1127424/2383430 (47.3%)]	Loss: 0.113314
Train Epoch: 1 [1178624/2383430 (49.5%)]	Loss: 0.111373
Train Epoch: 1 [1229824/2383430 (51.6%)]	Loss: 0.110483
Train Epoch: 1 [1281024/2383430 (53.7%)]	Loss: 0.125377
Train Epoch: 1 [1332224/2383430 (55.9%)]	Loss: 0.121149
Train Epoch: 1 [1383424/2383430 (58.0%)]	Loss: 0.117828
Train Epoch: 1 [1434624/2383430 (60.2%)]	Loss: 0.150583
Train Epoch: 1 [1485824/2383430 (62.3%)]	Loss: 0.117103
Train Epoch: 1 [1537024/2383430 (64.5%)]	Loss: 0.102132
Train Epoch: 1 [1588224/2383430 (66.6%)]	Loss: 0.120843
Train Epoch: 1 [1639424/2383430 (68.8%)]	Loss: 0.124612
Train Epoch: 1 [1690624/2383430 (70.9%)]	Loss: 0.114548
Train Epoch: 1 [1741824/2383430 (73.1%)]	Loss: 0.124676
Train Epoch: 1 [1793024/2383430 (75.2%)]	Loss: 0.112840
Train Epoch: 1 [1844224/2383430 (77.4%)]	Loss: 0.108773
Train Epoch: 1 [1895424/2383430 (79.5%)]	Loss: 0.120661
Train Epoch: 1 [1946624/2383430 (81.7%)]	Loss: 0.111764
Train Epoch: 1 [1997824/2383430 (83.8%)]	Loss: 0.105553
Train Epoch: 1 [2049024/2383430 (86.0%)]	Loss: 0.116842
Train Epoch: 1 [2100224/2383430 (88.1%)]	Loss: 0.126677
Train Epoch: 1 [2151424/2383430 (90.3%)]	Loss: 0.114277
Train Epoch: 1 [2202624/2383430 (92.4%)]	Loss: 0.110377
Train Epoch: 1 [2253824/2383430 (94.6%)]	Loss: 0.118118
Train Epoch: 1 [2305024/2383430 (96.7%)]	Loss: 0.106307
Train Epoch: 1 [2356224/2383430 (98.9%)]	Loss: 0.120606
Train Epoch: 2 [1024/2383430 (0.0%)]	Loss: 0.118131
Train Epoch: 2 [52224/2383430 (2.2%)]	Loss: 0.127028
Train Epoch: 2 [103424/2383430 (4.3%)]	Loss: 0.108128
Train Epoch: 2 [154624/2383430 (6.5%)]	Loss: 0.130201
Train Epoch: 2 [205824/2383430 (8.6%)]	Loss: 0.108195
Train Epoch: 2 [257024/2383430 (10.8%)]	Loss: 0.133525
Train Epoch: 2 [308224/2383430 (12.9%)]	Loss: 0.110363
Train Epoch: 2 [359424/2383430 (15.1%)]	Loss: 0.103912
Train Epoch: 2 [410624/2383430 (17.2%)]	Loss: 0.108793
Train Epoch: 2 [461824/2383430 (19.4%)]	Loss: 0.122643
Train Epoch: 2 [513024/2383430 (21.5%)]	Loss: 0.119856
Train Epoch: 2 [564224/2383430 (23.7%)]	Loss: 0.106726
Train Epoch: 2 [615424/2383430 (25.8%)]	Loss: 0.120048
Train Epoch: 2 [666624/2383430 (28.0%)]	Loss: 0.113687
Train Epoch: 2 [717824/2383430 (30.1%)]	Loss: 0.135713
Train Epoch: 2 [769024/2383430 (32.3%)]	Loss: 0.117595
Train Epoch: 2 [820224/2383430 (34.4%)]	Loss: 0.113533
Train Epoch: 2 [871424/2383430 (36.6%)]	Loss: 0.112791
Train Epoch: 2 [922624/2383430 (38.7%)]	Loss: 0.124557
Train Epoch: 2 [973824/2383430 (40.9%)]	Loss: 0.108589
Train Epoch: 2 [1025024/2383430 (43.0%)]	Loss: 0.103317
Train Epoch: 2 [1076224/2383430 (45.2%)]	Loss: 0.112628
Train Epoch: 2 [1127424/2383430 (47.3%)]	Loss: 0.109110
Train Epoch: 2 [1178624/2383430 (49.5%)]	Loss: 0.113077
Train Epoch: 2 [1229824/2383430 (51.6%)]	Loss: 0.108801
Train Epoch: 2 [1281024/2383430 (53.7%)]	Loss: 0.114072
Train Epoch: 2 [1332224/2383430 (55.9%)]	Loss: 0.110984
Train Epoch: 2 [1383424/2383430 (58.0%)]	Loss: 0.120517
Train Epoch: 2 [1434624/2383430 (60.2%)]	Loss: 0.096485
Train Epoch: 2 [1485824/2383430 (62.3%)]	Loss: 0.113024
Train Epoch: 2 [1537024/2383430 (64.5%)]	Loss: 0.139660
Train Epoch: 2 [1588224/2383430 (66.6%)]	Loss: 0.109469
Train Epoch: 2 [1639424/2383430 (68.8%)]	Loss: 0.122177
Train Epoch: 2 [1690624/2383430 (70.9%)]	Loss: 0.107421
Train Epoch: 2 [1741824/2383430 (73.1%)]	Loss: 0.113552
Train Epoch: 2 [1793024/2383430 (75.2%)]	Loss: 0.124112
Train Epoch: 2 [1844224/2383430 (77.4%)]	Loss: 0.121515
Train Epoch: 2 [1895424/2383430 (79.5%)]	Loss: 0.114310
Train Epoch: 2 [1946624/2383430 (81.7%)]	Loss: 0.122346
Train Epoch: 2 [1997824/2383430 (83.8%)]	Loss: 0.115035
Train Epoch: 2 [2049024/2383430 (86.0%)]	Loss: 0.114125
Train Epoch: 2 [2100224/2383430 (88.1%)]	Loss: 0.131661
Train Epoch: 2 [2151424/2383430 (90.3%)]	Loss: 0.109240
Train Epoch: 2 [2202624/2383430 (92.4%)]	Loss: 0.106535
Train Epoch: 2 [2253824/2383430 (94.6%)]	Loss: 0.122387
Train Epoch: 2 [2305024/2383430 (96.7%)]	Loss: 0.124217
Train Epoch: 2 [2356224/2383430 (98.9%)]	Loss: 0.119939

ACC in fold#0 was 0.908


Balanced ACC in fold#0 was 0.902


MCC in fold#0 was 0.808


Confusion Matrix in fold#0: 
           nonRipple  Ripple
nonRipple     206549   30287
Ripple         24347  334675


Classification Report in fold#0: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.895       0.917  ...       0.906         0.908
recall            0.872       0.932  ...       0.902         0.908
f1-score          0.883       0.925  ...       0.904         0.908
sample size  236836.000  359022.000  ...  595858.000    595858.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383430 (0.0%)]	Loss: 0.378441
Train Epoch: 1 [52224/2383430 (2.2%)]	Loss: 0.167304
Train Epoch: 1 [103424/2383430 (4.3%)]	Loss: 0.124727
Train Epoch: 1 [154624/2383430 (6.5%)]	Loss: 0.142555
Train Epoch: 1 [205824/2383430 (8.6%)]	Loss: 0.142577
Train Epoch: 1 [257024/2383430 (10.8%)]	Loss: 0.135681
Train Epoch: 1 [308224/2383430 (12.9%)]	Loss: 0.120450
Train Epoch: 1 [359424/2383430 (15.1%)]	Loss: 0.144444
Train Epoch: 1 [410624/2383430 (17.2%)]	Loss: 0.121065
Train Epoch: 1 [461824/2383430 (19.4%)]	Loss: 0.129567
Train Epoch: 1 [513024/2383430 (21.5%)]	Loss: 0.116376
Train Epoch: 1 [564224/2383430 (23.7%)]	Loss: 0.110279
Train Epoch: 1 [615424/2383430 (25.8%)]	Loss: 0.110572
Train Epoch: 1 [666624/2383430 (28.0%)]	Loss: 0.114813
Train Epoch: 1 [717824/2383430 (30.1%)]	Loss: 0.122268
Train Epoch: 1 [769024/2383430 (32.3%)]	Loss: 0.134393
Train Epoch: 1 [820224/2383430 (34.4%)]	Loss: 0.123624
Train Epoch: 1 [871424/2383430 (36.6%)]	Loss: 0.109906
Train Epoch: 1 [922624/2383430 (38.7%)]	Loss: 0.114019
Train Epoch: 1 [973824/2383430 (40.9%)]	Loss: 0.126713
Train Epoch: 1 [1025024/2383430 (43.0%)]	Loss: 0.111925
Train Epoch: 1 [1076224/2383430 (45.2%)]	Loss: 0.114524
Train Epoch: 1 [1127424/2383430 (47.3%)]	Loss: 0.128698
Train Epoch: 1 [1178624/2383430 (49.5%)]	Loss: 0.121934
Train Epoch: 1 [1229824/2383430 (51.6%)]	Loss: 0.123569
Train Epoch: 1 [1281024/2383430 (53.7%)]	Loss: 0.115944
Train Epoch: 1 [1332224/2383430 (55.9%)]	Loss: 0.110697
Train Epoch: 1 [1383424/2383430 (58.0%)]	Loss: 0.126394
Train Epoch: 1 [1434624/2383430 (60.2%)]	Loss: 0.111275
Train Epoch: 1 [1485824/2383430 (62.3%)]	Loss: 0.101772
Train Epoch: 1 [1537024/2383430 (64.5%)]	Loss: 0.118373
Train Epoch: 1 [1588224/2383430 (66.6%)]	Loss: 0.114855
Train Epoch: 1 [1639424/2383430 (68.8%)]	Loss: 0.123835
Train Epoch: 1 [1690624/2383430 (70.9%)]	Loss: 0.116628
Train Epoch: 1 [1741824/2383430 (73.1%)]	Loss: 0.123965
Train Epoch: 1 [1793024/2383430 (75.2%)]	Loss: 0.111390
Train Epoch: 1 [1844224/2383430 (77.4%)]	Loss: 0.108462
Train Epoch: 1 [1895424/2383430 (79.5%)]	Loss: 0.123360
Train Epoch: 1 [1946624/2383430 (81.7%)]	Loss: 0.121725
Train Epoch: 1 [1997824/2383430 (83.8%)]	Loss: 0.121331
Train Epoch: 1 [2049024/2383430 (86.0%)]	Loss: 0.114685
Train Epoch: 1 [2100224/2383430 (88.1%)]	Loss: 0.129480
Train Epoch: 1 [2151424/2383430 (90.3%)]	Loss: 0.108172
Train Epoch: 1 [2202624/2383430 (92.4%)]	Loss: 0.103096
Train Epoch: 1 [2253824/2383430 (94.6%)]	Loss: 0.127755
Train Epoch: 1 [2305024/2383430 (96.7%)]	Loss: 0.110127
Train Epoch: 1 [2356224/2383430 (98.9%)]	Loss: 0.123913
Train Epoch: 2 [1024/2383430 (0.0%)]	Loss: 0.122838
Train Epoch: 2 [52224/2383430 (2.2%)]	Loss: 0.127895
Train Epoch: 2 [103424/2383430 (4.3%)]	Loss: 0.118568
Train Epoch: 2 [154624/2383430 (6.5%)]	Loss: 0.118748
Train Epoch: 2 [205824/2383430 (8.6%)]	Loss: 0.094691
Train Epoch: 2 [257024/2383430 (10.8%)]	Loss: 0.125407
Train Epoch: 2 [308224/2383430 (12.9%)]	Loss: 0.126854
Train Epoch: 2 [359424/2383430 (15.1%)]	Loss: 0.117526
Train Epoch: 2 [410624/2383430 (17.2%)]	Loss: 0.121412
Train Epoch: 2 [461824/2383430 (19.4%)]	Loss: 0.127615
Train Epoch: 2 [513024/2383430 (21.5%)]	Loss: 0.127388
Train Epoch: 2 [564224/2383430 (23.7%)]	Loss: 0.124192
Train Epoch: 2 [615424/2383430 (25.8%)]	Loss: 0.119058
Train Epoch: 2 [666624/2383430 (28.0%)]	Loss: 0.112501
Train Epoch: 2 [717824/2383430 (30.1%)]	Loss: 0.116830
Train Epoch: 2 [769024/2383430 (32.3%)]	Loss: 0.123380
Train Epoch: 2 [820224/2383430 (34.4%)]	Loss: 0.117266
Train Epoch: 2 [871424/2383430 (36.6%)]	Loss: 0.116018
Train Epoch: 2 [922624/2383430 (38.7%)]	Loss: 0.120177
Train Epoch: 2 [973824/2383430 (40.9%)]	Loss: 0.112811
Train Epoch: 2 [1025024/2383430 (43.0%)]	Loss: 0.119540
Train Epoch: 2 [1076224/2383430 (45.2%)]	Loss: 0.106490
Train Epoch: 2 [1127424/2383430 (47.3%)]	Loss: 0.121241
Train Epoch: 2 [1178624/2383430 (49.5%)]	Loss: 0.121044
Train Epoch: 2 [1229824/2383430 (51.6%)]	Loss: 0.138022
Train Epoch: 2 [1281024/2383430 (53.7%)]	Loss: 0.121408
Train Epoch: 2 [1332224/2383430 (55.9%)]	Loss: 0.118460
Train Epoch: 2 [1383424/2383430 (58.0%)]	Loss: 0.110664
Train Epoch: 2 [1434624/2383430 (60.2%)]	Loss: 0.116744
Train Epoch: 2 [1485824/2383430 (62.3%)]	Loss: 0.116278
Train Epoch: 2 [1537024/2383430 (64.5%)]	Loss: 0.112633
Train Epoch: 2 [1588224/2383430 (66.6%)]	Loss: 0.117814
Train Epoch: 2 [1639424/2383430 (68.8%)]	Loss: 0.127784
Train Epoch: 2 [1690624/2383430 (70.9%)]	Loss: 0.112358
Train Epoch: 2 [1741824/2383430 (73.1%)]	Loss: 0.120325
Train Epoch: 2 [1793024/2383430 (75.2%)]	Loss: 0.114498
Train Epoch: 2 [1844224/2383430 (77.4%)]	Loss: 0.132287
Train Epoch: 2 [1895424/2383430 (79.5%)]	Loss: 0.120063
Train Epoch: 2 [1946624/2383430 (81.7%)]	Loss: 0.117473
Train Epoch: 2 [1997824/2383430 (83.8%)]	Loss: 0.120057
Train Epoch: 2 [2049024/2383430 (86.0%)]	Loss: 0.115080
Train Epoch: 2 [2100224/2383430 (88.1%)]	Loss: 0.115411
Train Epoch: 2 [2151424/2383430 (90.3%)]	Loss: 0.123998
Train Epoch: 2 [2202624/2383430 (92.4%)]	Loss: 0.109083
Train Epoch: 2 [2253824/2383430 (94.6%)]	Loss: 0.122020
Train Epoch: 2 [2305024/2383430 (96.7%)]	Loss: 0.118741
Train Epoch: 2 [2356224/2383430 (98.9%)]	Loss: 0.122688

ACC in fold#1 was 0.907


Balanced ACC in fold#1 was 0.909


MCC in fold#1 was 0.809


Confusion Matrix in fold#1: 
           nonRipple  Ripple
nonRipple     217128   19708
Ripple         35656  323366


Classification Report in fold#1: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.859       0.943  ...       0.901         0.909
recall            0.917       0.901  ...       0.909         0.907
f1-score          0.887       0.921  ...       0.904         0.908
sample size  236836.000  359022.000  ...  595858.000    595858.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383430 (0.0%)]	Loss: 0.352946
Train Epoch: 1 [52224/2383430 (2.2%)]	Loss: 0.163207
Train Epoch: 1 [103424/2383430 (4.3%)]	Loss: 0.131592
Train Epoch: 1 [154624/2383430 (6.5%)]	Loss: 0.109132
Train Epoch: 1 [205824/2383430 (8.6%)]	Loss: 0.117490
Train Epoch: 1 [257024/2383430 (10.8%)]	Loss: 0.121547
Train Epoch: 1 [308224/2383430 (12.9%)]	Loss: 0.132931
Train Epoch: 1 [359424/2383430 (15.1%)]	Loss: 0.116923
Train Epoch: 1 [410624/2383430 (17.2%)]	Loss: 0.122372
Train Epoch: 1 [461824/2383430 (19.4%)]	Loss: 0.122861
Train Epoch: 1 [513024/2383430 (21.5%)]	Loss: 0.126673
Train Epoch: 1 [564224/2383430 (23.7%)]	Loss: 0.108184
Train Epoch: 1 [615424/2383430 (25.8%)]	Loss: 0.131261
Train Epoch: 1 [666624/2383430 (28.0%)]	Loss: 0.117446
Train Epoch: 1 [717824/2383430 (30.1%)]	Loss: 0.138197
Train Epoch: 1 [769024/2383430 (32.3%)]	Loss: 0.122692
Train Epoch: 1 [820224/2383430 (34.4%)]	Loss: 0.128731
Train Epoch: 1 [871424/2383430 (36.6%)]	Loss: 0.126012
Train Epoch: 1 [922624/2383430 (38.7%)]	Loss: 0.105113
Train Epoch: 1 [973824/2383430 (40.9%)]	Loss: 0.119781
Train Epoch: 1 [1025024/2383430 (43.0%)]	Loss: 0.121179
Train Epoch: 1 [1076224/2383430 (45.2%)]	Loss: 0.119934
Train Epoch: 1 [1127424/2383430 (47.3%)]	Loss: 0.102783
Train Epoch: 1 [1178624/2383430 (49.5%)]	Loss: 0.132660
Train Epoch: 1 [1229824/2383430 (51.6%)]	Loss: 0.119874
Train Epoch: 1 [1281024/2383430 (53.7%)]	Loss: 0.109658
Train Epoch: 1 [1332224/2383430 (55.9%)]	Loss: 0.126077
Train Epoch: 1 [1383424/2383430 (58.0%)]	Loss: 0.120002
Train Epoch: 1 [1434624/2383430 (60.2%)]	Loss: 0.103971
Train Epoch: 1 [1485824/2383430 (62.3%)]	Loss: 0.125760
Train Epoch: 1 [1537024/2383430 (64.5%)]	Loss: 0.108931
Train Epoch: 1 [1588224/2383430 (66.6%)]	Loss: 0.129030
Train Epoch: 1 [1639424/2383430 (68.8%)]	Loss: 0.119963
Train Epoch: 1 [1690624/2383430 (70.9%)]	Loss: 0.119981
Train Epoch: 1 [1741824/2383430 (73.1%)]	Loss: 0.118414
Train Epoch: 1 [1793024/2383430 (75.2%)]	Loss: 0.117380
Train Epoch: 1 [1844224/2383430 (77.4%)]	Loss: 0.116891
Train Epoch: 1 [1895424/2383430 (79.5%)]	Loss: 0.126463
Train Epoch: 1 [1946624/2383430 (81.7%)]	Loss: 0.129211
Train Epoch: 1 [1997824/2383430 (83.8%)]	Loss: 0.123408
Train Epoch: 1 [2049024/2383430 (86.0%)]	Loss: 0.092356
Train Epoch: 1 [2100224/2383430 (88.1%)]	Loss: 0.102549
Train Epoch: 1 [2151424/2383430 (90.3%)]	Loss: 0.129925
Train Epoch: 1 [2202624/2383430 (92.4%)]	Loss: 0.130409
Train Epoch: 1 [2253824/2383430 (94.6%)]	Loss: 0.118525
Train Epoch: 1 [2305024/2383430 (96.7%)]	Loss: 0.116217
Train Epoch: 1 [2356224/2383430 (98.9%)]	Loss: 0.123733
Train Epoch: 2 [1024/2383430 (0.0%)]	Loss: 0.113606
Train Epoch: 2 [52224/2383430 (2.2%)]	Loss: 0.110641
Train Epoch: 2 [103424/2383430 (4.3%)]	Loss: 0.124143
Train Epoch: 2 [154624/2383430 (6.5%)]	Loss: 0.104410
Train Epoch: 2 [205824/2383430 (8.6%)]	Loss: 0.115650
Train Epoch: 2 [257024/2383430 (10.8%)]	Loss: 0.114635
Train Epoch: 2 [308224/2383430 (12.9%)]	Loss: 0.121497
Train Epoch: 2 [359424/2383430 (15.1%)]	Loss: 0.134641
Train Epoch: 2 [410624/2383430 (17.2%)]	Loss: 0.119547
Train Epoch: 2 [461824/2383430 (19.4%)]	Loss: 0.100646
Train Epoch: 2 [513024/2383430 (21.5%)]	Loss: 0.138601
Train Epoch: 2 [564224/2383430 (23.7%)]	Loss: 0.123592
Train Epoch: 2 [615424/2383430 (25.8%)]	Loss: 0.114520
Train Epoch: 2 [666624/2383430 (28.0%)]	Loss: 0.115620
Train Epoch: 2 [717824/2383430 (30.1%)]	Loss: 0.105513
Train Epoch: 2 [769024/2383430 (32.3%)]	Loss: 0.117880
Train Epoch: 2 [820224/2383430 (34.4%)]	Loss: 0.109417
Train Epoch: 2 [871424/2383430 (36.6%)]	Loss: 0.116261
Train Epoch: 2 [922624/2383430 (38.7%)]	Loss: 0.129647
Train Epoch: 2 [973824/2383430 (40.9%)]	Loss: 0.128151
Train Epoch: 2 [1025024/2383430 (43.0%)]	Loss: 0.115804
Train Epoch: 2 [1076224/2383430 (45.2%)]	Loss: 0.124423
Train Epoch: 2 [1127424/2383430 (47.3%)]	Loss: 0.128940
Train Epoch: 2 [1178624/2383430 (49.5%)]	Loss: 0.117741
Train Epoch: 2 [1229824/2383430 (51.6%)]	Loss: 0.115575
Train Epoch: 2 [1281024/2383430 (53.7%)]	Loss: 0.110567
Train Epoch: 2 [1332224/2383430 (55.9%)]	Loss: 0.121683
Train Epoch: 2 [1383424/2383430 (58.0%)]	Loss: 0.130450
Train Epoch: 2 [1434624/2383430 (60.2%)]	Loss: 0.112495
Train Epoch: 2 [1485824/2383430 (62.3%)]	Loss: 0.108243
Train Epoch: 2 [1537024/2383430 (64.5%)]	Loss: 0.122037
Train Epoch: 2 [1588224/2383430 (66.6%)]	Loss: 0.108862
Train Epoch: 2 [1639424/2383430 (68.8%)]	Loss: 0.106889
Train Epoch: 2 [1690624/2383430 (70.9%)]	Loss: 0.117824
Train Epoch: 2 [1741824/2383430 (73.1%)]	Loss: 0.100312
Train Epoch: 2 [1793024/2383430 (75.2%)]	Loss: 0.133530
Train Epoch: 2 [1844224/2383430 (77.4%)]	Loss: 0.112296
Train Epoch: 2 [1895424/2383430 (79.5%)]	Loss: 0.109690
Train Epoch: 2 [1946624/2383430 (81.7%)]	Loss: 0.115589
Train Epoch: 2 [1997824/2383430 (83.8%)]	Loss: 0.116722
Train Epoch: 2 [2049024/2383430 (86.0%)]	Loss: 0.109548
Train Epoch: 2 [2100224/2383430 (88.1%)]	Loss: 0.117427
Train Epoch: 2 [2151424/2383430 (90.3%)]	Loss: 0.121125
Train Epoch: 2 [2202624/2383430 (92.4%)]	Loss: 0.113974
Train Epoch: 2 [2253824/2383430 (94.6%)]	Loss: 0.109418
Train Epoch: 2 [2305024/2383430 (96.7%)]	Loss: 0.105720
Train Epoch: 2 [2356224/2383430 (98.9%)]	Loss: 0.114844

ACC in fold#2 was 0.907


Balanced ACC in fold#2 was 0.897


MCC in fold#2 was 0.805


Confusion Matrix in fold#2: 
           nonRipple  Ripple
nonRipple     200590   36246
Ripple         19259  339763


Classification Report in fold#2: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.912       0.904  ...       0.908         0.907
recall            0.847       0.946  ...       0.897         0.907
f1-score          0.878       0.924  ...       0.901         0.906
sample size  236836.000  359022.000  ...  595858.000    595858.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383431 (0.0%)]	Loss: 0.344328
Train Epoch: 1 [52224/2383431 (2.2%)]	Loss: 0.177349
Train Epoch: 1 [103424/2383431 (4.3%)]	Loss: 0.139492
Train Epoch: 1 [154624/2383431 (6.5%)]	Loss: 0.128585
Train Epoch: 1 [205824/2383431 (8.6%)]	Loss: 0.134511
Train Epoch: 1 [257024/2383431 (10.8%)]	Loss: 0.126826
Train Epoch: 1 [308224/2383431 (12.9%)]	Loss: 0.117488
Train Epoch: 1 [359424/2383431 (15.1%)]	Loss: 0.126199
Train Epoch: 1 [410624/2383431 (17.2%)]	Loss: 0.133214
Train Epoch: 1 [461824/2383431 (19.4%)]	Loss: 0.127121
Train Epoch: 1 [513024/2383431 (21.5%)]	Loss: 0.127108
Train Epoch: 1 [564224/2383431 (23.7%)]	Loss: 0.141021
Train Epoch: 1 [615424/2383431 (25.8%)]	Loss: 0.111161
Train Epoch: 1 [666624/2383431 (28.0%)]	Loss: 0.117639
Train Epoch: 1 [717824/2383431 (30.1%)]	Loss: 0.138861
Train Epoch: 1 [769024/2383431 (32.3%)]	Loss: 0.101564
Train Epoch: 1 [820224/2383431 (34.4%)]	Loss: 0.119595
Train Epoch: 1 [871424/2383431 (36.6%)]	Loss: 0.112725
Train Epoch: 1 [922624/2383431 (38.7%)]	Loss: 0.119072
Train Epoch: 1 [973824/2383431 (40.9%)]	Loss: 0.114270
Train Epoch: 1 [1025024/2383431 (43.0%)]	Loss: 0.109729
Train Epoch: 1 [1076224/2383431 (45.2%)]	Loss: 0.122358
Train Epoch: 1 [1127424/2383431 (47.3%)]	Loss: 0.126107
Train Epoch: 1 [1178624/2383431 (49.5%)]	Loss: 0.140828
Train Epoch: 1 [1229824/2383431 (51.6%)]	Loss: 0.105005
Train Epoch: 1 [1281024/2383431 (53.7%)]	Loss: 0.119883
Train Epoch: 1 [1332224/2383431 (55.9%)]	Loss: 0.115853
Train Epoch: 1 [1383424/2383431 (58.0%)]	Loss: 0.119219
Train Epoch: 1 [1434624/2383431 (60.2%)]	Loss: 0.111529
Train Epoch: 1 [1485824/2383431 (62.3%)]	Loss: 0.118034
Train Epoch: 1 [1537024/2383431 (64.5%)]	Loss: 0.125675
Train Epoch: 1 [1588224/2383431 (66.6%)]	Loss: 0.118980
Train Epoch: 1 [1639424/2383431 (68.8%)]	Loss: 0.127807
Train Epoch: 1 [1690624/2383431 (70.9%)]	Loss: 0.125989
Train Epoch: 1 [1741824/2383431 (73.1%)]	Loss: 0.110065
Train Epoch: 1 [1793024/2383431 (75.2%)]	Loss: 0.122554
Train Epoch: 1 [1844224/2383431 (77.4%)]	Loss: 0.099279
Train Epoch: 1 [1895424/2383431 (79.5%)]	Loss: 0.108768
Train Epoch: 1 [1946624/2383431 (81.7%)]	Loss: 0.116798
Train Epoch: 1 [1997824/2383431 (83.8%)]	Loss: 0.115222
Train Epoch: 1 [2049024/2383431 (86.0%)]	Loss: 0.119589
Train Epoch: 1 [2100224/2383431 (88.1%)]	Loss: 0.143207
Train Epoch: 1 [2151424/2383431 (90.3%)]	Loss: 0.111829
Train Epoch: 1 [2202624/2383431 (92.4%)]	Loss: 0.131950
Train Epoch: 1 [2253824/2383431 (94.6%)]	Loss: 0.102615
Train Epoch: 1 [2305024/2383431 (96.7%)]	Loss: 0.131764
Train Epoch: 1 [2356224/2383431 (98.9%)]	Loss: 0.107658
Train Epoch: 2 [1024/2383431 (0.0%)]	Loss: 0.122004
Train Epoch: 2 [52224/2383431 (2.2%)]	Loss: 0.114710
Train Epoch: 2 [103424/2383431 (4.3%)]	Loss: 0.127336
Train Epoch: 2 [154624/2383431 (6.5%)]	Loss: 0.109323
Train Epoch: 2 [205824/2383431 (8.6%)]	Loss: 0.116390
Train Epoch: 2 [257024/2383431 (10.8%)]	Loss: 0.106842
Train Epoch: 2 [308224/2383431 (12.9%)]	Loss: 0.128071
Train Epoch: 2 [359424/2383431 (15.1%)]	Loss: 0.108810
Train Epoch: 2 [410624/2383431 (17.2%)]	Loss: 0.101712
Train Epoch: 2 [461824/2383431 (19.4%)]	Loss: 0.120294
Train Epoch: 2 [513024/2383431 (21.5%)]	Loss: 0.105304
Train Epoch: 2 [564224/2383431 (23.7%)]	Loss: 0.122077
Train Epoch: 2 [615424/2383431 (25.8%)]	Loss: 0.111352
Train Epoch: 2 [666624/2383431 (28.0%)]	Loss: 0.112013
Train Epoch: 2 [717824/2383431 (30.1%)]	Loss: 0.118367
Train Epoch: 2 [769024/2383431 (32.3%)]	Loss: 0.112181
Train Epoch: 2 [820224/2383431 (34.4%)]	Loss: 0.118867
Train Epoch: 2 [871424/2383431 (36.6%)]	Loss: 0.123277
Train Epoch: 2 [922624/2383431 (38.7%)]	Loss: 0.119112
Train Epoch: 2 [973824/2383431 (40.9%)]	Loss: 0.109422
Train Epoch: 2 [1025024/2383431 (43.0%)]	Loss: 0.120826
Train Epoch: 2 [1076224/2383431 (45.2%)]	Loss: 0.123063
Train Epoch: 2 [1127424/2383431 (47.3%)]	Loss: 0.110786
Train Epoch: 2 [1178624/2383431 (49.5%)]	Loss: 0.105665
Train Epoch: 2 [1229824/2383431 (51.6%)]	Loss: 0.120495
Train Epoch: 2 [1281024/2383431 (53.7%)]	Loss: 0.112332
Train Epoch: 2 [1332224/2383431 (55.9%)]	Loss: 0.117248
Train Epoch: 2 [1383424/2383431 (58.0%)]	Loss: 0.109564
Train Epoch: 2 [1434624/2383431 (60.2%)]	Loss: 0.125365
Train Epoch: 2 [1485824/2383431 (62.3%)]	Loss: 0.117242
Train Epoch: 2 [1537024/2383431 (64.5%)]	Loss: 0.133475
Train Epoch: 2 [1588224/2383431 (66.6%)]	Loss: 0.125630
Train Epoch: 2 [1639424/2383431 (68.8%)]	Loss: 0.104676
Train Epoch: 2 [1690624/2383431 (70.9%)]	Loss: 0.110785
Train Epoch: 2 [1741824/2383431 (73.1%)]	Loss: 0.107676
Train Epoch: 2 [1793024/2383431 (75.2%)]	Loss: 0.105001
Train Epoch: 2 [1844224/2383431 (77.4%)]	Loss: 0.091694
Train Epoch: 2 [1895424/2383431 (79.5%)]	Loss: 0.126350
Train Epoch: 2 [1946624/2383431 (81.7%)]	Loss: 0.113465
Train Epoch: 2 [1997824/2383431 (83.8%)]	Loss: 0.118557
Train Epoch: 2 [2049024/2383431 (86.0%)]	Loss: 0.106234
Train Epoch: 2 [2100224/2383431 (88.1%)]	Loss: 0.114448
Train Epoch: 2 [2151424/2383431 (90.3%)]	Loss: 0.116800
Train Epoch: 2 [2202624/2383431 (92.4%)]	Loss: 0.103680
Train Epoch: 2 [2253824/2383431 (94.6%)]	Loss: 0.105999
Train Epoch: 2 [2305024/2383431 (96.7%)]	Loss: 0.112700
Train Epoch: 2 [2356224/2383431 (98.9%)]	Loss: 0.115540

ACC in fold#3 was 0.887


Balanced ACC in fold#3 was 0.894


MCC in fold#3 was 0.774


Confusion Matrix in fold#3: 
           nonRipple  Ripple
nonRipple     220145   16691
Ripple         50815  308206


Classification Report in fold#3: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.812       0.949  ...       0.881         0.895
recall            0.930       0.858  ...       0.894         0.887
f1-score          0.867       0.901  ...       0.884         0.888
sample size  236836.000  359021.000  ...  595857.000    595857.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/2383431 (0.0%)]	Loss: 0.346927
Train Epoch: 1 [52224/2383431 (2.2%)]	Loss: 0.161428
Train Epoch: 1 [103424/2383431 (4.3%)]	Loss: 0.141318
Train Epoch: 1 [154624/2383431 (6.5%)]	Loss: 0.127696
Train Epoch: 1 [205824/2383431 (8.6%)]	Loss: 0.123240
Train Epoch: 1 [257024/2383431 (10.8%)]	Loss: 0.126748
Train Epoch: 1 [308224/2383431 (12.9%)]	Loss: 0.122661
Train Epoch: 1 [359424/2383431 (15.1%)]	Loss: 0.120350
Train Epoch: 1 [410624/2383431 (17.2%)]	Loss: 0.112320
Train Epoch: 1 [461824/2383431 (19.4%)]	Loss: 0.102145
Train Epoch: 1 [513024/2383431 (21.5%)]	Loss: 0.110243
Train Epoch: 1 [564224/2383431 (23.7%)]	Loss: 0.112711
Train Epoch: 1 [615424/2383431 (25.8%)]	Loss: 0.122422
Train Epoch: 1 [666624/2383431 (28.0%)]	Loss: 0.127649
Train Epoch: 1 [717824/2383431 (30.1%)]	Loss: 0.119726
Train Epoch: 1 [769024/2383431 (32.3%)]	Loss: 0.114856
Train Epoch: 1 [820224/2383431 (34.4%)]	Loss: 0.106439
Train Epoch: 1 [871424/2383431 (36.6%)]	Loss: 0.120687
Train Epoch: 1 [922624/2383431 (38.7%)]	Loss: 0.123605
Train Epoch: 1 [973824/2383431 (40.9%)]	Loss: 0.114446
Train Epoch: 1 [1025024/2383431 (43.0%)]	Loss: 0.120748
Train Epoch: 1 [1076224/2383431 (45.2%)]	Loss: 0.119937
Train Epoch: 1 [1127424/2383431 (47.3%)]	Loss: 0.112553
Train Epoch: 1 [1178624/2383431 (49.5%)]	Loss: 0.126488
Train Epoch: 1 [1229824/2383431 (51.6%)]	Loss: 0.110414
Train Epoch: 1 [1281024/2383431 (53.7%)]	Loss: 0.118723
Train Epoch: 1 [1332224/2383431 (55.9%)]	Loss: 0.129721
Train Epoch: 1 [1383424/2383431 (58.0%)]	Loss: 0.107289
Train Epoch: 1 [1434624/2383431 (60.2%)]	Loss: 0.108486
Train Epoch: 1 [1485824/2383431 (62.3%)]	Loss: 0.109479
Train Epoch: 1 [1537024/2383431 (64.5%)]	Loss: 0.116268
Train Epoch: 1 [1588224/2383431 (66.6%)]	Loss: 0.131677
Train Epoch: 1 [1639424/2383431 (68.8%)]	Loss: 0.113521
Train Epoch: 1 [1690624/2383431 (70.9%)]	Loss: 0.113858
Train Epoch: 1 [1741824/2383431 (73.1%)]	Loss: 0.116221
Train Epoch: 1 [1793024/2383431 (75.2%)]	Loss: 0.114606
Train Epoch: 1 [1844224/2383431 (77.4%)]	Loss: 0.108157
Train Epoch: 1 [1895424/2383431 (79.5%)]	Loss: 0.124100
Train Epoch: 1 [1946624/2383431 (81.7%)]	Loss: 0.110295
Train Epoch: 1 [1997824/2383431 (83.8%)]	Loss: 0.099857
Train Epoch: 1 [2049024/2383431 (86.0%)]	Loss: 0.112681
Train Epoch: 1 [2100224/2383431 (88.1%)]	Loss: 0.102203
Train Epoch: 1 [2151424/2383431 (90.3%)]	Loss: 0.113515
Train Epoch: 1 [2202624/2383431 (92.4%)]	Loss: 0.125014
Train Epoch: 1 [2253824/2383431 (94.6%)]	Loss: 0.101680
Train Epoch: 1 [2305024/2383431 (96.7%)]	Loss: 0.128284
Train Epoch: 1 [2356224/2383431 (98.9%)]	Loss: 0.110097
Train Epoch: 2 [1024/2383431 (0.0%)]	Loss: 0.125910
Train Epoch: 2 [52224/2383431 (2.2%)]	Loss: 0.105344
Train Epoch: 2 [103424/2383431 (4.3%)]	Loss: 0.121107
Train Epoch: 2 [154624/2383431 (6.5%)]	Loss: 0.106508
Train Epoch: 2 [205824/2383431 (8.6%)]	Loss: 0.118035
Train Epoch: 2 [257024/2383431 (10.8%)]	Loss: 0.113430
Train Epoch: 2 [308224/2383431 (12.9%)]	Loss: 0.113761
Train Epoch: 2 [359424/2383431 (15.1%)]	Loss: 0.121465
Train Epoch: 2 [410624/2383431 (17.2%)]	Loss: 0.108154
Train Epoch: 2 [461824/2383431 (19.4%)]	Loss: 0.137940
Train Epoch: 2 [513024/2383431 (21.5%)]	Loss: 0.112175
Train Epoch: 2 [564224/2383431 (23.7%)]	Loss: 0.097803
Train Epoch: 2 [615424/2383431 (25.8%)]	Loss: 0.106466
Train Epoch: 2 [666624/2383431 (28.0%)]	Loss: 0.117024
Train Epoch: 2 [717824/2383431 (30.1%)]	Loss: 0.106139
Train Epoch: 2 [769024/2383431 (32.3%)]	Loss: 0.102130
Train Epoch: 2 [820224/2383431 (34.4%)]	Loss: 0.090591
Train Epoch: 2 [871424/2383431 (36.6%)]	Loss: 0.113435
Train Epoch: 2 [922624/2383431 (38.7%)]	Loss: 0.108950
Train Epoch: 2 [973824/2383431 (40.9%)]	Loss: 0.103801
Train Epoch: 2 [1025024/2383431 (43.0%)]	Loss: 0.119077
Train Epoch: 2 [1076224/2383431 (45.2%)]	Loss: 0.122515
Train Epoch: 2 [1127424/2383431 (47.3%)]	Loss: 0.098417
Train Epoch: 2 [1178624/2383431 (49.5%)]	Loss: 0.099775
Train Epoch: 2 [1229824/2383431 (51.6%)]	Loss: 0.103141
Train Epoch: 2 [1281024/2383431 (53.7%)]	Loss: 0.105217
Train Epoch: 2 [1332224/2383431 (55.9%)]	Loss: 0.120021
Train Epoch: 2 [1383424/2383431 (58.0%)]	Loss: 0.105552
Train Epoch: 2 [1434624/2383431 (60.2%)]	Loss: 0.115412
Train Epoch: 2 [1485824/2383431 (62.3%)]	Loss: 0.114384
Train Epoch: 2 [1537024/2383431 (64.5%)]	Loss: 0.113104
Train Epoch: 2 [1588224/2383431 (66.6%)]	Loss: 0.106306
Train Epoch: 2 [1639424/2383431 (68.8%)]	Loss: 0.107992
Train Epoch: 2 [1690624/2383431 (70.9%)]	Loss: 0.093180
Train Epoch: 2 [1741824/2383431 (73.1%)]	Loss: 0.108837
Train Epoch: 2 [1793024/2383431 (75.2%)]	Loss: 0.105794
Train Epoch: 2 [1844224/2383431 (77.4%)]	Loss: 0.129353
Train Epoch: 2 [1895424/2383431 (79.5%)]	Loss: 0.097336
Train Epoch: 2 [1946624/2383431 (81.7%)]	Loss: 0.103017
Train Epoch: 2 [1997824/2383431 (83.8%)]	Loss: 0.108514
Train Epoch: 2 [2049024/2383431 (86.0%)]	Loss: 0.088117
Train Epoch: 2 [2100224/2383431 (88.1%)]	Loss: 0.111089
Train Epoch: 2 [2151424/2383431 (90.3%)]	Loss: 0.114482
Train Epoch: 2 [2202624/2383431 (92.4%)]	Loss: 0.122499
Train Epoch: 2 [2253824/2383431 (94.6%)]	Loss: 0.114130
Train Epoch: 2 [2305024/2383431 (96.7%)]	Loss: 0.101249
Train Epoch: 2 [2356224/2383431 (98.9%)]	Loss: 0.103413

ACC in fold#4 was 0.861


Balanced ACC in fold#4 was 0.865


MCC in fold#4 was 0.719


Confusion Matrix in fold#4: 
           nonRipple  Ripple
nonRipple     208283   28552
Ripple         53978  305044


Classification Report in fold#4: 
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.794       0.914  ...       0.854         0.867
recall            0.879       0.850  ...       0.865         0.861
f1-score          0.835       0.881  ...       0.858         0.862
sample size  236835.000  359022.000  ...  595857.000    595857.000

[4 rows x 5 columns]


Label Errors Rate:
0.030


 --- 5-fold CV overall metrics --- 


The Mattews correlation coefficient: 0.783 +/- 0.035 (mean +/- std.; n=5)


Balanced Accuracy Score: 0.893 +/- 0.015 (mean +/- std.; n=5)


Confusion Matrix (Test; sum; num. folds=5)
           nonRipple   Ripple
nonRipple    1052695   131484
Ripple        184055  1611054


Classification Report (Test; mean; num. folds=5)
              nonRipple      Ripple  ...   macro avg  weighted avg
precision         0.854       0.925  ...       0.890         0.897
recall            0.889       0.897  ...       0.893         0.894
f1-score          0.870       0.910  ...       0.890         0.894
sample size  236835.800  359021.800  ...  595857.600    595857.600

[4 rows x 5 columns]


Classification Report (Test; std; num. folds=5)
             nonRipple  Ripple  balanced accuracy  macro avg  weighted avg
precision        0.046   0.017              0.015      0.020         0.016
recall           0.030   0.038              0.015      0.015         0.018
f1-score         0.019   0.017              0.015      0.018         0.018
sample size      0.400   0.400              0.015      0.490         0.490


ROC AUC micro Score: 0.965 +/- 0.008 (mean +/- std.; n=5)


ROC AUC macro Score: 0.963 +/- 0.007 (mean +/- std.; n=5)


Precision-Recall AUC micro Score: 0.965 +/- 0.007 (mean +/- std.; n=5)


Precision-Recall AUC macro Score: 0.962 +/- 0.007 (mean +/- std.; n=5)


Saved to: ./data/okada/cleanlab_results/D03+/are_errors.npy


Saved to: ./data/okada/cleanlab_results/D03+/are_ripple_GMM.npy


Saved to: ./data/okada/cleanlab_results/D03+/psx_ripple.npy


Saved to: ./data/okada/cleanlab_results/D03+/mccs.csv


Saved to: ./data/okada/cleanlab_results/D03+/balanced_accs.csv


Saved to: ./data/okada/cleanlab_results/D03+/conf_mat/fold#0.png


Saved to: ./data/okada/cleanlab_results/D03+/conf_mat/fold#1.png


Saved to: ./data/okada/cleanlab_results/D03+/conf_mat/fold#2.png


Saved to: ./data/okada/cleanlab_results/D03+/conf_mat/fold#3.png


Saved to: ./data/okada/cleanlab_results/D03+/conf_mat/fold#4.png


Saved to: ./data/okada/cleanlab_results/D03+/conf_mat/conf_mats.csv


Saved to: ./data/okada/cleanlab_results/D03+/conf_mat/overall_sum.png


Saved to: ./data/okada/cleanlab_results/D03+/clf_reports.csv


Saved to: ./data/okada/cleanlab_results/D03+/aucs.csv


Saved to: ./data/okada/cleanlab_results/D03+/roc_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D03+/roc_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D03+/roc_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D03+/roc_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D03+/roc_curves/fold#4.png


Saved to: ./data/okada/cleanlab_results/D03+/pr_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D03+/pr_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D03+/pr_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D03+/pr_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D03+/pr_curves/fold#4.png


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D03+/tt7-4_fp16.pkl

