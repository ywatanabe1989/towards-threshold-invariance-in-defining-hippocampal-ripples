
Random seeds have been fixed as 42


Random seeds have been fixed as 42


Standard Output/Error are going to be logged in the followings: 
  - ./ripples/define_ripples/using_CNN/makes_labels/log/stdout.log
  - ./ripples/define_ripples/using_CNN/makes_labels/log/stderr.log


Random seeds have been fixed as 42

D01-
['./data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy']

2021-0711-1211

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/10786048 (0.0%)]	Loss: 0.360794
Train Epoch: 1 [52224/10786048 (0.5%)]	Loss: 0.135961
Train Epoch: 1 [103424/10786048 (1.0%)]	Loss: 0.108747
Train Epoch: 1 [154624/10786048 (1.4%)]	Loss: 0.123849
Train Epoch: 1 [205824/10786048 (1.9%)]	Loss: 0.104013
Train Epoch: 1 [257024/10786048 (2.4%)]	Loss: 0.097451
Train Epoch: 1 [308224/10786048 (2.9%)]	Loss: 0.091851
Train Epoch: 1 [359424/10786048 (3.3%)]	Loss: 0.111766
Train Epoch: 1 [410624/10786048 (3.8%)]	Loss: 0.097852
Train Epoch: 1 [461824/10786048 (4.3%)]	Loss: 0.091266
Train Epoch: 1 [513024/10786048 (4.8%)]	Loss: 0.120329
Train Epoch: 1 [564224/10786048 (5.2%)]	Loss: 0.100080
Train Epoch: 1 [615424/10786048 (5.7%)]	Loss: 0.101785
Train Epoch: 1 [666624/10786048 (6.2%)]	Loss: 0.082835
Train Epoch: 1 [717824/10786048 (6.7%)]	Loss: 0.091156
Train Epoch: 1 [769024/10786048 (7.1%)]	Loss: 0.110655
Train Epoch: 1 [820224/10786048 (7.6%)]	Loss: 0.099998
Train Epoch: 1 [871424/10786048 (8.1%)]	Loss: 0.089391
Train Epoch: 1 [922624/10786048 (8.6%)]	Loss: 0.096920
Train Epoch: 1 [973824/10786048 (9.0%)]	Loss: 0.105415
Train Epoch: 1 [1025024/10786048 (9.5%)]	Loss: 0.107616
Train Epoch: 1 [1076224/10786048 (10.0%)]	Loss: 0.091177
Train Epoch: 1 [1127424/10786048 (10.5%)]	Loss: 0.109943
Train Epoch: 1 [1178624/10786048 (10.9%)]	Loss: 0.106600
Train Epoch: 1 [1229824/10786048 (11.4%)]	Loss: 0.106294
Train Epoch: 1 [1281024/10786048 (11.9%)]	Loss: 0.088302
Train Epoch: 1 [1332224/10786048 (12.4%)]	Loss: 0.106181
Train Epoch: 1 [1383424/10786048 (12.8%)]	Loss: 0.102599
Train Epoch: 1 [1434624/10786048 (13.3%)]	Loss: 0.101368
Train Epoch: 1 [1485824/10786048 (13.8%)]	Loss: 0.095965
Train Epoch: 1 [1537024/10786048 (14.3%)]	Loss: 0.083990
Train Epoch: 1 [1588224/10786048 (14.7%)]	Loss: 0.095701
Train Epoch: 1 [1639424/10786048 (15.2%)]	Loss: 0.113063
Train Epoch: 1 [1690624/10786048 (15.7%)]	Loss: 0.097513
Train Epoch: 1 [1741824/10786048 (16.1%)]	Loss: 0.099007
Train Epoch: 1 [1793024/10786048 (16.6%)]	Loss: 0.090096
Train Epoch: 1 [1844224/10786048 (17.1%)]	Loss: 0.106374
Train Epoch: 1 [1895424/10786048 (17.6%)]	Loss: 0.104971
Train Epoch: 1 [1946624/10786048 (18.0%)]	Loss: 0.102848
Train Epoch: 1 [1997824/10786048 (18.5%)]	Loss: 0.101489
Train Epoch: 1 [2049024/10786048 (19.0%)]	Loss: 0.107110
Train Epoch: 1 [2100224/10786048 (19.5%)]	Loss: 0.106049
Train Epoch: 1 [2151424/10786048 (19.9%)]	Loss: 0.097348
Train Epoch: 1 [2202624/10786048 (20.4%)]	Loss: 0.103001
Train Epoch: 1 [2253824/10786048 (20.9%)]	Loss: 0.110804
Train Epoch: 1 [2305024/10786048 (21.4%)]	Loss: 0.107651
Train Epoch: 1 [2356224/10786048 (21.8%)]	Loss: 0.093217
Train Epoch: 1 [2407424/10786048 (22.3%)]	Loss: 0.107217
Train Epoch: 1 [2458624/10786048 (22.8%)]	Loss: 0.092626
Train Epoch: 1 [2509824/10786048 (23.3%)]	Loss: 0.086027
Train Epoch: 1 [2561024/10786048 (23.7%)]	Loss: 0.098363
Train Epoch: 1 [2612224/10786048 (24.2%)]	Loss: 0.105801
Train Epoch: 1 [2663424/10786048 (24.7%)]	Loss: 0.104363
Train Epoch: 1 [2714624/10786048 (25.2%)]	Loss: 0.104955
Train Epoch: 1 [2765824/10786048 (25.6%)]	Loss: 0.091473
Train Epoch: 1 [2817024/10786048 (26.1%)]	Loss: 0.101111
Train Epoch: 1 [2868224/10786048 (26.6%)]	Loss: 0.095669
Train Epoch: 1 [2919424/10786048 (27.1%)]	Loss: 0.084487
Train Epoch: 1 [2970624/10786048 (27.5%)]	Loss: 0.093501
Train Epoch: 1 [3021824/10786048 (28.0%)]	Loss: 0.102490
Train Epoch: 1 [3073024/10786048 (28.5%)]	Loss: 0.099992
Train Epoch: 1 [3124224/10786048 (29.0%)]	Loss: 0.104201
Train Epoch: 1 [3175424/10786048 (29.4%)]	Loss: 0.106054
Train Epoch: 1 [3226624/10786048 (29.9%)]	Loss: 0.103024
Train Epoch: 1 [3277824/10786048 (30.4%)]	Loss: 0.101761
Train Epoch: 1 [3329024/10786048 (30.9%)]	Loss: 0.099996
Train Epoch: 1 [3380224/10786048 (31.3%)]	Loss: 0.100463
Train Epoch: 1 [3431424/10786048 (31.8%)]	Loss: 0.091113
Train Epoch: 1 [3482624/10786048 (32.3%)]	Loss: 0.109431
Train Epoch: 1 [3533824/10786048 (32.8%)]	Loss: 0.085617
Train Epoch: 1 [3585024/10786048 (33.2%)]	Loss: 0.088011
Train Epoch: 1 [3636224/10786048 (33.7%)]	Loss: 0.082876
Train Epoch: 1 [3687424/10786048 (34.2%)]	Loss: 0.098768
Train Epoch: 1 [3738624/10786048 (34.7%)]	Loss: 0.094555
Train Epoch: 1 [3789824/10786048 (35.1%)]	Loss: 0.091105
Train Epoch: 1 [3841024/10786048 (35.6%)]	Loss: 0.086197
Train Epoch: 1 [3892224/10786048 (36.1%)]	Loss: 0.111639
Train Epoch: 1 [3943424/10786048 (36.6%)]	Loss: 0.088284
Train Epoch: 1 [3994624/10786048 (37.0%)]	Loss: 0.114037
Train Epoch: 1 [4045824/10786048 (37.5%)]	Loss: 0.083879
Train Epoch: 1 [4097024/10786048 (38.0%)]	Loss: 0.093803
Train Epoch: 1 [4148224/10786048 (38.5%)]	Loss: 0.104318
Train Epoch: 1 [4199424/10786048 (38.9%)]	Loss: 0.112028
Train Epoch: 1 [4250624/10786048 (39.4%)]	Loss: 0.093621
Train Epoch: 1 [4301824/10786048 (39.9%)]	Loss: 0.093732
Train Epoch: 1 [4353024/10786048 (40.4%)]	Loss: 0.105195
Train Epoch: 1 [4404224/10786048 (40.8%)]	Loss: 0.094495
Train Epoch: 1 [4455424/10786048 (41.3%)]	Loss: 0.104692
Train Epoch: 1 [4506624/10786048 (41.8%)]	Loss: 0.086685
Train Epoch: 1 [4557824/10786048 (42.3%)]	Loss: 0.100948
Train Epoch: 1 [4609024/10786048 (42.7%)]	Loss: 0.102672
Train Epoch: 1 [4660224/10786048 (43.2%)]	Loss: 0.087489
Train Epoch: 1 [4711424/10786048 (43.7%)]	Loss: 0.092622
Train Epoch: 1 [4762624/10786048 (44.2%)]	Loss: 0.100467
Train Epoch: 1 [4813824/10786048 (44.6%)]	Loss: 0.084052
Train Epoch: 1 [4865024/10786048 (45.1%)]	Loss: 0.089157
Train Epoch: 1 [4916224/10786048 (45.6%)]	Loss: 0.084293
Train Epoch: 1 [4967424/10786048 (46.1%)]	Loss: 0.097578
Train Epoch: 1 [5018624/10786048 (46.5%)]	Loss: 0.098886
Train Epoch: 1 [5069824/10786048 (47.0%)]	Loss: 0.091377
Train Epoch: 1 [5121024/10786048 (47.5%)]	Loss: 0.095929
Train Epoch: 1 [5172224/10786048 (48.0%)]	Loss: 0.106756
Train Epoch: 1 [5223424/10786048 (48.4%)]	Loss: 0.093333
Train Epoch: 1 [5274624/10786048 (48.9%)]	Loss: 0.093508
Train Epoch: 1 [5325824/10786048 (49.4%)]	Loss: 0.091700
Train Epoch: 1 [5377024/10786048 (49.9%)]	Loss: 0.107022
Train Epoch: 1 [5428224/10786048 (50.3%)]	Loss: 0.099923
Train Epoch: 1 [5479424/10786048 (50.8%)]	Loss: 0.080633
Train Epoch: 1 [5530624/10786048 (51.3%)]	Loss: 0.082239
Train Epoch: 1 [5581824/10786048 (51.8%)]	Loss: 0.087607
Train Epoch: 1 [5633024/10786048 (52.2%)]	Loss: 0.109256
Train Epoch: 1 [5684224/10786048 (52.7%)]	Loss: 0.092580
Train Epoch: 1 [5735424/10786048 (53.2%)]	Loss: 0.100666
Train Epoch: 1 [5786624/10786048 (53.6%)]	Loss: 0.102047
Train Epoch: 1 [5837824/10786048 (54.1%)]	Loss: 0.096454
Train Epoch: 1 [5889024/10786048 (54.6%)]	Loss: 0.093262
Train Epoch: 1 [5940224/10786048 (55.1%)]	Loss: 0.098477
Train Epoch: 1 [5991424/10786048 (55.5%)]	Loss: 0.095178
Train Epoch: 1 [6042624/10786048 (56.0%)]	Loss: 0.089149
Train Epoch: 1 [6093824/10786048 (56.5%)]	Loss: 0.100085
Train Epoch: 1 [6145024/10786048 (57.0%)]	Loss: 0.092936
Train Epoch: 1 [6196224/10786048 (57.4%)]	Loss: 0.093535
Train Epoch: 1 [6247424/10786048 (57.9%)]	Loss: 0.099610
Train Epoch: 1 [6298624/10786048 (58.4%)]	Loss: 0.094755
Train Epoch: 1 [6349824/10786048 (58.9%)]	Loss: 0.097610
Train Epoch: 1 [6401024/10786048 (59.3%)]	Loss: 0.093623
Train Epoch: 1 [6452224/10786048 (59.8%)]	Loss: 0.092435
Train Epoch: 1 [6503424/10786048 (60.3%)]	Loss: 0.096701
Train Epoch: 1 [6554624/10786048 (60.8%)]	Loss: 0.092753
Train Epoch: 1 [6605824/10786048 (61.2%)]	Loss: 0.099295
Train Epoch: 1 [6657024/10786048 (61.7%)]	Loss: 0.086461
Train Epoch: 1 [6708224/10786048 (62.2%)]	Loss: 0.094403
Train Epoch: 1 [6759424/10786048 (62.7%)]	Loss: 0.107339
Train Epoch: 1 [6810624/10786048 (63.1%)]	Loss: 0.101169
Train Epoch: 1 [6861824/10786048 (63.6%)]	Loss: 0.101843
Train Epoch: 1 [6913024/10786048 (64.1%)]	Loss: 0.091284
Train Epoch: 1 [6964224/10786048 (64.6%)]	Loss: 0.089911
Train Epoch: 1 [7015424/10786048 (65.0%)]	Loss: 0.084476
Train Epoch: 1 [7066624/10786048 (65.5%)]	Loss: 0.086625
Train Epoch: 1 [7117824/10786048 (66.0%)]	Loss: 0.083388
Train Epoch: 1 [7169024/10786048 (66.5%)]	Loss: 0.089528
Train Epoch: 1 [7220224/10786048 (66.9%)]	Loss: 0.099837
Train Epoch: 1 [7271424/10786048 (67.4%)]	Loss: 0.100875
Train Epoch: 1 [7322624/10786048 (67.9%)]	Loss: 0.093089
Train Epoch: 1 [7373824/10786048 (68.4%)]	Loss: 0.101599
Train Epoch: 1 [7425024/10786048 (68.8%)]	Loss: 0.103162
Train Epoch: 1 [7476224/10786048 (69.3%)]	Loss: 0.087422
Train Epoch: 1 [7527424/10786048 (69.8%)]	Loss: 0.102549
Train Epoch: 1 [7578624/10786048 (70.3%)]	Loss: 0.094290
Train Epoch: 1 [7629824/10786048 (70.7%)]	Loss: 0.076376
Train Epoch: 1 [7681024/10786048 (71.2%)]	Loss: 0.099523
Train Epoch: 1 [7732224/10786048 (71.7%)]	Loss: 0.094292
Train Epoch: 1 [7783424/10786048 (72.2%)]	Loss: 0.095435
Train Epoch: 1 [7834624/10786048 (72.6%)]	Loss: 0.094918
Train Epoch: 1 [7885824/10786048 (73.1%)]	Loss: 0.103510
Train Epoch: 1 [7937024/10786048 (73.6%)]	Loss: 0.092754
Train Epoch: 1 [7988224/10786048 (74.1%)]	Loss: 0.096413
Train Epoch: 1 [8039424/10786048 (74.5%)]	Loss: 0.090883
Train Epoch: 1 [8090624/10786048 (75.0%)]	Loss: 0.079252
Train Epoch: 1 [8141824/10786048 (75.5%)]	Loss: 0.090151
Train Epoch: 1 [8193024/10786048 (76.0%)]	Loss: 0.089916
Train Epoch: 1 [8244224/10786048 (76.4%)]	Loss: 0.089320
Train Epoch: 1 [8295424/10786048 (76.9%)]	Loss: 0.096227
Train Epoch: 1 [8346624/10786048 (77.4%)]	Loss: 0.085533
Train Epoch: 1 [8397824/10786048 (77.9%)]	Loss: 0.109295
Train Epoch: 1 [8449024/10786048 (78.3%)]	Loss: 0.083771
Train Epoch: 1 [8500224/10786048 (78.8%)]	Loss: 0.103811
Train Epoch: 1 [8551424/10786048 (79.3%)]	Loss: 0.089060
Train Epoch: 1 [8602624/10786048 (79.8%)]	Loss: 0.085352
Train Epoch: 1 [8653824/10786048 (80.2%)]	Loss: 0.087092
Train Epoch: 1 [8705024/10786048 (80.7%)]	Loss: 0.097211
Train Epoch: 1 [8756224/10786048 (81.2%)]	Loss: 0.097076
Train Epoch: 1 [8807424/10786048 (81.7%)]	Loss: 0.111831
Train Epoch: 1 [8858624/10786048 (82.1%)]	Loss: 0.095240
Train Epoch: 1 [8909824/10786048 (82.6%)]	Loss: 0.093332
Train Epoch: 1 [8961024/10786048 (83.1%)]	Loss: 0.105066
Train Epoch: 1 [9012224/10786048 (83.6%)]	Loss: 0.088752
Train Epoch: 1 [9063424/10786048 (84.0%)]	Loss: 0.097803
Train Epoch: 1 [9114624/10786048 (84.5%)]	Loss: 0.097118
Train Epoch: 1 [9165824/10786048 (85.0%)]	Loss: 0.097787
Train Epoch: 1 [9217024/10786048 (85.5%)]	Loss: 0.097348
Train Epoch: 1 [9268224/10786048 (85.9%)]	Loss: 0.093481
Train Epoch: 1 [9319424/10786048 (86.4%)]	Loss: 0.087709
Train Epoch: 1 [9370624/10786048 (86.9%)]	Loss: 0.097677
Train Epoch: 1 [9421824/10786048 (87.4%)]	Loss: 0.099502
Train Epoch: 1 [9473024/10786048 (87.8%)]	Loss: 0.103064
Train Epoch: 1 [9524224/10786048 (88.3%)]	Loss: 0.092462
Train Epoch: 1 [9575424/10786048 (88.8%)]	Loss: 0.093309
Train Epoch: 1 [9626624/10786048 (89.3%)]	Loss: 0.092165
Train Epoch: 1 [9677824/10786048 (89.7%)]	Loss: 0.101662
Train Epoch: 1 [9729024/10786048 (90.2%)]	Loss: 0.088452
Train Epoch: 1 [9780224/10786048 (90.7%)]	Loss: 0.089088
Train Epoch: 1 [9831424/10786048 (91.1%)]	Loss: 0.088314
Train Epoch: 1 [9882624/10786048 (91.6%)]	Loss: 0.086706
Train Epoch: 1 [9933824/10786048 (92.1%)]	Loss: 0.105050
Train Epoch: 1 [9985024/10786048 (92.6%)]	Loss: 0.088472
Train Epoch: 1 [10036224/10786048 (93.0%)]	Loss: 0.072836
Train Epoch: 1 [10087424/10786048 (93.5%)]	Loss: 0.077513
Train Epoch: 1 [10138624/10786048 (94.0%)]	Loss: 0.082141
Train Epoch: 1 [10189824/10786048 (94.5%)]	Loss: 0.101376
Train Epoch: 1 [10241024/10786048 (94.9%)]	Loss: 0.093659
Train Epoch: 1 [10292224/10786048 (95.4%)]	Loss: 0.099003
Train Epoch: 1 [10343424/10786048 (95.9%)]	Loss: 0.079025
Train Epoch: 1 [10394624/10786048 (96.4%)]	Loss: 0.101092
Train Epoch: 1 [10445824/10786048 (96.8%)]	Loss: 0.086178
Train Epoch: 1 [10497024/10786048 (97.3%)]	Loss: 0.081462
Train Epoch: 1 [10548224/10786048 (97.8%)]	Loss: 0.101794
Train Epoch: 1 [10599424/10786048 (98.3%)]	Loss: 0.097151
Train Epoch: 1 [10650624/10786048 (98.7%)]	Loss: 0.097310
Train Epoch: 1 [10701824/10786048 (99.2%)]	Loss: 0.095547
Train Epoch: 1 [10753024/10786048 (99.7%)]	Loss: 0.081462
Train Epoch: 2 [1024/10786048 (0.0%)]	Loss: 0.107926
Train Epoch: 2 [52224/10786048 (0.5%)]	Loss: 0.089266
Train Epoch: 2 [103424/10786048 (1.0%)]	Loss: 0.091713
Train Epoch: 2 [154624/10786048 (1.4%)]	Loss: 0.086620
Train Epoch: 2 [205824/10786048 (1.9%)]	Loss: 0.103182
Train Epoch: 2 [257024/10786048 (2.4%)]	Loss: 0.097750
Train Epoch: 2 [308224/10786048 (2.9%)]	Loss: 0.089005
Train Epoch: 2 [359424/10786048 (3.3%)]	Loss: 0.107548
Train Epoch: 2 [410624/10786048 (3.8%)]	Loss: 0.115901
Train Epoch: 2 [461824/10786048 (4.3%)]	Loss: 0.084094
Train Epoch: 2 [513024/10786048 (4.8%)]	Loss: 0.084983
Train Epoch: 2 [564224/10786048 (5.2%)]	Loss: 0.093748
Train Epoch: 2 [615424/10786048 (5.7%)]	Loss: 0.109842
Train Epoch: 2 [666624/10786048 (6.2%)]	Loss: 0.098491
Train Epoch: 2 [717824/10786048 (6.7%)]	Loss: 0.098326
Train Epoch: 2 [769024/10786048 (7.1%)]	Loss: 0.106450
Train Epoch: 2 [820224/10786048 (7.6%)]	Loss: 0.101432
Train Epoch: 2 [871424/10786048 (8.1%)]	Loss: 0.095172
Train Epoch: 2 [922624/10786048 (8.6%)]	Loss: 0.104815
Train Epoch: 2 [973824/10786048 (9.0%)]	Loss: 0.099711
Train Epoch: 2 [1025024/10786048 (9.5%)]	Loss: 0.097133
Train Epoch: 2 [1076224/10786048 (10.0%)]	Loss: 0.085299
Train Epoch: 2 [1127424/10786048 (10.5%)]	Loss: 0.099448
Train Epoch: 2 [1178624/10786048 (10.9%)]	Loss: 0.080636
Train Epoch: 2 [1229824/10786048 (11.4%)]	Loss: 0.091631
Train Epoch: 2 [1281024/10786048 (11.9%)]	Loss: 0.101599
Train Epoch: 2 [1332224/10786048 (12.4%)]	Loss: 0.108587
Train Epoch: 2 [1383424/10786048 (12.8%)]	Loss: 0.090495
Train Epoch: 2 [1434624/10786048 (13.3%)]	Loss: 0.080444
Train Epoch: 2 [1485824/10786048 (13.8%)]	Loss: 0.100181
Train Epoch: 2 [1537024/10786048 (14.3%)]	Loss: 0.102273
Train Epoch: 2 [1588224/10786048 (14.7%)]	Loss: 0.090054
Train Epoch: 2 [1639424/10786048 (15.2%)]	Loss: 0.093504
Train Epoch: 2 [1690624/10786048 (15.7%)]	Loss: 0.092088
Train Epoch: 2 [1741824/10786048 (16.1%)]	Loss: 0.096577
Train Epoch: 2 [1793024/10786048 (16.6%)]	Loss: 0.085214
Train Epoch: 2 [1844224/10786048 (17.1%)]	Loss: 0.089065
Train Epoch: 2 [1895424/10786048 (17.6%)]	Loss: 0.097153
Train Epoch: 2 [1946624/10786048 (18.0%)]	Loss: 0.097700
Train Epoch: 2 [1997824/10786048 (18.5%)]	Loss: 0.097634
Train Epoch: 2 [2049024/10786048 (19.0%)]	Loss: 0.099378
Train Epoch: 2 [2100224/10786048 (19.5%)]	Loss: 0.090463
Train Epoch: 2 [2151424/10786048 (19.9%)]	Loss: 0.104805
Train Epoch: 2 [2202624/10786048 (20.4%)]	Loss: 0.080657
Train Epoch: 2 [2253824/10786048 (20.9%)]	Loss: 0.096107
Train Epoch: 2 [2305024/10786048 (21.4%)]	Loss: 0.076120
Train Epoch: 2 [2356224/10786048 (21.8%)]	Loss: 0.090952
Train Epoch: 2 [2407424/10786048 (22.3%)]	Loss: 0.102160
Train Epoch: 2 [2458624/10786048 (22.8%)]	Loss: 0.091906
Train Epoch: 2 [2509824/10786048 (23.3%)]	Loss: 0.104640
Train Epoch: 2 [2561024/10786048 (23.7%)]	Loss: 0.088932
Train Epoch: 2 [2612224/10786048 (24.2%)]	Loss: 0.109228
Train Epoch: 2 [2663424/10786048 (24.7%)]	Loss: 0.082777
Train Epoch: 2 [2714624/10786048 (25.2%)]	Loss: 0.086679
Train Epoch: 2 [2765824/10786048 (25.6%)]	Loss: 0.089078
Train Epoch: 2 [2817024/10786048 (26.1%)]	Loss: 0.106234
Train Epoch: 2 [2868224/10786048 (26.6%)]	Loss: 0.090094
Train Epoch: 2 [2919424/10786048 (27.1%)]	Loss: 0.095312
Train Epoch: 2 [2970624/10786048 (27.5%)]	Loss: 0.083133
Train Epoch: 2 [3021824/10786048 (28.0%)]	Loss: 0.095263
Train Epoch: 2 [3073024/10786048 (28.5%)]	Loss: 0.084909
Train Epoch: 2 [3124224/10786048 (29.0%)]	Loss: 0.084634
Train Epoch: 2 [3175424/10786048 (29.4%)]	Loss: 0.100276
Train Epoch: 2 [3226624/10786048 (29.9%)]	Loss: 0.098526
Train Epoch: 2 [3277824/10786048 (30.4%)]	Loss: 0.102432
Train Epoch: 2 [3329024/10786048 (30.9%)]	Loss: 0.089490
Train Epoch: 2 [3380224/10786048 (31.3%)]	Loss: 0.079200
Train Epoch: 2 [3431424/10786048 (31.8%)]	Loss: 0.074711
Train Epoch: 2 [3482624/10786048 (32.3%)]	Loss: 0.081910
Train Epoch: 2 [3533824/10786048 (32.8%)]	Loss: 0.087737
Train Epoch: 2 [3585024/10786048 (33.2%)]	Loss: 0.097970
Train Epoch: 2 [3636224/10786048 (33.7%)]	Loss: 0.090655
Train Epoch: 2 [3687424/10786048 (34.2%)]	Loss: 0.081818
Train Epoch: 2 [3738624/10786048 (34.7%)]	Loss: 0.103684
Train Epoch: 2 [3789824/10786048 (35.1%)]	Loss: 0.089133
Train Epoch: 2 [3841024/10786048 (35.6%)]	Loss: 0.075355
Train Epoch: 2 [3892224/10786048 (36.1%)]	Loss: 0.083505
Train Epoch: 2 [3943424/10786048 (36.6%)]	Loss: 0.086133
Train Epoch: 2 [3994624/10786048 (37.0%)]	Loss: 0.091748
Train Epoch: 2 [4045824/10786048 (37.5%)]	Loss: 0.093057
Train Epoch: 2 [4097024/10786048 (38.0%)]	Loss: 0.072753
Train Epoch: 2 [4148224/10786048 (38.5%)]	Loss: 0.088080
Train Epoch: 2 [4199424/10786048 (38.9%)]	Loss: 0.075118
Train Epoch: 2 [4250624/10786048 (39.4%)]	Loss: 0.088856
Train Epoch: 2 [4301824/10786048 (39.9%)]	Loss: 0.091880
Train Epoch: 2 [4353024/10786048 (40.4%)]	Loss: 0.082595
Train Epoch: 2 [4404224/10786048 (40.8%)]	Loss: 0.081719
Train Epoch: 2 [4455424/10786048 (41.3%)]	Loss: 0.089091
Train Epoch: 2 [4506624/10786048 (41.8%)]	Loss: 0.100263
Train Epoch: 2 [4557824/10786048 (42.3%)]	Loss: 0.091733
Train Epoch: 2 [4609024/10786048 (42.7%)]	Loss: 0.115509
Train Epoch: 2 [4660224/10786048 (43.2%)]	Loss: 0.091363
Train Epoch: 2 [4711424/10786048 (43.7%)]	Loss: 0.099307
Train Epoch: 2 [4762624/10786048 (44.2%)]	Loss: 0.101577
Train Epoch: 2 [4813824/10786048 (44.6%)]	Loss: 0.082716
Train Epoch: 2 [4865024/10786048 (45.1%)]	Loss: 0.097321
Train Epoch: 2 [4916224/10786048 (45.6%)]	Loss: 0.085197
Train Epoch: 2 [4967424/10786048 (46.1%)]	Loss: 0.084519
Train Epoch: 2 [5018624/10786048 (46.5%)]	Loss: 0.099958
Train Epoch: 2 [5069824/10786048 (47.0%)]	Loss: 0.096916
Train Epoch: 2 [5121024/10786048 (47.5%)]	Loss: 0.091254
Train Epoch: 2 [5172224/10786048 (48.0%)]	Loss: 0.083379
Train Epoch: 2 [5223424/10786048 (48.4%)]	Loss: 0.096230
Train Epoch: 2 [5274624/10786048 (48.9%)]	Loss: 0.093873
Train Epoch: 2 [5325824/10786048 (49.4%)]	Loss: 0.100161
Train Epoch: 2 [5377024/10786048 (49.9%)]	Loss: 0.091345
Train Epoch: 2 [5428224/10786048 (50.3%)]	Loss: 0.104057
Train Epoch: 2 [5479424/10786048 (50.8%)]	Loss: 0.092845
Train Epoch: 2 [5530624/10786048 (51.3%)]	Loss: 0.074413
Train Epoch: 2 [5581824/10786048 (51.8%)]	Loss: 0.097876
Train Epoch: 2 [5633024/10786048 (52.2%)]	Loss: 0.093759
Train Epoch: 2 [5684224/10786048 (52.7%)]	Loss: 0.095620
Train Epoch: 2 [5735424/10786048 (53.2%)]	Loss: 0.103251
Train Epoch: 2 [5786624/10786048 (53.6%)]	Loss: 0.108884
Train Epoch: 2 [5837824/10786048 (54.1%)]	Loss: 0.091160
Train Epoch: 2 [5889024/10786048 (54.6%)]	Loss: 0.103447
Train Epoch: 2 [5940224/10786048 (55.1%)]	Loss: 0.096276
Train Epoch: 2 [5991424/10786048 (55.5%)]	Loss: 0.105178
Train Epoch: 2 [6042624/10786048 (56.0%)]	Loss: 0.085163
Train Epoch: 2 [6093824/10786048 (56.5%)]	Loss: 0.099836
Train Epoch: 2 [6145024/10786048 (57.0%)]	Loss: 0.102147
Train Epoch: 2 [6196224/10786048 (57.4%)]	Loss: 0.096982
Train Epoch: 2 [6247424/10786048 (57.9%)]	Loss: 0.081240
Train Epoch: 2 [6298624/10786048 (58.4%)]	Loss: 0.096290
Train Epoch: 2 [6349824/10786048 (58.9%)]	Loss: 0.087367
Train Epoch: 2 [6401024/10786048 (59.3%)]	Loss: 0.081960
Train Epoch: 2 [6452224/10786048 (59.8%)]	Loss: 0.089822
Train Epoch: 2 [6503424/10786048 (60.3%)]	Loss: 0.097431
Train Epoch: 2 [6554624/10786048 (60.8%)]	Loss: 0.076748
Train Epoch: 2 [6605824/10786048 (61.2%)]	Loss: 0.081204
Train Epoch: 2 [6657024/10786048 (61.7%)]	Loss: 0.100262
Train Epoch: 2 [6708224/10786048 (62.2%)]	Loss: 0.088322
Train Epoch: 2 [6759424/10786048 (62.7%)]	Loss: 0.093941
Train Epoch: 2 [6810624/10786048 (63.1%)]	Loss: 0.077420
Train Epoch: 2 [6861824/10786048 (63.6%)]	Loss: 0.097319
Train Epoch: 2 [6913024/10786048 (64.1%)]	Loss: 0.089837
Train Epoch: 2 [6964224/10786048 (64.6%)]	Loss: 0.097992
Train Epoch: 2 [7015424/10786048 (65.0%)]	Loss: 0.092760
Train Epoch: 2 [7066624/10786048 (65.5%)]	Loss: 0.092462
Train Epoch: 2 [7117824/10786048 (66.0%)]	Loss: 0.092325
Train Epoch: 2 [7169024/10786048 (66.5%)]	Loss: 0.109872
Train Epoch: 2 [7220224/10786048 (66.9%)]	Loss: 0.090886
Train Epoch: 2 [7271424/10786048 (67.4%)]	Loss: 0.090806
Train Epoch: 2 [7322624/10786048 (67.9%)]	Loss: 0.088158
Train Epoch: 2 [7373824/10786048 (68.4%)]	Loss: 0.093187
Train Epoch: 2 [7425024/10786048 (68.8%)]	Loss: 0.103691
Train Epoch: 2 [7476224/10786048 (69.3%)]	Loss: 0.089866
Train Epoch: 2 [7527424/10786048 (69.8%)]	Loss: 0.090939
Train Epoch: 2 [7578624/10786048 (70.3%)]	Loss: 0.093578
Train Epoch: 2 [7629824/10786048 (70.7%)]	Loss: 0.082639
Train Epoch: 2 [7681024/10786048 (71.2%)]	Loss: 0.093777
Train Epoch: 2 [7732224/10786048 (71.7%)]	Loss: 0.100448
Train Epoch: 2 [7783424/10786048 (72.2%)]	Loss: 0.092445
Train Epoch: 2 [7834624/10786048 (72.6%)]	Loss: 0.102951
Train Epoch: 2 [7885824/10786048 (73.1%)]	Loss: 0.088719
Train Epoch: 2 [7937024/10786048 (73.6%)]	Loss: 0.097935
Train Epoch: 2 [7988224/10786048 (74.1%)]	Loss: 0.092585
Train Epoch: 2 [8039424/10786048 (74.5%)]	Loss: 0.094770
Train Epoch: 2 [8090624/10786048 (75.0%)]	Loss: 0.096956
Train Epoch: 2 [8141824/10786048 (75.5%)]	Loss: 0.094370
Train Epoch: 2 [8193024/10786048 (76.0%)]	Loss: 0.091915
Train Epoch: 2 [8244224/10786048 (76.4%)]	Loss: 0.086308
Train Epoch: 2 [8295424/10786048 (76.9%)]	Loss: 0.089236
Train Epoch: 2 [8346624/10786048 (77.4%)]	Loss: 0.093256
Train Epoch: 2 [8397824/10786048 (77.9%)]	Loss: 0.103477
Train Epoch: 2 [8449024/10786048 (78.3%)]	Loss: 0.082009
Train Epoch: 2 [8500224/10786048 (78.8%)]	Loss: 0.076725
Train Epoch: 2 [8551424/10786048 (79.3%)]	Loss: 0.086059
Train Epoch: 2 [8602624/10786048 (79.8%)]	Loss: 0.095999
Train Epoch: 2 [8653824/10786048 (80.2%)]	Loss: 0.087350
Train Epoch: 2 [8705024/10786048 (80.7%)]	Loss: 0.091169
Train Epoch: 2 [8756224/10786048 (81.2%)]	Loss: 0.100835
Train Epoch: 2 [8807424/10786048 (81.7%)]	Loss: 0.083307
Train Epoch: 2 [8858624/10786048 (82.1%)]	Loss: 0.078700
Train Epoch: 2 [8909824/10786048 (82.6%)]	Loss: 0.085998
Train Epoch: 2 [8961024/10786048 (83.1%)]	Loss: 0.092595
Train Epoch: 2 [9012224/10786048 (83.6%)]	Loss: 0.087402
Train Epoch: 2 [9063424/10786048 (84.0%)]	Loss: 0.085692
Train Epoch: 2 [9114624/10786048 (84.5%)]	Loss: 0.081898
Train Epoch: 2 [9165824/10786048 (85.0%)]	Loss: 0.107778
Train Epoch: 2 [9217024/10786048 (85.5%)]	Loss: 0.066660
Train Epoch: 2 [9268224/10786048 (85.9%)]	Loss: 0.095199
Train Epoch: 2 [9319424/10786048 (86.4%)]	Loss: 0.080224
Train Epoch: 2 [9370624/10786048 (86.9%)]	Loss: 0.092433
Train Epoch: 2 [9421824/10786048 (87.4%)]	Loss: 0.078400
Train Epoch: 2 [9473024/10786048 (87.8%)]	Loss: 0.101729
Train Epoch: 2 [9524224/10786048 (88.3%)]	Loss: 0.093574
Train Epoch: 2 [9575424/10786048 (88.8%)]	Loss: 0.075454
Train Epoch: 2 [9626624/10786048 (89.3%)]	Loss: 0.095561
Train Epoch: 2 [9677824/10786048 (89.7%)]	Loss: 0.109355
Train Epoch: 2 [9729024/10786048 (90.2%)]	Loss: 0.097257
Train Epoch: 2 [9780224/10786048 (90.7%)]	Loss: 0.097099
Train Epoch: 2 [9831424/10786048 (91.1%)]	Loss: 0.091285
Train Epoch: 2 [9882624/10786048 (91.6%)]	Loss: 0.093097
Train Epoch: 2 [9933824/10786048 (92.1%)]	Loss: 0.086145
Train Epoch: 2 [9985024/10786048 (92.6%)]	Loss: 0.083008
Train Epoch: 2 [10036224/10786048 (93.0%)]	Loss: 0.070348
Train Epoch: 2 [10087424/10786048 (93.5%)]	Loss: 0.092414
Train Epoch: 2 [10138624/10786048 (94.0%)]	Loss: 0.087366
Train Epoch: 2 [10189824/10786048 (94.5%)]	Loss: 0.096158
Train Epoch: 2 [10241024/10786048 (94.9%)]	Loss: 0.090751
Train Epoch: 2 [10292224/10786048 (95.4%)]	Loss: 0.095287
Train Epoch: 2 [10343424/10786048 (95.9%)]	Loss: 0.083526
Train Epoch: 2 [10394624/10786048 (96.4%)]	Loss: 0.094468
Train Epoch: 2 [10445824/10786048 (96.8%)]	Loss: 0.088533
Train Epoch: 2 [10497024/10786048 (97.3%)]	Loss: 0.098060
Train Epoch: 2 [10548224/10786048 (97.8%)]	Loss: 0.083708
Train Epoch: 2 [10599424/10786048 (98.3%)]	Loss: 0.093894
Train Epoch: 2 [10650624/10786048 (98.7%)]	Loss: 0.105226
Train Epoch: 2 [10701824/10786048 (99.2%)]	Loss: 0.081091
Train Epoch: 2 [10753024/10786048 (99.7%)]	Loss: 0.091440

ACC in fold#0 was 0.937


Balanced ACC in fold#0 was 0.900


MCC in fold#0 was 0.835


Confusion Matrix in fold#0: 
           nonRipple   Ripple
nonRipple     595519   129347
Ripple         41536  1930111


Classification Report in fold#0: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.935        0.937  ...        0.936         0.937
recall            0.822        0.979  ...        0.900         0.937
f1-score          0.875        0.958  ...        0.916         0.935
sample size  724866.000  1971647.000  ...  2696513.000   2696513.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/10786049 (0.0%)]	Loss: 0.345997
Train Epoch: 1 [52224/10786049 (0.5%)]	Loss: 0.141561
Train Epoch: 1 [103424/10786049 (1.0%)]	Loss: 0.108906
Train Epoch: 1 [154624/10786049 (1.4%)]	Loss: 0.109160
Train Epoch: 1 [205824/10786049 (1.9%)]	Loss: 0.100751
Train Epoch: 1 [257024/10786049 (2.4%)]	Loss: 0.110529
Train Epoch: 1 [308224/10786049 (2.9%)]	Loss: 0.116681
Train Epoch: 1 [359424/10786049 (3.3%)]	Loss: 0.083281
Train Epoch: 1 [410624/10786049 (3.8%)]	Loss: 0.090818
Train Epoch: 1 [461824/10786049 (4.3%)]	Loss: 0.102946
Train Epoch: 1 [513024/10786049 (4.8%)]	Loss: 0.093966
Train Epoch: 1 [564224/10786049 (5.2%)]	Loss: 0.103677
Train Epoch: 1 [615424/10786049 (5.7%)]	Loss: 0.096469
Train Epoch: 1 [666624/10786049 (6.2%)]	Loss: 0.087603
Train Epoch: 1 [717824/10786049 (6.7%)]	Loss: 0.119809
Train Epoch: 1 [769024/10786049 (7.1%)]	Loss: 0.092956
Train Epoch: 1 [820224/10786049 (7.6%)]	Loss: 0.101412
Train Epoch: 1 [871424/10786049 (8.1%)]	Loss: 0.097619
Train Epoch: 1 [922624/10786049 (8.6%)]	Loss: 0.108994
Train Epoch: 1 [973824/10786049 (9.0%)]	Loss: 0.099643
Train Epoch: 1 [1025024/10786049 (9.5%)]	Loss: 0.103929
Train Epoch: 1 [1076224/10786049 (10.0%)]	Loss: 0.095537
Train Epoch: 1 [1127424/10786049 (10.5%)]	Loss: 0.091185
Train Epoch: 1 [1178624/10786049 (10.9%)]	Loss: 0.086942
Train Epoch: 1 [1229824/10786049 (11.4%)]	Loss: 0.090046
Train Epoch: 1 [1281024/10786049 (11.9%)]	Loss: 0.096019
Train Epoch: 1 [1332224/10786049 (12.4%)]	Loss: 0.094567
Train Epoch: 1 [1383424/10786049 (12.8%)]	Loss: 0.095730
Train Epoch: 1 [1434624/10786049 (13.3%)]	Loss: 0.094859
Train Epoch: 1 [1485824/10786049 (13.8%)]	Loss: 0.088129
Train Epoch: 1 [1537024/10786049 (14.3%)]	Loss: 0.100100
Train Epoch: 1 [1588224/10786049 (14.7%)]	Loss: 0.073428
Train Epoch: 1 [1639424/10786049 (15.2%)]	Loss: 0.099040
Train Epoch: 1 [1690624/10786049 (15.7%)]	Loss: 0.102762
Train Epoch: 1 [1741824/10786049 (16.1%)]	Loss: 0.100071
Train Epoch: 1 [1793024/10786049 (16.6%)]	Loss: 0.097467
Train Epoch: 1 [1844224/10786049 (17.1%)]	Loss: 0.095337
Train Epoch: 1 [1895424/10786049 (17.6%)]	Loss: 0.086399
Train Epoch: 1 [1946624/10786049 (18.0%)]	Loss: 0.089565
Train Epoch: 1 [1997824/10786049 (18.5%)]	Loss: 0.090883
Train Epoch: 1 [2049024/10786049 (19.0%)]	Loss: 0.085372
Train Epoch: 1 [2100224/10786049 (19.5%)]	Loss: 0.075530
Train Epoch: 1 [2151424/10786049 (19.9%)]	Loss: 0.097940
Train Epoch: 1 [2202624/10786049 (20.4%)]	Loss: 0.105363
Train Epoch: 1 [2253824/10786049 (20.9%)]	Loss: 0.091707
Train Epoch: 1 [2305024/10786049 (21.4%)]	Loss: 0.097577
Train Epoch: 1 [2356224/10786049 (21.8%)]	Loss: 0.105637
Train Epoch: 1 [2407424/10786049 (22.3%)]	Loss: 0.090458
Train Epoch: 1 [2458624/10786049 (22.8%)]	Loss: 0.097349
Train Epoch: 1 [2509824/10786049 (23.3%)]	Loss: 0.091272
Train Epoch: 1 [2561024/10786049 (23.7%)]	Loss: 0.101598
Train Epoch: 1 [2612224/10786049 (24.2%)]	Loss: 0.095192
Train Epoch: 1 [2663424/10786049 (24.7%)]	Loss: 0.094763
Train Epoch: 1 [2714624/10786049 (25.2%)]	Loss: 0.085720
Train Epoch: 1 [2765824/10786049 (25.6%)]	Loss: 0.098687
Train Epoch: 1 [2817024/10786049 (26.1%)]	Loss: 0.094230
Train Epoch: 1 [2868224/10786049 (26.6%)]	Loss: 0.101862
Train Epoch: 1 [2919424/10786049 (27.1%)]	Loss: 0.092771
Train Epoch: 1 [2970624/10786049 (27.5%)]	Loss: 0.083066
Train Epoch: 1 [3021824/10786049 (28.0%)]	Loss: 0.084036
Train Epoch: 1 [3073024/10786049 (28.5%)]	Loss: 0.096086
Train Epoch: 1 [3124224/10786049 (29.0%)]	Loss: 0.087662
Train Epoch: 1 [3175424/10786049 (29.4%)]	Loss: 0.089377
Train Epoch: 1 [3226624/10786049 (29.9%)]	Loss: 0.099135
Train Epoch: 1 [3277824/10786049 (30.4%)]	Loss: 0.100695
Train Epoch: 1 [3329024/10786049 (30.9%)]	Loss: 0.083959
Train Epoch: 1 [3380224/10786049 (31.3%)]	Loss: 0.107245
Train Epoch: 1 [3431424/10786049 (31.8%)]	Loss: 0.088693
Train Epoch: 1 [3482624/10786049 (32.3%)]	Loss: 0.091020
Train Epoch: 1 [3533824/10786049 (32.8%)]	Loss: 0.101852
Train Epoch: 1 [3585024/10786049 (33.2%)]	Loss: 0.099323
Train Epoch: 1 [3636224/10786049 (33.7%)]	Loss: 0.100551
Train Epoch: 1 [3687424/10786049 (34.2%)]	Loss: 0.098476
Train Epoch: 1 [3738624/10786049 (34.7%)]	Loss: 0.076714
Train Epoch: 1 [3789824/10786049 (35.1%)]	Loss: 0.082534
Train Epoch: 1 [3841024/10786049 (35.6%)]	Loss: 0.098948
Train Epoch: 1 [3892224/10786049 (36.1%)]	Loss: 0.096890
Train Epoch: 1 [3943424/10786049 (36.6%)]	Loss: 0.094949
Train Epoch: 1 [3994624/10786049 (37.0%)]	Loss: 0.089376
Train Epoch: 1 [4045824/10786049 (37.5%)]	Loss: 0.069939
Train Epoch: 1 [4097024/10786049 (38.0%)]	Loss: 0.097350
Train Epoch: 1 [4148224/10786049 (38.5%)]	Loss: 0.098352
Train Epoch: 1 [4199424/10786049 (38.9%)]	Loss: 0.085459
Train Epoch: 1 [4250624/10786049 (39.4%)]	Loss: 0.105136
Train Epoch: 1 [4301824/10786049 (39.9%)]	Loss: 0.088745
Train Epoch: 1 [4353024/10786049 (40.4%)]	Loss: 0.079877
Train Epoch: 1 [4404224/10786049 (40.8%)]	Loss: 0.101320
Train Epoch: 1 [4455424/10786049 (41.3%)]	Loss: 0.089894
Train Epoch: 1 [4506624/10786049 (41.8%)]	Loss: 0.106394
Train Epoch: 1 [4557824/10786049 (42.3%)]	Loss: 0.098750
Train Epoch: 1 [4609024/10786049 (42.7%)]	Loss: 0.109274
Train Epoch: 1 [4660224/10786049 (43.2%)]	Loss: 0.108892
Train Epoch: 1 [4711424/10786049 (43.7%)]	Loss: 0.105926
Train Epoch: 1 [4762624/10786049 (44.2%)]	Loss: 0.089142
Train Epoch: 1 [4813824/10786049 (44.6%)]	Loss: 0.083254
Train Epoch: 1 [4865024/10786049 (45.1%)]	Loss: 0.079676
Train Epoch: 1 [4916224/10786049 (45.6%)]	Loss: 0.078353
Train Epoch: 1 [4967424/10786049 (46.1%)]	Loss: 0.094117
Train Epoch: 1 [5018624/10786049 (46.5%)]	Loss: 0.089135
Train Epoch: 1 [5069824/10786049 (47.0%)]	Loss: 0.103284
Train Epoch: 1 [5121024/10786049 (47.5%)]	Loss: 0.085045
Train Epoch: 1 [5172224/10786049 (48.0%)]	Loss: 0.071657
Train Epoch: 1 [5223424/10786049 (48.4%)]	Loss: 0.080759
Train Epoch: 1 [5274624/10786049 (48.9%)]	Loss: 0.082453
Train Epoch: 1 [5325824/10786049 (49.4%)]	Loss: 0.090538
Train Epoch: 1 [5377024/10786049 (49.9%)]	Loss: 0.097275
Train Epoch: 1 [5428224/10786049 (50.3%)]	Loss: 0.085128
Train Epoch: 1 [5479424/10786049 (50.8%)]	Loss: 0.096720
Train Epoch: 1 [5530624/10786049 (51.3%)]	Loss: 0.075565
Train Epoch: 1 [5581824/10786049 (51.8%)]	Loss: 0.090311
Train Epoch: 1 [5633024/10786049 (52.2%)]	Loss: 0.096132
Train Epoch: 1 [5684224/10786049 (52.7%)]	Loss: 0.107325
Train Epoch: 1 [5735424/10786049 (53.2%)]	Loss: 0.093887
Train Epoch: 1 [5786624/10786049 (53.6%)]	Loss: 0.097769
Train Epoch: 1 [5837824/10786049 (54.1%)]	Loss: 0.095225
Train Epoch: 1 [5889024/10786049 (54.6%)]	Loss: 0.078485
Train Epoch: 1 [5940224/10786049 (55.1%)]	Loss: 0.092953
Train Epoch: 1 [5991424/10786049 (55.5%)]	Loss: 0.092372
Train Epoch: 1 [6042624/10786049 (56.0%)]	Loss: 0.103202
Train Epoch: 1 [6093824/10786049 (56.5%)]	Loss: 0.089857
Train Epoch: 1 [6145024/10786049 (57.0%)]	Loss: 0.090272
Train Epoch: 1 [6196224/10786049 (57.4%)]	Loss: 0.090445
Train Epoch: 1 [6247424/10786049 (57.9%)]	Loss: 0.099715
Train Epoch: 1 [6298624/10786049 (58.4%)]	Loss: 0.089761
Train Epoch: 1 [6349824/10786049 (58.9%)]	Loss: 0.105355
Train Epoch: 1 [6401024/10786049 (59.3%)]	Loss: 0.082041
Train Epoch: 1 [6452224/10786049 (59.8%)]	Loss: 0.089982
Train Epoch: 1 [6503424/10786049 (60.3%)]	Loss: 0.090407
Train Epoch: 1 [6554624/10786049 (60.8%)]	Loss: 0.102067
Train Epoch: 1 [6605824/10786049 (61.2%)]	Loss: 0.099077
Train Epoch: 1 [6657024/10786049 (61.7%)]	Loss: 0.087294
Train Epoch: 1 [6708224/10786049 (62.2%)]	Loss: 0.091969
Train Epoch: 1 [6759424/10786049 (62.7%)]	Loss: 0.093718
Train Epoch: 1 [6810624/10786049 (63.1%)]	Loss: 0.101632
Train Epoch: 1 [6861824/10786049 (63.6%)]	Loss: 0.081618
Train Epoch: 1 [6913024/10786049 (64.1%)]	Loss: 0.090115
Train Epoch: 1 [6964224/10786049 (64.6%)]	Loss: 0.095846
Train Epoch: 1 [7015424/10786049 (65.0%)]	Loss: 0.085740
Train Epoch: 1 [7066624/10786049 (65.5%)]	Loss: 0.090247
Train Epoch: 1 [7117824/10786049 (66.0%)]	Loss: 0.093811
Train Epoch: 1 [7169024/10786049 (66.5%)]	Loss: 0.100253
Train Epoch: 1 [7220224/10786049 (66.9%)]	Loss: 0.083817
Train Epoch: 1 [7271424/10786049 (67.4%)]	Loss: 0.120004
Train Epoch: 1 [7322624/10786049 (67.9%)]	Loss: 0.086695
Train Epoch: 1 [7373824/10786049 (68.4%)]	Loss: 0.088243
Train Epoch: 1 [7425024/10786049 (68.8%)]	Loss: 0.086415
Train Epoch: 1 [7476224/10786049 (69.3%)]	Loss: 0.084657
Train Epoch: 1 [7527424/10786049 (69.8%)]	Loss: 0.094042
Train Epoch: 1 [7578624/10786049 (70.3%)]	Loss: 0.099524
Train Epoch: 1 [7629824/10786049 (70.7%)]	Loss: 0.099951
Train Epoch: 1 [7681024/10786049 (71.2%)]	Loss: 0.080817
Train Epoch: 1 [7732224/10786049 (71.7%)]	Loss: 0.091702
Train Epoch: 1 [7783424/10786049 (72.2%)]	Loss: 0.101855
Train Epoch: 1 [7834624/10786049 (72.6%)]	Loss: 0.096880
Train Epoch: 1 [7885824/10786049 (73.1%)]	Loss: 0.097883
Train Epoch: 1 [7937024/10786049 (73.6%)]	Loss: 0.073422
Train Epoch: 1 [7988224/10786049 (74.1%)]	Loss: 0.087713
Train Epoch: 1 [8039424/10786049 (74.5%)]	Loss: 0.096763
Train Epoch: 1 [8090624/10786049 (75.0%)]	Loss: 0.092158
Train Epoch: 1 [8141824/10786049 (75.5%)]	Loss: 0.100453
Train Epoch: 1 [8193024/10786049 (76.0%)]	Loss: 0.096036
Train Epoch: 1 [8244224/10786049 (76.4%)]	Loss: 0.095201
Train Epoch: 1 [8295424/10786049 (76.9%)]	Loss: 0.078786
Train Epoch: 1 [8346624/10786049 (77.4%)]	Loss: 0.079769
Train Epoch: 1 [8397824/10786049 (77.9%)]	Loss: 0.091340
Train Epoch: 1 [8449024/10786049 (78.3%)]	Loss: 0.091728
Train Epoch: 1 [8500224/10786049 (78.8%)]	Loss: 0.101142
Train Epoch: 1 [8551424/10786049 (79.3%)]	Loss: 0.089666
Train Epoch: 1 [8602624/10786049 (79.8%)]	Loss: 0.095052
Train Epoch: 1 [8653824/10786049 (80.2%)]	Loss: 0.085402
Train Epoch: 1 [8705024/10786049 (80.7%)]	Loss: 0.088601
Train Epoch: 1 [8756224/10786049 (81.2%)]	Loss: 0.095083
Train Epoch: 1 [8807424/10786049 (81.7%)]	Loss: 0.082948
Train Epoch: 1 [8858624/10786049 (82.1%)]	Loss: 0.103277
Train Epoch: 1 [8909824/10786049 (82.6%)]	Loss: 0.090001
Train Epoch: 1 [8961024/10786049 (83.1%)]	Loss: 0.096035
Train Epoch: 1 [9012224/10786049 (83.6%)]	Loss: 0.110720
Train Epoch: 1 [9063424/10786049 (84.0%)]	Loss: 0.107880
Train Epoch: 1 [9114624/10786049 (84.5%)]	Loss: 0.092308
Train Epoch: 1 [9165824/10786049 (85.0%)]	Loss: 0.094322
Train Epoch: 1 [9217024/10786049 (85.5%)]	Loss: 0.088217
Train Epoch: 1 [9268224/10786049 (85.9%)]	Loss: 0.098596
Train Epoch: 1 [9319424/10786049 (86.4%)]	Loss: 0.103512
Train Epoch: 1 [9370624/10786049 (86.9%)]	Loss: 0.088477
Train Epoch: 1 [9421824/10786049 (87.4%)]	Loss: 0.080385
Train Epoch: 1 [9473024/10786049 (87.8%)]	Loss: 0.090263
Train Epoch: 1 [9524224/10786049 (88.3%)]	Loss: 0.097225
Train Epoch: 1 [9575424/10786049 (88.8%)]	Loss: 0.088780
Train Epoch: 1 [9626624/10786049 (89.3%)]	Loss: 0.109331
Train Epoch: 1 [9677824/10786049 (89.7%)]	Loss: 0.087597
Train Epoch: 1 [9729024/10786049 (90.2%)]	Loss: 0.094650
Train Epoch: 1 [9780224/10786049 (90.7%)]	Loss: 0.098658
Train Epoch: 1 [9831424/10786049 (91.1%)]	Loss: 0.079889
Train Epoch: 1 [9882624/10786049 (91.6%)]	Loss: 0.095885
Train Epoch: 1 [9933824/10786049 (92.1%)]	Loss: 0.091469
Train Epoch: 1 [9985024/10786049 (92.6%)]	Loss: 0.100079
Train Epoch: 1 [10036224/10786049 (93.0%)]	Loss: 0.088531
Train Epoch: 1 [10087424/10786049 (93.5%)]	Loss: 0.092190
Train Epoch: 1 [10138624/10786049 (94.0%)]	Loss: 0.100840
Train Epoch: 1 [10189824/10786049 (94.5%)]	Loss: 0.089817
Train Epoch: 1 [10241024/10786049 (94.9%)]	Loss: 0.098046
Train Epoch: 1 [10292224/10786049 (95.4%)]	Loss: 0.096182
Train Epoch: 1 [10343424/10786049 (95.9%)]	Loss: 0.080274
Train Epoch: 1 [10394624/10786049 (96.4%)]	Loss: 0.093935
Train Epoch: 1 [10445824/10786049 (96.8%)]	Loss: 0.093240
Train Epoch: 1 [10497024/10786049 (97.3%)]	Loss: 0.091944
Train Epoch: 1 [10548224/10786049 (97.8%)]	Loss: 0.089721
Train Epoch: 1 [10599424/10786049 (98.3%)]	Loss: 0.091652
Train Epoch: 1 [10650624/10786049 (98.7%)]	Loss: 0.072624
Train Epoch: 1 [10701824/10786049 (99.2%)]	Loss: 0.080587
Train Epoch: 1 [10753024/10786049 (99.7%)]	Loss: 0.084509
Train Epoch: 2 [1024/10786049 (0.0%)]	Loss: 0.088091
Train Epoch: 2 [52224/10786049 (0.5%)]	Loss: 0.075265
Train Epoch: 2 [103424/10786049 (1.0%)]	Loss: 0.085994
Train Epoch: 2 [154624/10786049 (1.4%)]	Loss: 0.085808
Train Epoch: 2 [205824/10786049 (1.9%)]	Loss: 0.097459
Train Epoch: 2 [257024/10786049 (2.4%)]	Loss: 0.086846
Train Epoch: 2 [308224/10786049 (2.9%)]	Loss: 0.094184
Train Epoch: 2 [359424/10786049 (3.3%)]	Loss: 0.095031
Train Epoch: 2 [410624/10786049 (3.8%)]	Loss: 0.093768
Train Epoch: 2 [461824/10786049 (4.3%)]	Loss: 0.088053
Train Epoch: 2 [513024/10786049 (4.8%)]	Loss: 0.079799
Train Epoch: 2 [564224/10786049 (5.2%)]	Loss: 0.098913
Train Epoch: 2 [615424/10786049 (5.7%)]	Loss: 0.091658
Train Epoch: 2 [666624/10786049 (6.2%)]	Loss: 0.103070
Train Epoch: 2 [717824/10786049 (6.7%)]	Loss: 0.097926
Train Epoch: 2 [769024/10786049 (7.1%)]	Loss: 0.087151
Train Epoch: 2 [820224/10786049 (7.6%)]	Loss: 0.112241
Train Epoch: 2 [871424/10786049 (8.1%)]	Loss: 0.078259
Train Epoch: 2 [922624/10786049 (8.6%)]	Loss: 0.085773
Train Epoch: 2 [973824/10786049 (9.0%)]	Loss: 0.094019
Train Epoch: 2 [1025024/10786049 (9.5%)]	Loss: 0.099125
Train Epoch: 2 [1076224/10786049 (10.0%)]	Loss: 0.104611
Train Epoch: 2 [1127424/10786049 (10.5%)]	Loss: 0.097034
Train Epoch: 2 [1178624/10786049 (10.9%)]	Loss: 0.104970
Train Epoch: 2 [1229824/10786049 (11.4%)]	Loss: 0.083614
Train Epoch: 2 [1281024/10786049 (11.9%)]	Loss: 0.091792
Train Epoch: 2 [1332224/10786049 (12.4%)]	Loss: 0.075786
Train Epoch: 2 [1383424/10786049 (12.8%)]	Loss: 0.093914
Train Epoch: 2 [1434624/10786049 (13.3%)]	Loss: 0.081708
Train Epoch: 2 [1485824/10786049 (13.8%)]	Loss: 0.096616
Train Epoch: 2 [1537024/10786049 (14.3%)]	Loss: 0.091888
Train Epoch: 2 [1588224/10786049 (14.7%)]	Loss: 0.079898
Train Epoch: 2 [1639424/10786049 (15.2%)]	Loss: 0.091291
Train Epoch: 2 [1690624/10786049 (15.7%)]	Loss: 0.093953
Train Epoch: 2 [1741824/10786049 (16.1%)]	Loss: 0.085081
Train Epoch: 2 [1793024/10786049 (16.6%)]	Loss: 0.096544
Train Epoch: 2 [1844224/10786049 (17.1%)]	Loss: 0.078180
Train Epoch: 2 [1895424/10786049 (17.6%)]	Loss: 0.089632
Train Epoch: 2 [1946624/10786049 (18.0%)]	Loss: 0.086982
Train Epoch: 2 [1997824/10786049 (18.5%)]	Loss: 0.085573
Train Epoch: 2 [2049024/10786049 (19.0%)]	Loss: 0.086707
Train Epoch: 2 [2100224/10786049 (19.5%)]	Loss: 0.087934
Train Epoch: 2 [2151424/10786049 (19.9%)]	Loss: 0.095664
Train Epoch: 2 [2202624/10786049 (20.4%)]	Loss: 0.093046
Train Epoch: 2 [2253824/10786049 (20.9%)]	Loss: 0.086648
Train Epoch: 2 [2305024/10786049 (21.4%)]	Loss: 0.084727
Train Epoch: 2 [2356224/10786049 (21.8%)]	Loss: 0.095399
Train Epoch: 2 [2407424/10786049 (22.3%)]	Loss: 0.097921
Train Epoch: 2 [2458624/10786049 (22.8%)]	Loss: 0.099972
Train Epoch: 2 [2509824/10786049 (23.3%)]	Loss: 0.095132
Train Epoch: 2 [2561024/10786049 (23.7%)]	Loss: 0.100842
Train Epoch: 2 [2612224/10786049 (24.2%)]	Loss: 0.096719
Train Epoch: 2 [2663424/10786049 (24.7%)]	Loss: 0.106990
Train Epoch: 2 [2714624/10786049 (25.2%)]	Loss: 0.084318
Train Epoch: 2 [2765824/10786049 (25.6%)]	Loss: 0.099014
Train Epoch: 2 [2817024/10786049 (26.1%)]	Loss: 0.085755
Train Epoch: 2 [2868224/10786049 (26.6%)]	Loss: 0.095401
Train Epoch: 2 [2919424/10786049 (27.1%)]	Loss: 0.095647
Train Epoch: 2 [2970624/10786049 (27.5%)]	Loss: 0.093236
Train Epoch: 2 [3021824/10786049 (28.0%)]	Loss: 0.086370
Train Epoch: 2 [3073024/10786049 (28.5%)]	Loss: 0.091621
Train Epoch: 2 [3124224/10786049 (29.0%)]	Loss: 0.085473
Train Epoch: 2 [3175424/10786049 (29.4%)]	Loss: 0.092106
Train Epoch: 2 [3226624/10786049 (29.9%)]	Loss: 0.088153
Train Epoch: 2 [3277824/10786049 (30.4%)]	Loss: 0.069681
Train Epoch: 2 [3329024/10786049 (30.9%)]	Loss: 0.076822
Train Epoch: 2 [3380224/10786049 (31.3%)]	Loss: 0.091087
Train Epoch: 2 [3431424/10786049 (31.8%)]	Loss: 0.079903
Train Epoch: 2 [3482624/10786049 (32.3%)]	Loss: 0.093039
Train Epoch: 2 [3533824/10786049 (32.8%)]	Loss: 0.082101
Train Epoch: 2 [3585024/10786049 (33.2%)]	Loss: 0.088670
Train Epoch: 2 [3636224/10786049 (33.7%)]	Loss: 0.086400
Train Epoch: 2 [3687424/10786049 (34.2%)]	Loss: 0.087671
Train Epoch: 2 [3738624/10786049 (34.7%)]	Loss: 0.092009
Train Epoch: 2 [3789824/10786049 (35.1%)]	Loss: 0.093228
Train Epoch: 2 [3841024/10786049 (35.6%)]	Loss: 0.082370
Train Epoch: 2 [3892224/10786049 (36.1%)]	Loss: 0.083693
Train Epoch: 2 [3943424/10786049 (36.6%)]	Loss: 0.093461
Train Epoch: 2 [3994624/10786049 (37.0%)]	Loss: 0.096357
Train Epoch: 2 [4045824/10786049 (37.5%)]	Loss: 0.102886
Train Epoch: 2 [4097024/10786049 (38.0%)]	Loss: 0.090311
Train Epoch: 2 [4148224/10786049 (38.5%)]	Loss: 0.083720
Train Epoch: 2 [4199424/10786049 (38.9%)]	Loss: 0.104971
Train Epoch: 2 [4250624/10786049 (39.4%)]	Loss: 0.086051
Train Epoch: 2 [4301824/10786049 (39.9%)]	Loss: 0.094172
Train Epoch: 2 [4353024/10786049 (40.4%)]	Loss: 0.081714
Train Epoch: 2 [4404224/10786049 (40.8%)]	Loss: 0.078703
Train Epoch: 2 [4455424/10786049 (41.3%)]	Loss: 0.108664
Train Epoch: 2 [4506624/10786049 (41.8%)]	Loss: 0.081561
Train Epoch: 2 [4557824/10786049 (42.3%)]	Loss: 0.080748
Train Epoch: 2 [4609024/10786049 (42.7%)]	Loss: 0.095055
Train Epoch: 2 [4660224/10786049 (43.2%)]	Loss: 0.086281
Train Epoch: 2 [4711424/10786049 (43.7%)]	Loss: 0.091646
Train Epoch: 2 [4762624/10786049 (44.2%)]	Loss: 0.078456
Train Epoch: 2 [4813824/10786049 (44.6%)]	Loss: 0.096016
Train Epoch: 2 [4865024/10786049 (45.1%)]	Loss: 0.085174
Train Epoch: 2 [4916224/10786049 (45.6%)]	Loss: 0.088704
Train Epoch: 2 [4967424/10786049 (46.1%)]	Loss: 0.094363
Train Epoch: 2 [5018624/10786049 (46.5%)]	Loss: 0.080600
Train Epoch: 2 [5069824/10786049 (47.0%)]	Loss: 0.081750
Train Epoch: 2 [5121024/10786049 (47.5%)]	Loss: 0.088650
Train Epoch: 2 [5172224/10786049 (48.0%)]	Loss: 0.094604
Train Epoch: 2 [5223424/10786049 (48.4%)]	Loss: 0.078740
Train Epoch: 2 [5274624/10786049 (48.9%)]	Loss: 0.091464
Train Epoch: 2 [5325824/10786049 (49.4%)]	Loss: 0.086532
Train Epoch: 2 [5377024/10786049 (49.9%)]	Loss: 0.083398
Train Epoch: 2 [5428224/10786049 (50.3%)]	Loss: 0.086183
Train Epoch: 2 [5479424/10786049 (50.8%)]	Loss: 0.097283
Train Epoch: 2 [5530624/10786049 (51.3%)]	Loss: 0.097701
Train Epoch: 2 [5581824/10786049 (51.8%)]	Loss: 0.098905
Train Epoch: 2 [5633024/10786049 (52.2%)]	Loss: 0.090658
Train Epoch: 2 [5684224/10786049 (52.7%)]	Loss: 0.108319
Train Epoch: 2 [5735424/10786049 (53.2%)]	Loss: 0.100311
Train Epoch: 2 [5786624/10786049 (53.6%)]	Loss: 0.080902
Train Epoch: 2 [5837824/10786049 (54.1%)]	Loss: 0.082891
Train Epoch: 2 [5889024/10786049 (54.6%)]	Loss: 0.080216
Train Epoch: 2 [5940224/10786049 (55.1%)]	Loss: 0.089499
Train Epoch: 2 [5991424/10786049 (55.5%)]	Loss: 0.094463
Train Epoch: 2 [6042624/10786049 (56.0%)]	Loss: 0.082412
Train Epoch: 2 [6093824/10786049 (56.5%)]	Loss: 0.074005
Train Epoch: 2 [6145024/10786049 (57.0%)]	Loss: 0.076959
Train Epoch: 2 [6196224/10786049 (57.4%)]	Loss: 0.087058
Train Epoch: 2 [6247424/10786049 (57.9%)]	Loss: 0.096317
Train Epoch: 2 [6298624/10786049 (58.4%)]	Loss: 0.106642
Train Epoch: 2 [6349824/10786049 (58.9%)]	Loss: 0.085013
Train Epoch: 2 [6401024/10786049 (59.3%)]	Loss: 0.084045
Train Epoch: 2 [6452224/10786049 (59.8%)]	Loss: 0.100104
Train Epoch: 2 [6503424/10786049 (60.3%)]	Loss: 0.093471
Train Epoch: 2 [6554624/10786049 (60.8%)]	Loss: 0.091086
Train Epoch: 2 [6605824/10786049 (61.2%)]	Loss: 0.082065
Train Epoch: 2 [6657024/10786049 (61.7%)]	Loss: 0.097230
Train Epoch: 2 [6708224/10786049 (62.2%)]	Loss: 0.086075
Train Epoch: 2 [6759424/10786049 (62.7%)]	Loss: 0.090739
Train Epoch: 2 [6810624/10786049 (63.1%)]	Loss: 0.083312
Train Epoch: 2 [6861824/10786049 (63.6%)]	Loss: 0.078488
Train Epoch: 2 [6913024/10786049 (64.1%)]	Loss: 0.097323
Train Epoch: 2 [6964224/10786049 (64.6%)]	Loss: 0.089058
Train Epoch: 2 [7015424/10786049 (65.0%)]	Loss: 0.086314
Train Epoch: 2 [7066624/10786049 (65.5%)]	Loss: 0.107403
Train Epoch: 2 [7117824/10786049 (66.0%)]	Loss: 0.098815
Train Epoch: 2 [7169024/10786049 (66.5%)]	Loss: 0.091351
Train Epoch: 2 [7220224/10786049 (66.9%)]	Loss: 0.082970
Train Epoch: 2 [7271424/10786049 (67.4%)]	Loss: 0.078108
Train Epoch: 2 [7322624/10786049 (67.9%)]	Loss: 0.089041
Train Epoch: 2 [7373824/10786049 (68.4%)]	Loss: 0.081567
Train Epoch: 2 [7425024/10786049 (68.8%)]	Loss: 0.095107
Train Epoch: 2 [7476224/10786049 (69.3%)]	Loss: 0.085503
Train Epoch: 2 [7527424/10786049 (69.8%)]	Loss: 0.083403
Train Epoch: 2 [7578624/10786049 (70.3%)]	Loss: 0.095277
Train Epoch: 2 [7629824/10786049 (70.7%)]	Loss: 0.085367
Train Epoch: 2 [7681024/10786049 (71.2%)]	Loss: 0.088638
Train Epoch: 2 [7732224/10786049 (71.7%)]	Loss: 0.100090
Train Epoch: 2 [7783424/10786049 (72.2%)]	Loss: 0.098993
Train Epoch: 2 [7834624/10786049 (72.6%)]	Loss: 0.095017
Train Epoch: 2 [7885824/10786049 (73.1%)]	Loss: 0.085543
Train Epoch: 2 [7937024/10786049 (73.6%)]	Loss: 0.089181
Train Epoch: 2 [7988224/10786049 (74.1%)]	Loss: 0.092322
Train Epoch: 2 [8039424/10786049 (74.5%)]	Loss: 0.087538
Train Epoch: 2 [8090624/10786049 (75.0%)]	Loss: 0.083310
Train Epoch: 2 [8141824/10786049 (75.5%)]	Loss: 0.085998
Train Epoch: 2 [8193024/10786049 (76.0%)]	Loss: 0.080902
Train Epoch: 2 [8244224/10786049 (76.4%)]	Loss: 0.089411
Train Epoch: 2 [8295424/10786049 (76.9%)]	Loss: 0.080931
Train Epoch: 2 [8346624/10786049 (77.4%)]	Loss: 0.090781
Train Epoch: 2 [8397824/10786049 (77.9%)]	Loss: 0.077577
Train Epoch: 2 [8449024/10786049 (78.3%)]	Loss: 0.101256
Train Epoch: 2 [8500224/10786049 (78.8%)]	Loss: 0.101254
Train Epoch: 2 [8551424/10786049 (79.3%)]	Loss: 0.095144
Train Epoch: 2 [8602624/10786049 (79.8%)]	Loss: 0.095973
Train Epoch: 2 [8653824/10786049 (80.2%)]	Loss: 0.102158
Train Epoch: 2 [8705024/10786049 (80.7%)]	Loss: 0.087774
Train Epoch: 2 [8756224/10786049 (81.2%)]	Loss: 0.091150
Train Epoch: 2 [8807424/10786049 (81.7%)]	Loss: 0.083514
Train Epoch: 2 [8858624/10786049 (82.1%)]	Loss: 0.084831
Train Epoch: 2 [8909824/10786049 (82.6%)]	Loss: 0.089491
Train Epoch: 2 [8961024/10786049 (83.1%)]	Loss: 0.088955
Train Epoch: 2 [9012224/10786049 (83.6%)]	Loss: 0.087206
Train Epoch: 2 [9063424/10786049 (84.0%)]	Loss: 0.074801
Train Epoch: 2 [9114624/10786049 (84.5%)]	Loss: 0.085462
Train Epoch: 2 [9165824/10786049 (85.0%)]	Loss: 0.095146
Train Epoch: 2 [9217024/10786049 (85.5%)]	Loss: 0.081077
Train Epoch: 2 [9268224/10786049 (85.9%)]	Loss: 0.078300
Train Epoch: 2 [9319424/10786049 (86.4%)]	Loss: 0.100053
Train Epoch: 2 [9370624/10786049 (86.9%)]	Loss: 0.063590
Train Epoch: 2 [9421824/10786049 (87.4%)]	Loss: 0.083714
Train Epoch: 2 [9473024/10786049 (87.8%)]	Loss: 0.087946
Train Epoch: 2 [9524224/10786049 (88.3%)]	Loss: 0.084903
Train Epoch: 2 [9575424/10786049 (88.8%)]	Loss: 0.076349
Train Epoch: 2 [9626624/10786049 (89.3%)]	Loss: 0.089554
Train Epoch: 2 [9677824/10786049 (89.7%)]	Loss: 0.078758
Train Epoch: 2 [9729024/10786049 (90.2%)]	Loss: 0.079645
Train Epoch: 2 [9780224/10786049 (90.7%)]	Loss: 0.087481
Train Epoch: 2 [9831424/10786049 (91.1%)]	Loss: 0.082581
Train Epoch: 2 [9882624/10786049 (91.6%)]	Loss: 0.089444
Train Epoch: 2 [9933824/10786049 (92.1%)]	Loss: 0.081961
Train Epoch: 2 [9985024/10786049 (92.6%)]	Loss: 0.081780
Train Epoch: 2 [10036224/10786049 (93.0%)]	Loss: 0.079443
Train Epoch: 2 [10087424/10786049 (93.5%)]	Loss: 0.105456
Train Epoch: 2 [10138624/10786049 (94.0%)]	Loss: 0.079377
Train Epoch: 2 [10189824/10786049 (94.5%)]	Loss: 0.097191
Train Epoch: 2 [10241024/10786049 (94.9%)]	Loss: 0.085680
Train Epoch: 2 [10292224/10786049 (95.4%)]	Loss: 0.081922
Train Epoch: 2 [10343424/10786049 (95.9%)]	Loss: 0.083255
Train Epoch: 2 [10394624/10786049 (96.4%)]	Loss: 0.089937
Train Epoch: 2 [10445824/10786049 (96.8%)]	Loss: 0.100542
Train Epoch: 2 [10497024/10786049 (97.3%)]	Loss: 0.090778
Train Epoch: 2 [10548224/10786049 (97.8%)]	Loss: 0.097130
Train Epoch: 2 [10599424/10786049 (98.3%)]	Loss: 0.075536
Train Epoch: 2 [10650624/10786049 (98.7%)]	Loss: 0.086799
Train Epoch: 2 [10701824/10786049 (99.2%)]	Loss: 0.092763
Train Epoch: 2 [10753024/10786049 (99.7%)]	Loss: 0.090858

ACC in fold#1 was 0.914


Balanced ACC in fold#1 was 0.857


MCC in fold#1 was 0.776


Confusion Matrix in fold#1: 
           nonRipple   Ripple
nonRipple     532283   192582
Ripple         38604  1933043


Classification Report in fold#1: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.932        0.909  ...        0.921         0.916
recall            0.734        0.980  ...        0.857         0.914
f1-score          0.822        0.944  ...        0.883         0.911
sample size  724865.000  1971647.000  ...  2696512.000   2696512.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/10786049 (0.0%)]	Loss: 0.384316
Train Epoch: 1 [52224/10786049 (0.5%)]	Loss: 0.154058
Train Epoch: 1 [103424/10786049 (1.0%)]	Loss: 0.113628
Train Epoch: 1 [154624/10786049 (1.4%)]	Loss: 0.107754
Train Epoch: 1 [205824/10786049 (1.9%)]	Loss: 0.107526
Train Epoch: 1 [257024/10786049 (2.4%)]	Loss: 0.120138
Train Epoch: 1 [308224/10786049 (2.9%)]	Loss: 0.101804
Train Epoch: 1 [359424/10786049 (3.3%)]	Loss: 0.107092
Train Epoch: 1 [410624/10786049 (3.8%)]	Loss: 0.100356
Train Epoch: 1 [461824/10786049 (4.3%)]	Loss: 0.099510
Train Epoch: 1 [513024/10786049 (4.8%)]	Loss: 0.094209
Train Epoch: 1 [564224/10786049 (5.2%)]	Loss: 0.109608
Train Epoch: 1 [615424/10786049 (5.7%)]	Loss: 0.091577
Train Epoch: 1 [666624/10786049 (6.2%)]	Loss: 0.115563
Train Epoch: 1 [717824/10786049 (6.7%)]	Loss: 0.105723
Train Epoch: 1 [769024/10786049 (7.1%)]	Loss: 0.092448
Train Epoch: 1 [820224/10786049 (7.6%)]	Loss: 0.113937
Train Epoch: 1 [871424/10786049 (8.1%)]	Loss: 0.115171
Train Epoch: 1 [922624/10786049 (8.6%)]	Loss: 0.092178
Train Epoch: 1 [973824/10786049 (9.0%)]	Loss: 0.106019
Train Epoch: 1 [1025024/10786049 (9.5%)]	Loss: 0.100241
Train Epoch: 1 [1076224/10786049 (10.0%)]	Loss: 0.110572
Train Epoch: 1 [1127424/10786049 (10.5%)]	Loss: 0.120768
Train Epoch: 1 [1178624/10786049 (10.9%)]	Loss: 0.110503
Train Epoch: 1 [1229824/10786049 (11.4%)]	Loss: 0.085374
Train Epoch: 1 [1281024/10786049 (11.9%)]	Loss: 0.091727
Train Epoch: 1 [1332224/10786049 (12.4%)]	Loss: 0.105318
Train Epoch: 1 [1383424/10786049 (12.8%)]	Loss: 0.110851
Train Epoch: 1 [1434624/10786049 (13.3%)]	Loss: 0.098661
Train Epoch: 1 [1485824/10786049 (13.8%)]	Loss: 0.101946
Train Epoch: 1 [1537024/10786049 (14.3%)]	Loss: 0.090626
Train Epoch: 1 [1588224/10786049 (14.7%)]	Loss: 0.095231
Train Epoch: 1 [1639424/10786049 (15.2%)]	Loss: 0.111073
Train Epoch: 1 [1690624/10786049 (15.7%)]	Loss: 0.095951
Train Epoch: 1 [1741824/10786049 (16.1%)]	Loss: 0.102629
Train Epoch: 1 [1793024/10786049 (16.6%)]	Loss: 0.107591
Train Epoch: 1 [1844224/10786049 (17.1%)]	Loss: 0.104159
Train Epoch: 1 [1895424/10786049 (17.6%)]	Loss: 0.105515
Train Epoch: 1 [1946624/10786049 (18.0%)]	Loss: 0.098479
Train Epoch: 1 [1997824/10786049 (18.5%)]	Loss: 0.103638
Train Epoch: 1 [2049024/10786049 (19.0%)]	Loss: 0.109853
Train Epoch: 1 [2100224/10786049 (19.5%)]	Loss: 0.100459
Train Epoch: 1 [2151424/10786049 (19.9%)]	Loss: 0.086846
Train Epoch: 1 [2202624/10786049 (20.4%)]	Loss: 0.101621
Train Epoch: 1 [2253824/10786049 (20.9%)]	Loss: 0.107311
Train Epoch: 1 [2305024/10786049 (21.4%)]	Loss: 0.102598
Train Epoch: 1 [2356224/10786049 (21.8%)]	Loss: 0.104620
Train Epoch: 1 [2407424/10786049 (22.3%)]	Loss: 0.104247
Train Epoch: 1 [2458624/10786049 (22.8%)]	Loss: 0.086243
Train Epoch: 1 [2509824/10786049 (23.3%)]	Loss: 0.108400
Train Epoch: 1 [2561024/10786049 (23.7%)]	Loss: 0.111161
Train Epoch: 1 [2612224/10786049 (24.2%)]	Loss: 0.099290
Train Epoch: 1 [2663424/10786049 (24.7%)]	Loss: 0.098207
Train Epoch: 1 [2714624/10786049 (25.2%)]	Loss: 0.080606
Train Epoch: 1 [2765824/10786049 (25.6%)]	Loss: 0.091572
Train Epoch: 1 [2817024/10786049 (26.1%)]	Loss: 0.094472
Train Epoch: 1 [2868224/10786049 (26.6%)]	Loss: 0.110484
Train Epoch: 1 [2919424/10786049 (27.1%)]	Loss: 0.102161
Train Epoch: 1 [2970624/10786049 (27.5%)]	Loss: 0.089273
Train Epoch: 1 [3021824/10786049 (28.0%)]	Loss: 0.101003
Train Epoch: 1 [3073024/10786049 (28.5%)]	Loss: 0.099471
Train Epoch: 1 [3124224/10786049 (29.0%)]	Loss: 0.092282
Train Epoch: 1 [3175424/10786049 (29.4%)]	Loss: 0.108944
Train Epoch: 1 [3226624/10786049 (29.9%)]	Loss: 0.105648
Train Epoch: 1 [3277824/10786049 (30.4%)]	Loss: 0.091047
Train Epoch: 1 [3329024/10786049 (30.9%)]	Loss: 0.093865
Train Epoch: 1 [3380224/10786049 (31.3%)]	Loss: 0.093462
Train Epoch: 1 [3431424/10786049 (31.8%)]	Loss: 0.115350
Train Epoch: 1 [3482624/10786049 (32.3%)]	Loss: 0.102970
Train Epoch: 1 [3533824/10786049 (32.8%)]	Loss: 0.106468
Train Epoch: 1 [3585024/10786049 (33.2%)]	Loss: 0.106918
Train Epoch: 1 [3636224/10786049 (33.7%)]	Loss: 0.095872
Train Epoch: 1 [3687424/10786049 (34.2%)]	Loss: 0.091230
Train Epoch: 1 [3738624/10786049 (34.7%)]	Loss: 0.093695
Train Epoch: 1 [3789824/10786049 (35.1%)]	Loss: 0.104739
Train Epoch: 1 [3841024/10786049 (35.6%)]	Loss: 0.078589
Train Epoch: 1 [3892224/10786049 (36.1%)]	Loss: 0.085069
Train Epoch: 1 [3943424/10786049 (36.6%)]	Loss: 0.106936
Train Epoch: 1 [3994624/10786049 (37.0%)]	Loss: 0.108112
Train Epoch: 1 [4045824/10786049 (37.5%)]	Loss: 0.083583
Train Epoch: 1 [4097024/10786049 (38.0%)]	Loss: 0.098806
Train Epoch: 1 [4148224/10786049 (38.5%)]	Loss: 0.099474
Train Epoch: 1 [4199424/10786049 (38.9%)]	Loss: 0.092873
Train Epoch: 1 [4250624/10786049 (39.4%)]	Loss: 0.093974
Train Epoch: 1 [4301824/10786049 (39.9%)]	Loss: 0.104825
Train Epoch: 1 [4353024/10786049 (40.4%)]	Loss: 0.100111
Train Epoch: 1 [4404224/10786049 (40.8%)]	Loss: 0.099206
Train Epoch: 1 [4455424/10786049 (41.3%)]	Loss: 0.104306
Train Epoch: 1 [4506624/10786049 (41.8%)]	Loss: 0.089600
Train Epoch: 1 [4557824/10786049 (42.3%)]	Loss: 0.088373
Train Epoch: 1 [4609024/10786049 (42.7%)]	Loss: 0.096713
Train Epoch: 1 [4660224/10786049 (43.2%)]	Loss: 0.094834
Train Epoch: 1 [4711424/10786049 (43.7%)]	Loss: 0.110606
Train Epoch: 1 [4762624/10786049 (44.2%)]	Loss: 0.092951
Train Epoch: 1 [4813824/10786049 (44.6%)]	Loss: 0.087093
Train Epoch: 1 [4865024/10786049 (45.1%)]	Loss: 0.112087
Train Epoch: 1 [4916224/10786049 (45.6%)]	Loss: 0.104021
Train Epoch: 1 [4967424/10786049 (46.1%)]	Loss: 0.096007
Train Epoch: 1 [5018624/10786049 (46.5%)]	Loss: 0.095464
Train Epoch: 1 [5069824/10786049 (47.0%)]	Loss: 0.097211
Train Epoch: 1 [5121024/10786049 (47.5%)]	Loss: 0.096689
Train Epoch: 1 [5172224/10786049 (48.0%)]	Loss: 0.093976
Train Epoch: 1 [5223424/10786049 (48.4%)]	Loss: 0.081692
Train Epoch: 1 [5274624/10786049 (48.9%)]	Loss: 0.100952
Train Epoch: 1 [5325824/10786049 (49.4%)]	Loss: 0.091816
Train Epoch: 1 [5377024/10786049 (49.9%)]	Loss: 0.091968
Train Epoch: 1 [5428224/10786049 (50.3%)]	Loss: 0.115964
Train Epoch: 1 [5479424/10786049 (50.8%)]	Loss: 0.086406
Train Epoch: 1 [5530624/10786049 (51.3%)]	Loss: 0.090847
Train Epoch: 1 [5581824/10786049 (51.8%)]	Loss: 0.076656
Train Epoch: 1 [5633024/10786049 (52.2%)]	Loss: 0.091509
Train Epoch: 1 [5684224/10786049 (52.7%)]	Loss: 0.091133
Train Epoch: 1 [5735424/10786049 (53.2%)]	Loss: 0.106086
Train Epoch: 1 [5786624/10786049 (53.6%)]	Loss: 0.092304
Train Epoch: 1 [5837824/10786049 (54.1%)]	Loss: 0.098220
Train Epoch: 1 [5889024/10786049 (54.6%)]	Loss: 0.104989
Train Epoch: 1 [5940224/10786049 (55.1%)]	Loss: 0.105279
Train Epoch: 1 [5991424/10786049 (55.5%)]	Loss: 0.091295
Train Epoch: 1 [6042624/10786049 (56.0%)]	Loss: 0.094857
Train Epoch: 1 [6093824/10786049 (56.5%)]	Loss: 0.092408
Train Epoch: 1 [6145024/10786049 (57.0%)]	Loss: 0.096209
Train Epoch: 1 [6196224/10786049 (57.4%)]	Loss: 0.086365
Train Epoch: 1 [6247424/10786049 (57.9%)]	Loss: 0.099257
Train Epoch: 1 [6298624/10786049 (58.4%)]	Loss: 0.099451
Train Epoch: 1 [6349824/10786049 (58.9%)]	Loss: 0.098914
Train Epoch: 1 [6401024/10786049 (59.3%)]	Loss: 0.083250
Train Epoch: 1 [6452224/10786049 (59.8%)]	Loss: 0.099041
Train Epoch: 1 [6503424/10786049 (60.3%)]	Loss: 0.091159
Train Epoch: 1 [6554624/10786049 (60.8%)]	Loss: 0.089512
Train Epoch: 1 [6605824/10786049 (61.2%)]	Loss: 0.104006
Train Epoch: 1 [6657024/10786049 (61.7%)]	Loss: 0.098059
Train Epoch: 1 [6708224/10786049 (62.2%)]	Loss: 0.103086
Train Epoch: 1 [6759424/10786049 (62.7%)]	Loss: 0.092915
Train Epoch: 1 [6810624/10786049 (63.1%)]	Loss: 0.093480
Train Epoch: 1 [6861824/10786049 (63.6%)]	Loss: 0.086627
Train Epoch: 1 [6913024/10786049 (64.1%)]	Loss: 0.092959
Train Epoch: 1 [6964224/10786049 (64.6%)]	Loss: 0.091231
Train Epoch: 1 [7015424/10786049 (65.0%)]	Loss: 0.077327
Train Epoch: 1 [7066624/10786049 (65.5%)]	Loss: 0.096618
Train Epoch: 1 [7117824/10786049 (66.0%)]	Loss: 0.113678
Train Epoch: 1 [7169024/10786049 (66.5%)]	Loss: 0.089372
Train Epoch: 1 [7220224/10786049 (66.9%)]	Loss: 0.098108
Train Epoch: 1 [7271424/10786049 (67.4%)]	Loss: 0.082219
Train Epoch: 1 [7322624/10786049 (67.9%)]	Loss: 0.107332
Train Epoch: 1 [7373824/10786049 (68.4%)]	Loss: 0.088510
Train Epoch: 1 [7425024/10786049 (68.8%)]	Loss: 0.098051
Train Epoch: 1 [7476224/10786049 (69.3%)]	Loss: 0.091785
Train Epoch: 1 [7527424/10786049 (69.8%)]	Loss: 0.076140
Train Epoch: 1 [7578624/10786049 (70.3%)]	Loss: 0.097530
Train Epoch: 1 [7629824/10786049 (70.7%)]	Loss: 0.103343
Train Epoch: 1 [7681024/10786049 (71.2%)]	Loss: 0.097112
Train Epoch: 1 [7732224/10786049 (71.7%)]	Loss: 0.099408
Train Epoch: 1 [7783424/10786049 (72.2%)]	Loss: 0.091979
Train Epoch: 1 [7834624/10786049 (72.6%)]	Loss: 0.107203
Train Epoch: 1 [7885824/10786049 (73.1%)]	Loss: 0.088340
Train Epoch: 1 [7937024/10786049 (73.6%)]	Loss: 0.098380
Train Epoch: 1 [7988224/10786049 (74.1%)]	Loss: 0.095082
Train Epoch: 1 [8039424/10786049 (74.5%)]	Loss: 0.109685
Train Epoch: 1 [8090624/10786049 (75.0%)]	Loss: 0.100788
Train Epoch: 1 [8141824/10786049 (75.5%)]	Loss: 0.096415
Train Epoch: 1 [8193024/10786049 (76.0%)]	Loss: 0.090691
Train Epoch: 1 [8244224/10786049 (76.4%)]	Loss: 0.092697
Train Epoch: 1 [8295424/10786049 (76.9%)]	Loss: 0.090881
Train Epoch: 1 [8346624/10786049 (77.4%)]	Loss: 0.086984
Train Epoch: 1 [8397824/10786049 (77.9%)]	Loss: 0.088832
Train Epoch: 1 [8449024/10786049 (78.3%)]	Loss: 0.094307
Train Epoch: 1 [8500224/10786049 (78.8%)]	Loss: 0.102652
Train Epoch: 1 [8551424/10786049 (79.3%)]	Loss: 0.100713
Train Epoch: 1 [8602624/10786049 (79.8%)]	Loss: 0.084409
Train Epoch: 1 [8653824/10786049 (80.2%)]	Loss: 0.098459
Train Epoch: 1 [8705024/10786049 (80.7%)]	Loss: 0.095756
Train Epoch: 1 [8756224/10786049 (81.2%)]	Loss: 0.089589
Train Epoch: 1 [8807424/10786049 (81.7%)]	Loss: 0.093736
Train Epoch: 1 [8858624/10786049 (82.1%)]	Loss: 0.077937
Train Epoch: 1 [8909824/10786049 (82.6%)]	Loss: 0.103749
Train Epoch: 1 [8961024/10786049 (83.1%)]	Loss: 0.103445
Train Epoch: 1 [9012224/10786049 (83.6%)]	Loss: 0.087588
Train Epoch: 1 [9063424/10786049 (84.0%)]	Loss: 0.103331
Train Epoch: 1 [9114624/10786049 (84.5%)]	Loss: 0.095966
Train Epoch: 1 [9165824/10786049 (85.0%)]	Loss: 0.086988
Train Epoch: 1 [9217024/10786049 (85.5%)]	Loss: 0.099583
Train Epoch: 1 [9268224/10786049 (85.9%)]	Loss: 0.091532
Train Epoch: 1 [9319424/10786049 (86.4%)]	Loss: 0.086823
Train Epoch: 1 [9370624/10786049 (86.9%)]	Loss: 0.084383
Train Epoch: 1 [9421824/10786049 (87.4%)]	Loss: 0.089226
Train Epoch: 1 [9473024/10786049 (87.8%)]	Loss: 0.106803
Train Epoch: 1 [9524224/10786049 (88.3%)]	Loss: 0.093531
Train Epoch: 1 [9575424/10786049 (88.8%)]	Loss: 0.099741
Train Epoch: 1 [9626624/10786049 (89.3%)]	Loss: 0.089691
Train Epoch: 1 [9677824/10786049 (89.7%)]	Loss: 0.087074
Train Epoch: 1 [9729024/10786049 (90.2%)]	Loss: 0.091396
Train Epoch: 1 [9780224/10786049 (90.7%)]	Loss: 0.100087
Train Epoch: 1 [9831424/10786049 (91.1%)]	Loss: 0.095846
Train Epoch: 1 [9882624/10786049 (91.6%)]	Loss: 0.108175
Train Epoch: 1 [9933824/10786049 (92.1%)]	Loss: 0.097400
Train Epoch: 1 [9985024/10786049 (92.6%)]	Loss: 0.088842
Train Epoch: 1 [10036224/10786049 (93.0%)]	Loss: 0.093637
Train Epoch: 1 [10087424/10786049 (93.5%)]	Loss: 0.084697
Train Epoch: 1 [10138624/10786049 (94.0%)]	Loss: 0.085890
Train Epoch: 1 [10189824/10786049 (94.5%)]	Loss: 0.079675
Train Epoch: 1 [10241024/10786049 (94.9%)]	Loss: 0.094361
Train Epoch: 1 [10292224/10786049 (95.4%)]	Loss: 0.090714
Train Epoch: 1 [10343424/10786049 (95.9%)]	Loss: 0.092622
Train Epoch: 1 [10394624/10786049 (96.4%)]	Loss: 0.096925
Train Epoch: 1 [10445824/10786049 (96.8%)]	Loss: 0.092966
Train Epoch: 1 [10497024/10786049 (97.3%)]	Loss: 0.095589
Train Epoch: 1 [10548224/10786049 (97.8%)]	Loss: 0.092920
Train Epoch: 1 [10599424/10786049 (98.3%)]	Loss: 0.100961
Train Epoch: 1 [10650624/10786049 (98.7%)]	Loss: 0.082340
Train Epoch: 1 [10701824/10786049 (99.2%)]	Loss: 0.088079
Train Epoch: 1 [10753024/10786049 (99.7%)]	Loss: 0.096434
Train Epoch: 2 [1024/10786049 (0.0%)]	Loss: 0.104636
Train Epoch: 2 [52224/10786049 (0.5%)]	Loss: 0.099773
Train Epoch: 2 [103424/10786049 (1.0%)]	Loss: 0.093881
Train Epoch: 2 [154624/10786049 (1.4%)]	Loss: 0.090970
Train Epoch: 2 [205824/10786049 (1.9%)]	Loss: 0.094369
Train Epoch: 2 [257024/10786049 (2.4%)]	Loss: 0.093289
Train Epoch: 2 [308224/10786049 (2.9%)]	Loss: 0.092327
Train Epoch: 2 [359424/10786049 (3.3%)]	Loss: 0.092220
Train Epoch: 2 [410624/10786049 (3.8%)]	Loss: 0.087302
Train Epoch: 2 [461824/10786049 (4.3%)]	Loss: 0.097566
Train Epoch: 2 [513024/10786049 (4.8%)]	Loss: 0.087464
Train Epoch: 2 [564224/10786049 (5.2%)]	Loss: 0.098522
Train Epoch: 2 [615424/10786049 (5.7%)]	Loss: 0.087404
Train Epoch: 2 [666624/10786049 (6.2%)]	Loss: 0.081581
Train Epoch: 2 [717824/10786049 (6.7%)]	Loss: 0.099330
Train Epoch: 2 [769024/10786049 (7.1%)]	Loss: 0.100190
Train Epoch: 2 [820224/10786049 (7.6%)]	Loss: 0.090097
Train Epoch: 2 [871424/10786049 (8.1%)]	Loss: 0.084044
Train Epoch: 2 [922624/10786049 (8.6%)]	Loss: 0.087321
Train Epoch: 2 [973824/10786049 (9.0%)]	Loss: 0.084694
Train Epoch: 2 [1025024/10786049 (9.5%)]	Loss: 0.087944
Train Epoch: 2 [1076224/10786049 (10.0%)]	Loss: 0.099822
Train Epoch: 2 [1127424/10786049 (10.5%)]	Loss: 0.105556
Train Epoch: 2 [1178624/10786049 (10.9%)]	Loss: 0.095243
Train Epoch: 2 [1229824/10786049 (11.4%)]	Loss: 0.109058
Train Epoch: 2 [1281024/10786049 (11.9%)]	Loss: 0.081871
Train Epoch: 2 [1332224/10786049 (12.4%)]	Loss: 0.097599
Train Epoch: 2 [1383424/10786049 (12.8%)]	Loss: 0.078892
Train Epoch: 2 [1434624/10786049 (13.3%)]	Loss: 0.095350
Train Epoch: 2 [1485824/10786049 (13.8%)]	Loss: 0.093674
Train Epoch: 2 [1537024/10786049 (14.3%)]	Loss: 0.102217
Train Epoch: 2 [1588224/10786049 (14.7%)]	Loss: 0.100799
Train Epoch: 2 [1639424/10786049 (15.2%)]	Loss: 0.101724
Train Epoch: 2 [1690624/10786049 (15.7%)]	Loss: 0.100322
Train Epoch: 2 [1741824/10786049 (16.1%)]	Loss: 0.080930
Train Epoch: 2 [1793024/10786049 (16.6%)]	Loss: 0.112541
Train Epoch: 2 [1844224/10786049 (17.1%)]	Loss: 0.097565
Train Epoch: 2 [1895424/10786049 (17.6%)]	Loss: 0.087790
Train Epoch: 2 [1946624/10786049 (18.0%)]	Loss: 0.090973
Train Epoch: 2 [1997824/10786049 (18.5%)]	Loss: 0.089298
Train Epoch: 2 [2049024/10786049 (19.0%)]	Loss: 0.098063
Train Epoch: 2 [2100224/10786049 (19.5%)]	Loss: 0.102718
Train Epoch: 2 [2151424/10786049 (19.9%)]	Loss: 0.104577
Train Epoch: 2 [2202624/10786049 (20.4%)]	Loss: 0.097787
Train Epoch: 2 [2253824/10786049 (20.9%)]	Loss: 0.088912
Train Epoch: 2 [2305024/10786049 (21.4%)]	Loss: 0.107685
Train Epoch: 2 [2356224/10786049 (21.8%)]	Loss: 0.101257
Train Epoch: 2 [2407424/10786049 (22.3%)]	Loss: 0.096781
Train Epoch: 2 [2458624/10786049 (22.8%)]	Loss: 0.082359
Train Epoch: 2 [2509824/10786049 (23.3%)]	Loss: 0.094151
Train Epoch: 2 [2561024/10786049 (23.7%)]	Loss: 0.092067
Train Epoch: 2 [2612224/10786049 (24.2%)]	Loss: 0.093185
Train Epoch: 2 [2663424/10786049 (24.7%)]	Loss: 0.092371
Train Epoch: 2 [2714624/10786049 (25.2%)]	Loss: 0.107478
Train Epoch: 2 [2765824/10786049 (25.6%)]	Loss: 0.091931
Train Epoch: 2 [2817024/10786049 (26.1%)]	Loss: 0.091948
Train Epoch: 2 [2868224/10786049 (26.6%)]	Loss: 0.104251
Train Epoch: 2 [2919424/10786049 (27.1%)]	Loss: 0.090295
Train Epoch: 2 [2970624/10786049 (27.5%)]	Loss: 0.091216
Train Epoch: 2 [3021824/10786049 (28.0%)]	Loss: 0.088975
Train Epoch: 2 [3073024/10786049 (28.5%)]	Loss: 0.099359
Train Epoch: 2 [3124224/10786049 (29.0%)]	Loss: 0.095723
Train Epoch: 2 [3175424/10786049 (29.4%)]	Loss: 0.088933
Train Epoch: 2 [3226624/10786049 (29.9%)]	Loss: 0.089234
Train Epoch: 2 [3277824/10786049 (30.4%)]	Loss: 0.112210
Train Epoch: 2 [3329024/10786049 (30.9%)]	Loss: 0.099013
Train Epoch: 2 [3380224/10786049 (31.3%)]	Loss: 0.098856
Train Epoch: 2 [3431424/10786049 (31.8%)]	Loss: 0.083845
Train Epoch: 2 [3482624/10786049 (32.3%)]	Loss: 0.084305
Train Epoch: 2 [3533824/10786049 (32.8%)]	Loss: 0.090108
Train Epoch: 2 [3585024/10786049 (33.2%)]	Loss: 0.099933
Train Epoch: 2 [3636224/10786049 (33.7%)]	Loss: 0.102214
Train Epoch: 2 [3687424/10786049 (34.2%)]	Loss: 0.083124
Train Epoch: 2 [3738624/10786049 (34.7%)]	Loss: 0.098944
Train Epoch: 2 [3789824/10786049 (35.1%)]	Loss: 0.093680
Train Epoch: 2 [3841024/10786049 (35.6%)]	Loss: 0.097232
Train Epoch: 2 [3892224/10786049 (36.1%)]	Loss: 0.093618
Train Epoch: 2 [3943424/10786049 (36.6%)]	Loss: 0.091985
Train Epoch: 2 [3994624/10786049 (37.0%)]	Loss: 0.102684
Train Epoch: 2 [4045824/10786049 (37.5%)]	Loss: 0.086114
Train Epoch: 2 [4097024/10786049 (38.0%)]	Loss: 0.092059
Train Epoch: 2 [4148224/10786049 (38.5%)]	Loss: 0.100182
Train Epoch: 2 [4199424/10786049 (38.9%)]	Loss: 0.110563
Train Epoch: 2 [4250624/10786049 (39.4%)]	Loss: 0.105705
Train Epoch: 2 [4301824/10786049 (39.9%)]	Loss: 0.096618
Train Epoch: 2 [4353024/10786049 (40.4%)]	Loss: 0.096032
Train Epoch: 2 [4404224/10786049 (40.8%)]	Loss: 0.085049
Train Epoch: 2 [4455424/10786049 (41.3%)]	Loss: 0.082403
Train Epoch: 2 [4506624/10786049 (41.8%)]	Loss: 0.086982
Train Epoch: 2 [4557824/10786049 (42.3%)]	Loss: 0.099852
Train Epoch: 2 [4609024/10786049 (42.7%)]	Loss: 0.093032
Train Epoch: 2 [4660224/10786049 (43.2%)]	Loss: 0.094818
Train Epoch: 2 [4711424/10786049 (43.7%)]	Loss: 0.079679
Train Epoch: 2 [4762624/10786049 (44.2%)]	Loss: 0.104388
Train Epoch: 2 [4813824/10786049 (44.6%)]	Loss: 0.094150
Train Epoch: 2 [4865024/10786049 (45.1%)]	Loss: 0.088633
Train Epoch: 2 [4916224/10786049 (45.6%)]	Loss: 0.088433
Train Epoch: 2 [4967424/10786049 (46.1%)]	Loss: 0.088479
Train Epoch: 2 [5018624/10786049 (46.5%)]	Loss: 0.086760
Train Epoch: 2 [5069824/10786049 (47.0%)]	Loss: 0.093013
Train Epoch: 2 [5121024/10786049 (47.5%)]	Loss: 0.082761
Train Epoch: 2 [5172224/10786049 (48.0%)]	Loss: 0.080001
Train Epoch: 2 [5223424/10786049 (48.4%)]	Loss: 0.119499
Train Epoch: 2 [5274624/10786049 (48.9%)]	Loss: 0.080987
Train Epoch: 2 [5325824/10786049 (49.4%)]	Loss: 0.097549
Train Epoch: 2 [5377024/10786049 (49.9%)]	Loss: 0.092353
Train Epoch: 2 [5428224/10786049 (50.3%)]	Loss: 0.093076
Train Epoch: 2 [5479424/10786049 (50.8%)]	Loss: 0.106982
Train Epoch: 2 [5530624/10786049 (51.3%)]	Loss: 0.099444
Train Epoch: 2 [5581824/10786049 (51.8%)]	Loss: 0.092580
Train Epoch: 2 [5633024/10786049 (52.2%)]	Loss: 0.088667
Train Epoch: 2 [5684224/10786049 (52.7%)]	Loss: 0.084261
Train Epoch: 2 [5735424/10786049 (53.2%)]	Loss: 0.086077
Train Epoch: 2 [5786624/10786049 (53.6%)]	Loss: 0.092673
Train Epoch: 2 [5837824/10786049 (54.1%)]	Loss: 0.093495
Train Epoch: 2 [5889024/10786049 (54.6%)]	Loss: 0.098790
Train Epoch: 2 [5940224/10786049 (55.1%)]	Loss: 0.086287
Train Epoch: 2 [5991424/10786049 (55.5%)]	Loss: 0.104488
Train Epoch: 2 [6042624/10786049 (56.0%)]	Loss: 0.096596
Train Epoch: 2 [6093824/10786049 (56.5%)]	Loss: 0.091288
Train Epoch: 2 [6145024/10786049 (57.0%)]	Loss: 0.083398
Train Epoch: 2 [6196224/10786049 (57.4%)]	Loss: 0.100353
Train Epoch: 2 [6247424/10786049 (57.9%)]	Loss: 0.106077
Train Epoch: 2 [6298624/10786049 (58.4%)]	Loss: 0.091422
Train Epoch: 2 [6349824/10786049 (58.9%)]	Loss: 0.097662
Train Epoch: 2 [6401024/10786049 (59.3%)]	Loss: 0.096445
Train Epoch: 2 [6452224/10786049 (59.8%)]	Loss: 0.111904
Train Epoch: 2 [6503424/10786049 (60.3%)]	Loss: 0.082008
Train Epoch: 2 [6554624/10786049 (60.8%)]	Loss: 0.085519
Train Epoch: 2 [6605824/10786049 (61.2%)]	Loss: 0.090912
Train Epoch: 2 [6657024/10786049 (61.7%)]	Loss: 0.106692
Train Epoch: 2 [6708224/10786049 (62.2%)]	Loss: 0.094282
Train Epoch: 2 [6759424/10786049 (62.7%)]	Loss: 0.091864
Train Epoch: 2 [6810624/10786049 (63.1%)]	Loss: 0.101362
Train Epoch: 2 [6861824/10786049 (63.6%)]	Loss: 0.090636
Train Epoch: 2 [6913024/10786049 (64.1%)]	Loss: 0.083241
Train Epoch: 2 [6964224/10786049 (64.6%)]	Loss: 0.099614
Train Epoch: 2 [7015424/10786049 (65.0%)]	Loss: 0.095298
Train Epoch: 2 [7066624/10786049 (65.5%)]	Loss: 0.086529
Train Epoch: 2 [7117824/10786049 (66.0%)]	Loss: 0.100342
Train Epoch: 2 [7169024/10786049 (66.5%)]	Loss: 0.091145
Train Epoch: 2 [7220224/10786049 (66.9%)]	Loss: 0.087510
Train Epoch: 2 [7271424/10786049 (67.4%)]	Loss: 0.091075
Train Epoch: 2 [7322624/10786049 (67.9%)]	Loss: 0.092064
Train Epoch: 2 [7373824/10786049 (68.4%)]	Loss: 0.101777
Train Epoch: 2 [7425024/10786049 (68.8%)]	Loss: 0.084548
Train Epoch: 2 [7476224/10786049 (69.3%)]	Loss: 0.113783
Train Epoch: 2 [7527424/10786049 (69.8%)]	Loss: 0.082764
Train Epoch: 2 [7578624/10786049 (70.3%)]	Loss: 0.081735
Train Epoch: 2 [7629824/10786049 (70.7%)]	Loss: 0.084839
Train Epoch: 2 [7681024/10786049 (71.2%)]	Loss: 0.083419
Train Epoch: 2 [7732224/10786049 (71.7%)]	Loss: 0.102052
Train Epoch: 2 [7783424/10786049 (72.2%)]	Loss: 0.084747
Train Epoch: 2 [7834624/10786049 (72.6%)]	Loss: 0.082366
Train Epoch: 2 [7885824/10786049 (73.1%)]	Loss: 0.098094
Train Epoch: 2 [7937024/10786049 (73.6%)]	Loss: 0.102293
Train Epoch: 2 [7988224/10786049 (74.1%)]	Loss: 0.096079
Train Epoch: 2 [8039424/10786049 (74.5%)]	Loss: 0.100357
Train Epoch: 2 [8090624/10786049 (75.0%)]	Loss: 0.083981
Train Epoch: 2 [8141824/10786049 (75.5%)]	Loss: 0.089050
Train Epoch: 2 [8193024/10786049 (76.0%)]	Loss: 0.096320
Train Epoch: 2 [8244224/10786049 (76.4%)]	Loss: 0.086880
Train Epoch: 2 [8295424/10786049 (76.9%)]	Loss: 0.091931
Train Epoch: 2 [8346624/10786049 (77.4%)]	Loss: 0.091545
Train Epoch: 2 [8397824/10786049 (77.9%)]	Loss: 0.069490
Train Epoch: 2 [8449024/10786049 (78.3%)]	Loss: 0.090806
Train Epoch: 2 [8500224/10786049 (78.8%)]	Loss: 0.079235
Train Epoch: 2 [8551424/10786049 (79.3%)]	Loss: 0.088529
Train Epoch: 2 [8602624/10786049 (79.8%)]	Loss: 0.099060
Train Epoch: 2 [8653824/10786049 (80.2%)]	Loss: 0.088860
Train Epoch: 2 [8705024/10786049 (80.7%)]	Loss: 0.094582
Train Epoch: 2 [8756224/10786049 (81.2%)]	Loss: 0.099264
Train Epoch: 2 [8807424/10786049 (81.7%)]	Loss: 0.082259
Train Epoch: 2 [8858624/10786049 (82.1%)]	Loss: 0.093939
Train Epoch: 2 [8909824/10786049 (82.6%)]	Loss: 0.091810
Train Epoch: 2 [8961024/10786049 (83.1%)]	Loss: 0.089576
Train Epoch: 2 [9012224/10786049 (83.6%)]	Loss: 0.086360
Train Epoch: 2 [9063424/10786049 (84.0%)]	Loss: 0.096796
Train Epoch: 2 [9114624/10786049 (84.5%)]	Loss: 0.100763
Train Epoch: 2 [9165824/10786049 (85.0%)]	Loss: 0.086388
Train Epoch: 2 [9217024/10786049 (85.5%)]	Loss: 0.104220
Train Epoch: 2 [9268224/10786049 (85.9%)]	Loss: 0.090168
Train Epoch: 2 [9319424/10786049 (86.4%)]	Loss: 0.098520
Train Epoch: 2 [9370624/10786049 (86.9%)]	Loss: 0.084383
Train Epoch: 2 [9421824/10786049 (87.4%)]	Loss: 0.097612
Train Epoch: 2 [9473024/10786049 (87.8%)]	Loss: 0.077281
Train Epoch: 2 [9524224/10786049 (88.3%)]	Loss: 0.086977
Train Epoch: 2 [9575424/10786049 (88.8%)]	Loss: 0.099105
Train Epoch: 2 [9626624/10786049 (89.3%)]	Loss: 0.090491
Train Epoch: 2 [9677824/10786049 (89.7%)]	Loss: 0.094810
Train Epoch: 2 [9729024/10786049 (90.2%)]	Loss: 0.084804
Train Epoch: 2 [9780224/10786049 (90.7%)]	Loss: 0.080495
Train Epoch: 2 [9831424/10786049 (91.1%)]	Loss: 0.085020
Train Epoch: 2 [9882624/10786049 (91.6%)]	Loss: 0.092862
Train Epoch: 2 [9933824/10786049 (92.1%)]	Loss: 0.107221
Train Epoch: 2 [9985024/10786049 (92.6%)]	Loss: 0.088596
Train Epoch: 2 [10036224/10786049 (93.0%)]	Loss: 0.102496
Train Epoch: 2 [10087424/10786049 (93.5%)]	Loss: 0.093470
Train Epoch: 2 [10138624/10786049 (94.0%)]	Loss: 0.099659
Train Epoch: 2 [10189824/10786049 (94.5%)]	Loss: 0.083449
Train Epoch: 2 [10241024/10786049 (94.9%)]	Loss: 0.092539
Train Epoch: 2 [10292224/10786049 (95.4%)]	Loss: 0.096281
Train Epoch: 2 [10343424/10786049 (95.9%)]	Loss: 0.098082
Train Epoch: 2 [10394624/10786049 (96.4%)]	Loss: 0.093200
Train Epoch: 2 [10445824/10786049 (96.8%)]	Loss: 0.090494
Train Epoch: 2 [10497024/10786049 (97.3%)]	Loss: 0.101053
Train Epoch: 2 [10548224/10786049 (97.8%)]	Loss: 0.097949
Train Epoch: 2 [10599424/10786049 (98.3%)]	Loss: 0.086134
Train Epoch: 2 [10650624/10786049 (98.7%)]	Loss: 0.108119
Train Epoch: 2 [10701824/10786049 (99.2%)]	Loss: 0.077920
Train Epoch: 2 [10753024/10786049 (99.7%)]	Loss: 0.095093

ACC in fold#2 was 0.925


Balanced ACC in fold#2 was 0.915


MCC in fold#2 was 0.814


Confusion Matrix in fold#2: 
           nonRipple   Ripple
nonRipple     647361    77504
Ripple        125144  1846503


Classification Report in fold#2: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.838        0.960  ...        0.899         0.927
recall            0.893        0.937  ...        0.915         0.925
f1-score          0.865        0.948  ...        0.906         0.926
sample size  724865.000  1971647.000  ...  2696512.000   2696512.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/10786049 (0.0%)]	Loss: 0.355458
Train Epoch: 1 [52224/10786049 (0.5%)]	Loss: 0.131329
Train Epoch: 1 [103424/10786049 (1.0%)]	Loss: 0.099823
Train Epoch: 1 [154624/10786049 (1.4%)]	Loss: 0.084358
Train Epoch: 1 [205824/10786049 (1.9%)]	Loss: 0.108641
Train Epoch: 1 [257024/10786049 (2.4%)]	Loss: 0.109533
Train Epoch: 1 [308224/10786049 (2.9%)]	Loss: 0.096297
Train Epoch: 1 [359424/10786049 (3.3%)]	Loss: 0.107898
Train Epoch: 1 [410624/10786049 (3.8%)]	Loss: 0.092524
Train Epoch: 1 [461824/10786049 (4.3%)]	Loss: 0.099766
Train Epoch: 1 [513024/10786049 (4.8%)]	Loss: 0.099424
Train Epoch: 1 [564224/10786049 (5.2%)]	Loss: 0.109172
Train Epoch: 1 [615424/10786049 (5.7%)]	Loss: 0.088436
Train Epoch: 1 [666624/10786049 (6.2%)]	Loss: 0.105677
Train Epoch: 1 [717824/10786049 (6.7%)]	Loss: 0.096881
Train Epoch: 1 [769024/10786049 (7.1%)]	Loss: 0.103914
Train Epoch: 1 [820224/10786049 (7.6%)]	Loss: 0.081893
Train Epoch: 1 [871424/10786049 (8.1%)]	Loss: 0.072836
Train Epoch: 1 [922624/10786049 (8.6%)]	Loss: 0.084945
Train Epoch: 1 [973824/10786049 (9.0%)]	Loss: 0.097203
Train Epoch: 1 [1025024/10786049 (9.5%)]	Loss: 0.089406
Train Epoch: 1 [1076224/10786049 (10.0%)]	Loss: 0.098137
Train Epoch: 1 [1127424/10786049 (10.5%)]	Loss: 0.080520
Train Epoch: 1 [1178624/10786049 (10.9%)]	Loss: 0.093063
Train Epoch: 1 [1229824/10786049 (11.4%)]	Loss: 0.096403
Train Epoch: 1 [1281024/10786049 (11.9%)]	Loss: 0.078218
Train Epoch: 1 [1332224/10786049 (12.4%)]	Loss: 0.084218
Train Epoch: 1 [1383424/10786049 (12.8%)]	Loss: 0.089792
Train Epoch: 1 [1434624/10786049 (13.3%)]	Loss: 0.090325
Train Epoch: 1 [1485824/10786049 (13.8%)]	Loss: 0.082247
Train Epoch: 1 [1537024/10786049 (14.3%)]	Loss: 0.085937
Train Epoch: 1 [1588224/10786049 (14.7%)]	Loss: 0.097190
Train Epoch: 1 [1639424/10786049 (15.2%)]	Loss: 0.085900
Train Epoch: 1 [1690624/10786049 (15.7%)]	Loss: 0.086960
Train Epoch: 1 [1741824/10786049 (16.1%)]	Loss: 0.086339
Train Epoch: 1 [1793024/10786049 (16.6%)]	Loss: 0.077038
Train Epoch: 1 [1844224/10786049 (17.1%)]	Loss: 0.086404
Train Epoch: 1 [1895424/10786049 (17.6%)]	Loss: 0.099340
Train Epoch: 1 [1946624/10786049 (18.0%)]	Loss: 0.095525
Train Epoch: 1 [1997824/10786049 (18.5%)]	Loss: 0.086466
Train Epoch: 1 [2049024/10786049 (19.0%)]	Loss: 0.089385
Train Epoch: 1 [2100224/10786049 (19.5%)]	Loss: 0.086415
Train Epoch: 1 [2151424/10786049 (19.9%)]	Loss: 0.095308
Train Epoch: 1 [2202624/10786049 (20.4%)]	Loss: 0.085371
Train Epoch: 1 [2253824/10786049 (20.9%)]	Loss: 0.080383
Train Epoch: 1 [2305024/10786049 (21.4%)]	Loss: 0.084015
Train Epoch: 1 [2356224/10786049 (21.8%)]	Loss: 0.079280
Train Epoch: 1 [2407424/10786049 (22.3%)]	Loss: 0.084014
Train Epoch: 1 [2458624/10786049 (22.8%)]	Loss: 0.087233
Train Epoch: 1 [2509824/10786049 (23.3%)]	Loss: 0.084661
Train Epoch: 1 [2561024/10786049 (23.7%)]	Loss: 0.095272
Train Epoch: 1 [2612224/10786049 (24.2%)]	Loss: 0.083261
Train Epoch: 1 [2663424/10786049 (24.7%)]	Loss: 0.086728
Train Epoch: 1 [2714624/10786049 (25.2%)]	Loss: 0.094629
Train Epoch: 1 [2765824/10786049 (25.6%)]	Loss: 0.099531
Train Epoch: 1 [2817024/10786049 (26.1%)]	Loss: 0.086548
Train Epoch: 1 [2868224/10786049 (26.6%)]	Loss: 0.078806
Train Epoch: 1 [2919424/10786049 (27.1%)]	Loss: 0.098489
Train Epoch: 1 [2970624/10786049 (27.5%)]	Loss: 0.090129
Train Epoch: 1 [3021824/10786049 (28.0%)]	Loss: 0.077664
Train Epoch: 1 [3073024/10786049 (28.5%)]	Loss: 0.070074
Train Epoch: 1 [3124224/10786049 (29.0%)]	Loss: 0.083880
Train Epoch: 1 [3175424/10786049 (29.4%)]	Loss: 0.086392
Train Epoch: 1 [3226624/10786049 (29.9%)]	Loss: 0.090439
Train Epoch: 1 [3277824/10786049 (30.4%)]	Loss: 0.095841
Train Epoch: 1 [3329024/10786049 (30.9%)]	Loss: 0.069839
Train Epoch: 1 [3380224/10786049 (31.3%)]	Loss: 0.093283
Train Epoch: 1 [3431424/10786049 (31.8%)]	Loss: 0.086049
Train Epoch: 1 [3482624/10786049 (32.3%)]	Loss: 0.087106
Train Epoch: 1 [3533824/10786049 (32.8%)]	Loss: 0.075654
Train Epoch: 1 [3585024/10786049 (33.2%)]	Loss: 0.088292
Train Epoch: 1 [3636224/10786049 (33.7%)]	Loss: 0.080901
Train Epoch: 1 [3687424/10786049 (34.2%)]	Loss: 0.077419
Train Epoch: 1 [3738624/10786049 (34.7%)]	Loss: 0.089115
Train Epoch: 1 [3789824/10786049 (35.1%)]	Loss: 0.092514
Train Epoch: 1 [3841024/10786049 (35.6%)]	Loss: 0.069556
Train Epoch: 1 [3892224/10786049 (36.1%)]	Loss: 0.082148
Train Epoch: 1 [3943424/10786049 (36.6%)]	Loss: 0.099848
Train Epoch: 1 [3994624/10786049 (37.0%)]	Loss: 0.081170
Train Epoch: 1 [4045824/10786049 (37.5%)]	Loss: 0.085061
Train Epoch: 1 [4097024/10786049 (38.0%)]	Loss: 0.079310
Train Epoch: 1 [4148224/10786049 (38.5%)]	Loss: 0.071767
Train Epoch: 1 [4199424/10786049 (38.9%)]	Loss: 0.103481
Train Epoch: 1 [4250624/10786049 (39.4%)]	Loss: 0.082547
Train Epoch: 1 [4301824/10786049 (39.9%)]	Loss: 0.100110
Train Epoch: 1 [4353024/10786049 (40.4%)]	Loss: 0.093330
Train Epoch: 1 [4404224/10786049 (40.8%)]	Loss: 0.080247
Train Epoch: 1 [4455424/10786049 (41.3%)]	Loss: 0.088079
Train Epoch: 1 [4506624/10786049 (41.8%)]	Loss: 0.097365
Train Epoch: 1 [4557824/10786049 (42.3%)]	Loss: 0.083287
Train Epoch: 1 [4609024/10786049 (42.7%)]	Loss: 0.080600
Train Epoch: 1 [4660224/10786049 (43.2%)]	Loss: 0.084631
Train Epoch: 1 [4711424/10786049 (43.7%)]	Loss: 0.093216
Train Epoch: 1 [4762624/10786049 (44.2%)]	Loss: 0.073542
Train Epoch: 1 [4813824/10786049 (44.6%)]	Loss: 0.074069
Train Epoch: 1 [4865024/10786049 (45.1%)]	Loss: 0.081627
Train Epoch: 1 [4916224/10786049 (45.6%)]	Loss: 0.091235
Train Epoch: 1 [4967424/10786049 (46.1%)]	Loss: 0.101399
Train Epoch: 1 [5018624/10786049 (46.5%)]	Loss: 0.085576
Train Epoch: 1 [5069824/10786049 (47.0%)]	Loss: 0.072149
Train Epoch: 1 [5121024/10786049 (47.5%)]	Loss: 0.084864
Train Epoch: 1 [5172224/10786049 (48.0%)]	Loss: 0.087486
Train Epoch: 1 [5223424/10786049 (48.4%)]	Loss: 0.069274
Train Epoch: 1 [5274624/10786049 (48.9%)]	Loss: 0.087537
Train Epoch: 1 [5325824/10786049 (49.4%)]	Loss: 0.101384
Train Epoch: 1 [5377024/10786049 (49.9%)]	Loss: 0.093616
Train Epoch: 1 [5428224/10786049 (50.3%)]	Loss: 0.083075
Train Epoch: 1 [5479424/10786049 (50.8%)]	Loss: 0.086756
Train Epoch: 1 [5530624/10786049 (51.3%)]	Loss: 0.084143
Train Epoch: 1 [5581824/10786049 (51.8%)]	Loss: 0.094622
Train Epoch: 1 [5633024/10786049 (52.2%)]	Loss: 0.103307
Train Epoch: 1 [5684224/10786049 (52.7%)]	Loss: 0.079853
Train Epoch: 1 [5735424/10786049 (53.2%)]	Loss: 0.081534
Train Epoch: 1 [5786624/10786049 (53.6%)]	Loss: 0.077502
Train Epoch: 1 [5837824/10786049 (54.1%)]	Loss: 0.080623
Train Epoch: 1 [5889024/10786049 (54.6%)]	Loss: 0.090401
Train Epoch: 1 [5940224/10786049 (55.1%)]	Loss: 0.079357
Train Epoch: 1 [5991424/10786049 (55.5%)]	Loss: 0.078958
Train Epoch: 1 [6042624/10786049 (56.0%)]	Loss: 0.089076
Train Epoch: 1 [6093824/10786049 (56.5%)]	Loss: 0.086789
Train Epoch: 1 [6145024/10786049 (57.0%)]	Loss: 0.083370
Train Epoch: 1 [6196224/10786049 (57.4%)]	Loss: 0.086301
Train Epoch: 1 [6247424/10786049 (57.9%)]	Loss: 0.085327
Train Epoch: 1 [6298624/10786049 (58.4%)]	Loss: 0.089395
Train Epoch: 1 [6349824/10786049 (58.9%)]	Loss: 0.077081
Train Epoch: 1 [6401024/10786049 (59.3%)]	Loss: 0.083057
Train Epoch: 1 [6452224/10786049 (59.8%)]	Loss: 0.087136
Train Epoch: 1 [6503424/10786049 (60.3%)]	Loss: 0.090655
Train Epoch: 1 [6554624/10786049 (60.8%)]	Loss: 0.073505
Train Epoch: 1 [6605824/10786049 (61.2%)]	Loss: 0.069092
Train Epoch: 1 [6657024/10786049 (61.7%)]	Loss: 0.080800
Train Epoch: 1 [6708224/10786049 (62.2%)]	Loss: 0.089408
Train Epoch: 1 [6759424/10786049 (62.7%)]	Loss: 0.084695
Train Epoch: 1 [6810624/10786049 (63.1%)]	Loss: 0.080841
Train Epoch: 1 [6861824/10786049 (63.6%)]	Loss: 0.079788
Train Epoch: 1 [6913024/10786049 (64.1%)]	Loss: 0.075258
Train Epoch: 1 [6964224/10786049 (64.6%)]	Loss: 0.094830
Train Epoch: 1 [7015424/10786049 (65.0%)]	Loss: 0.075815
Train Epoch: 1 [7066624/10786049 (65.5%)]	Loss: 0.082673
Train Epoch: 1 [7117824/10786049 (66.0%)]	Loss: 0.079721
Train Epoch: 1 [7169024/10786049 (66.5%)]	Loss: 0.079185
Train Epoch: 1 [7220224/10786049 (66.9%)]	Loss: 0.079609
Train Epoch: 1 [7271424/10786049 (67.4%)]	Loss: 0.086749
Train Epoch: 1 [7322624/10786049 (67.9%)]	Loss: 0.084785
Train Epoch: 1 [7373824/10786049 (68.4%)]	Loss: 0.069324
Train Epoch: 1 [7425024/10786049 (68.8%)]	Loss: 0.082036
Train Epoch: 1 [7476224/10786049 (69.3%)]	Loss: 0.073521
Train Epoch: 1 [7527424/10786049 (69.8%)]	Loss: 0.083717
Train Epoch: 1 [7578624/10786049 (70.3%)]	Loss: 0.069706
Train Epoch: 1 [7629824/10786049 (70.7%)]	Loss: 0.078164
Train Epoch: 1 [7681024/10786049 (71.2%)]	Loss: 0.075295
Train Epoch: 1 [7732224/10786049 (71.7%)]	Loss: 0.095338
Train Epoch: 1 [7783424/10786049 (72.2%)]	Loss: 0.087456
Train Epoch: 1 [7834624/10786049 (72.6%)]	Loss: 0.091519
Train Epoch: 1 [7885824/10786049 (73.1%)]	Loss: 0.091303
Train Epoch: 1 [7937024/10786049 (73.6%)]	Loss: 0.103809
Train Epoch: 1 [7988224/10786049 (74.1%)]	Loss: 0.083831
Train Epoch: 1 [8039424/10786049 (74.5%)]	Loss: 0.087719
Train Epoch: 1 [8090624/10786049 (75.0%)]	Loss: 0.080034
Train Epoch: 1 [8141824/10786049 (75.5%)]	Loss: 0.092834
Train Epoch: 1 [8193024/10786049 (76.0%)]	Loss: 0.083079
Train Epoch: 1 [8244224/10786049 (76.4%)]	Loss: 0.084460
Train Epoch: 1 [8295424/10786049 (76.9%)]	Loss: 0.072343
Train Epoch: 1 [8346624/10786049 (77.4%)]	Loss: 0.090918
Train Epoch: 1 [8397824/10786049 (77.9%)]	Loss: 0.081148
Train Epoch: 1 [8449024/10786049 (78.3%)]	Loss: 0.074268
Train Epoch: 1 [8500224/10786049 (78.8%)]	Loss: 0.085245
Train Epoch: 1 [8551424/10786049 (79.3%)]	Loss: 0.084910
Train Epoch: 1 [8602624/10786049 (79.8%)]	Loss: 0.066942
Train Epoch: 1 [8653824/10786049 (80.2%)]	Loss: 0.079873
Train Epoch: 1 [8705024/10786049 (80.7%)]	Loss: 0.095383
Train Epoch: 1 [8756224/10786049 (81.2%)]	Loss: 0.056501
Train Epoch: 1 [8807424/10786049 (81.7%)]	Loss: 0.079648
Train Epoch: 1 [8858624/10786049 (82.1%)]	Loss: 0.086062
Train Epoch: 1 [8909824/10786049 (82.6%)]	Loss: 0.070018
Train Epoch: 1 [8961024/10786049 (83.1%)]	Loss: 0.092024
Train Epoch: 1 [9012224/10786049 (83.6%)]	Loss: 0.079407
Train Epoch: 1 [9063424/10786049 (84.0%)]	Loss: 0.096781
Train Epoch: 1 [9114624/10786049 (84.5%)]	Loss: 0.079276
Train Epoch: 1 [9165824/10786049 (85.0%)]	Loss: 0.079813
Train Epoch: 1 [9217024/10786049 (85.5%)]	Loss: 0.090246
Train Epoch: 1 [9268224/10786049 (85.9%)]	Loss: 0.084505
Train Epoch: 1 [9319424/10786049 (86.4%)]	Loss: 0.077709
Train Epoch: 1 [9370624/10786049 (86.9%)]	Loss: 0.096896
Train Epoch: 1 [9421824/10786049 (87.4%)]	Loss: 0.073919
Train Epoch: 1 [9473024/10786049 (87.8%)]	Loss: 0.088422
Train Epoch: 1 [9524224/10786049 (88.3%)]	Loss: 0.084486
Train Epoch: 1 [9575424/10786049 (88.8%)]	Loss: 0.083259
Train Epoch: 1 [9626624/10786049 (89.3%)]	Loss: 0.082476
Train Epoch: 1 [9677824/10786049 (89.7%)]	Loss: 0.084249
Train Epoch: 1 [9729024/10786049 (90.2%)]	Loss: 0.091854
Train Epoch: 1 [9780224/10786049 (90.7%)]	Loss: 0.086001
Train Epoch: 1 [9831424/10786049 (91.1%)]	Loss: 0.097207
Train Epoch: 1 [9882624/10786049 (91.6%)]	Loss: 0.090177
Train Epoch: 1 [9933824/10786049 (92.1%)]	Loss: 0.083434
Train Epoch: 1 [9985024/10786049 (92.6%)]	Loss: 0.098942
Train Epoch: 1 [10036224/10786049 (93.0%)]	Loss: 0.081056
Train Epoch: 1 [10087424/10786049 (93.5%)]	Loss: 0.074928
Train Epoch: 1 [10138624/10786049 (94.0%)]	Loss: 0.091745
Train Epoch: 1 [10189824/10786049 (94.5%)]	Loss: 0.093097
Train Epoch: 1 [10241024/10786049 (94.9%)]	Loss: 0.083405
Train Epoch: 1 [10292224/10786049 (95.4%)]	Loss: 0.092964
Train Epoch: 1 [10343424/10786049 (95.9%)]	Loss: 0.077637
Train Epoch: 1 [10394624/10786049 (96.4%)]	Loss: 0.097165
Train Epoch: 1 [10445824/10786049 (96.8%)]	Loss: 0.080484
Train Epoch: 1 [10497024/10786049 (97.3%)]	Loss: 0.096477
Train Epoch: 1 [10548224/10786049 (97.8%)]	Loss: 0.079250
Train Epoch: 1 [10599424/10786049 (98.3%)]	Loss: 0.075334
Train Epoch: 1 [10650624/10786049 (98.7%)]	Loss: 0.080575
Train Epoch: 1 [10701824/10786049 (99.2%)]	Loss: 0.085424
Train Epoch: 1 [10753024/10786049 (99.7%)]	Loss: 0.082143
Train Epoch: 2 [1024/10786049 (0.0%)]	Loss: 0.083259
Train Epoch: 2 [52224/10786049 (0.5%)]	Loss: 0.079680
Train Epoch: 2 [103424/10786049 (1.0%)]	Loss: 0.091665
Train Epoch: 2 [154624/10786049 (1.4%)]	Loss: 0.087541
Train Epoch: 2 [205824/10786049 (1.9%)]	Loss: 0.080285
Train Epoch: 2 [257024/10786049 (2.4%)]	Loss: 0.088922
Train Epoch: 2 [308224/10786049 (2.9%)]	Loss: 0.076952
Train Epoch: 2 [359424/10786049 (3.3%)]	Loss: 0.089488
Train Epoch: 2 [410624/10786049 (3.8%)]	Loss: 0.088013
Train Epoch: 2 [461824/10786049 (4.3%)]	Loss: 0.085800
Train Epoch: 2 [513024/10786049 (4.8%)]	Loss: 0.089933
Train Epoch: 2 [564224/10786049 (5.2%)]	Loss: 0.065286
Train Epoch: 2 [615424/10786049 (5.7%)]	Loss: 0.085255
Train Epoch: 2 [666624/10786049 (6.2%)]	Loss: 0.075810
Train Epoch: 2 [717824/10786049 (6.7%)]	Loss: 0.091288
Train Epoch: 2 [769024/10786049 (7.1%)]	Loss: 0.102009
Train Epoch: 2 [820224/10786049 (7.6%)]	Loss: 0.072618
Train Epoch: 2 [871424/10786049 (8.1%)]	Loss: 0.078730
Train Epoch: 2 [922624/10786049 (8.6%)]	Loss: 0.089758
Train Epoch: 2 [973824/10786049 (9.0%)]	Loss: 0.084162
Train Epoch: 2 [1025024/10786049 (9.5%)]	Loss: 0.069408
Train Epoch: 2 [1076224/10786049 (10.0%)]	Loss: 0.087810
Train Epoch: 2 [1127424/10786049 (10.5%)]	Loss: 0.085933
Train Epoch: 2 [1178624/10786049 (10.9%)]	Loss: 0.078400
Train Epoch: 2 [1229824/10786049 (11.4%)]	Loss: 0.071819
Train Epoch: 2 [1281024/10786049 (11.9%)]	Loss: 0.083681
Train Epoch: 2 [1332224/10786049 (12.4%)]	Loss: 0.079164
Train Epoch: 2 [1383424/10786049 (12.8%)]	Loss: 0.080003
Train Epoch: 2 [1434624/10786049 (13.3%)]	Loss: 0.095109
Train Epoch: 2 [1485824/10786049 (13.8%)]	Loss: 0.090349
Train Epoch: 2 [1537024/10786049 (14.3%)]	Loss: 0.088731
Train Epoch: 2 [1588224/10786049 (14.7%)]	Loss: 0.087611
Train Epoch: 2 [1639424/10786049 (15.2%)]	Loss: 0.071874
Train Epoch: 2 [1690624/10786049 (15.7%)]	Loss: 0.078195
Train Epoch: 2 [1741824/10786049 (16.1%)]	Loss: 0.087122
Train Epoch: 2 [1793024/10786049 (16.6%)]	Loss: 0.071799
Train Epoch: 2 [1844224/10786049 (17.1%)]	Loss: 0.100641
Train Epoch: 2 [1895424/10786049 (17.6%)]	Loss: 0.093077
Train Epoch: 2 [1946624/10786049 (18.0%)]	Loss: 0.095477
Train Epoch: 2 [1997824/10786049 (18.5%)]	Loss: 0.073593
Train Epoch: 2 [2049024/10786049 (19.0%)]	Loss: 0.092304
Train Epoch: 2 [2100224/10786049 (19.5%)]	Loss: 0.087105
Train Epoch: 2 [2151424/10786049 (19.9%)]	Loss: 0.079014
Train Epoch: 2 [2202624/10786049 (20.4%)]	Loss: 0.085573
Train Epoch: 2 [2253824/10786049 (20.9%)]	Loss: 0.076779
Train Epoch: 2 [2305024/10786049 (21.4%)]	Loss: 0.068752
Train Epoch: 2 [2356224/10786049 (21.8%)]	Loss: 0.078228
Train Epoch: 2 [2407424/10786049 (22.3%)]	Loss: 0.090049
Train Epoch: 2 [2458624/10786049 (22.8%)]	Loss: 0.076712
Train Epoch: 2 [2509824/10786049 (23.3%)]	Loss: 0.088134
Train Epoch: 2 [2561024/10786049 (23.7%)]	Loss: 0.087947
Train Epoch: 2 [2612224/10786049 (24.2%)]	Loss: 0.089882
Train Epoch: 2 [2663424/10786049 (24.7%)]	Loss: 0.086643
Train Epoch: 2 [2714624/10786049 (25.2%)]	Loss: 0.072938
Train Epoch: 2 [2765824/10786049 (25.6%)]	Loss: 0.079757
Train Epoch: 2 [2817024/10786049 (26.1%)]	Loss: 0.092337
Train Epoch: 2 [2868224/10786049 (26.6%)]	Loss: 0.082710
Train Epoch: 2 [2919424/10786049 (27.1%)]	Loss: 0.096091
Train Epoch: 2 [2970624/10786049 (27.5%)]	Loss: 0.069279
Train Epoch: 2 [3021824/10786049 (28.0%)]	Loss: 0.087667
Train Epoch: 2 [3073024/10786049 (28.5%)]	Loss: 0.094824
Train Epoch: 2 [3124224/10786049 (29.0%)]	Loss: 0.087434
Train Epoch: 2 [3175424/10786049 (29.4%)]	Loss: 0.093639
Train Epoch: 2 [3226624/10786049 (29.9%)]	Loss: 0.069947
Train Epoch: 2 [3277824/10786049 (30.4%)]	Loss: 0.085866
Train Epoch: 2 [3329024/10786049 (30.9%)]	Loss: 0.079862
Train Epoch: 2 [3380224/10786049 (31.3%)]	Loss: 0.097567
Train Epoch: 2 [3431424/10786049 (31.8%)]	Loss: 0.069850
Train Epoch: 2 [3482624/10786049 (32.3%)]	Loss: 0.083905
Train Epoch: 2 [3533824/10786049 (32.8%)]	Loss: 0.083436
Train Epoch: 2 [3585024/10786049 (33.2%)]	Loss: 0.080744
Train Epoch: 2 [3636224/10786049 (33.7%)]	Loss: 0.076513
Train Epoch: 2 [3687424/10786049 (34.2%)]	Loss: 0.076332
Train Epoch: 2 [3738624/10786049 (34.7%)]	Loss: 0.087313
Train Epoch: 2 [3789824/10786049 (35.1%)]	Loss: 0.107275
Train Epoch: 2 [3841024/10786049 (35.6%)]	Loss: 0.073464
Train Epoch: 2 [3892224/10786049 (36.1%)]	Loss: 0.107823
Train Epoch: 2 [3943424/10786049 (36.6%)]	Loss: 0.083197
Train Epoch: 2 [3994624/10786049 (37.0%)]	Loss: 0.097960
Train Epoch: 2 [4045824/10786049 (37.5%)]	Loss: 0.089112
Train Epoch: 2 [4097024/10786049 (38.0%)]	Loss: 0.089365
Train Epoch: 2 [4148224/10786049 (38.5%)]	Loss: 0.075730
Train Epoch: 2 [4199424/10786049 (38.9%)]	Loss: 0.087486
Train Epoch: 2 [4250624/10786049 (39.4%)]	Loss: 0.073417
Train Epoch: 2 [4301824/10786049 (39.9%)]	Loss: 0.077687
Train Epoch: 2 [4353024/10786049 (40.4%)]	Loss: 0.082348
Train Epoch: 2 [4404224/10786049 (40.8%)]	Loss: 0.085539
Train Epoch: 2 [4455424/10786049 (41.3%)]	Loss: 0.078620
Train Epoch: 2 [4506624/10786049 (41.8%)]	Loss: 0.089296
Train Epoch: 2 [4557824/10786049 (42.3%)]	Loss: 0.090799
Train Epoch: 2 [4609024/10786049 (42.7%)]	Loss: 0.084531
Train Epoch: 2 [4660224/10786049 (43.2%)]	Loss: 0.088862
Train Epoch: 2 [4711424/10786049 (43.7%)]	Loss: 0.095140
Train Epoch: 2 [4762624/10786049 (44.2%)]	Loss: 0.094625
Train Epoch: 2 [4813824/10786049 (44.6%)]	Loss: 0.078236
Train Epoch: 2 [4865024/10786049 (45.1%)]	Loss: 0.083551
Train Epoch: 2 [4916224/10786049 (45.6%)]	Loss: 0.073069
Train Epoch: 2 [4967424/10786049 (46.1%)]	Loss: 0.082674
Train Epoch: 2 [5018624/10786049 (46.5%)]	Loss: 0.094919
Train Epoch: 2 [5069824/10786049 (47.0%)]	Loss: 0.079007
Train Epoch: 2 [5121024/10786049 (47.5%)]	Loss: 0.091497
Train Epoch: 2 [5172224/10786049 (48.0%)]	Loss: 0.084495
Train Epoch: 2 [5223424/10786049 (48.4%)]	Loss: 0.088655
Train Epoch: 2 [5274624/10786049 (48.9%)]	Loss: 0.093652
Train Epoch: 2 [5325824/10786049 (49.4%)]	Loss: 0.096203
Train Epoch: 2 [5377024/10786049 (49.9%)]	Loss: 0.077503
Train Epoch: 2 [5428224/10786049 (50.3%)]	Loss: 0.075237
Train Epoch: 2 [5479424/10786049 (50.8%)]	Loss: 0.099702
Train Epoch: 2 [5530624/10786049 (51.3%)]	Loss: 0.087731
Train Epoch: 2 [5581824/10786049 (51.8%)]	Loss: 0.081100
Train Epoch: 2 [5633024/10786049 (52.2%)]	Loss: 0.083022
Train Epoch: 2 [5684224/10786049 (52.7%)]	Loss: 0.086442
Train Epoch: 2 [5735424/10786049 (53.2%)]	Loss: 0.089792
Train Epoch: 2 [5786624/10786049 (53.6%)]	Loss: 0.099626
Train Epoch: 2 [5837824/10786049 (54.1%)]	Loss: 0.067307
Train Epoch: 2 [5889024/10786049 (54.6%)]	Loss: 0.088867
Train Epoch: 2 [5940224/10786049 (55.1%)]	Loss: 0.082428
Train Epoch: 2 [5991424/10786049 (55.5%)]	Loss: 0.085319
Train Epoch: 2 [6042624/10786049 (56.0%)]	Loss: 0.094802
Train Epoch: 2 [6093824/10786049 (56.5%)]	Loss: 0.071021
Train Epoch: 2 [6145024/10786049 (57.0%)]	Loss: 0.081831
Train Epoch: 2 [6196224/10786049 (57.4%)]	Loss: 0.084446
Train Epoch: 2 [6247424/10786049 (57.9%)]	Loss: 0.095832
Train Epoch: 2 [6298624/10786049 (58.4%)]	Loss: 0.087222
Train Epoch: 2 [6349824/10786049 (58.9%)]	Loss: 0.084073
Train Epoch: 2 [6401024/10786049 (59.3%)]	Loss: 0.076578
Train Epoch: 2 [6452224/10786049 (59.8%)]	Loss: 0.083750
Train Epoch: 2 [6503424/10786049 (60.3%)]	Loss: 0.071808
Train Epoch: 2 [6554624/10786049 (60.8%)]	Loss: 0.078111
Train Epoch: 2 [6605824/10786049 (61.2%)]	Loss: 0.074944
Train Epoch: 2 [6657024/10786049 (61.7%)]	Loss: 0.065595
Train Epoch: 2 [6708224/10786049 (62.2%)]	Loss: 0.076834
Train Epoch: 2 [6759424/10786049 (62.7%)]	Loss: 0.074875
Train Epoch: 2 [6810624/10786049 (63.1%)]	Loss: 0.075870
Train Epoch: 2 [6861824/10786049 (63.6%)]	Loss: 0.082746
Train Epoch: 2 [6913024/10786049 (64.1%)]	Loss: 0.077860
Train Epoch: 2 [6964224/10786049 (64.6%)]	Loss: 0.087278
Train Epoch: 2 [7015424/10786049 (65.0%)]	Loss: 0.085061
Train Epoch: 2 [7066624/10786049 (65.5%)]	Loss: 0.075346
Train Epoch: 2 [7117824/10786049 (66.0%)]	Loss: 0.082376
Train Epoch: 2 [7169024/10786049 (66.5%)]	Loss: 0.084929
Train Epoch: 2 [7220224/10786049 (66.9%)]	Loss: 0.082692
Train Epoch: 2 [7271424/10786049 (67.4%)]	Loss: 0.077250
Train Epoch: 2 [7322624/10786049 (67.9%)]	Loss: 0.073473
Train Epoch: 2 [7373824/10786049 (68.4%)]	Loss: 0.080271
Train Epoch: 2 [7425024/10786049 (68.8%)]	Loss: 0.074268
Train Epoch: 2 [7476224/10786049 (69.3%)]	Loss: 0.084838
Train Epoch: 2 [7527424/10786049 (69.8%)]	Loss: 0.094566
Train Epoch: 2 [7578624/10786049 (70.3%)]	Loss: 0.080308
Train Epoch: 2 [7629824/10786049 (70.7%)]	Loss: 0.083287
Train Epoch: 2 [7681024/10786049 (71.2%)]	Loss: 0.076801
Train Epoch: 2 [7732224/10786049 (71.7%)]	Loss: 0.085290
Train Epoch: 2 [7783424/10786049 (72.2%)]	Loss: 0.090736
Train Epoch: 2 [7834624/10786049 (72.6%)]	Loss: 0.086312
Train Epoch: 2 [7885824/10786049 (73.1%)]	Loss: 0.082376
Train Epoch: 2 [7937024/10786049 (73.6%)]	Loss: 0.100983
Train Epoch: 2 [7988224/10786049 (74.1%)]	Loss: 0.070071
Train Epoch: 2 [8039424/10786049 (74.5%)]	Loss: 0.092312
Train Epoch: 2 [8090624/10786049 (75.0%)]	Loss: 0.084130
Train Epoch: 2 [8141824/10786049 (75.5%)]	Loss: 0.079265
Train Epoch: 2 [8193024/10786049 (76.0%)]	Loss: 0.088990
Train Epoch: 2 [8244224/10786049 (76.4%)]	Loss: 0.089278
Train Epoch: 2 [8295424/10786049 (76.9%)]	Loss: 0.082058
Train Epoch: 2 [8346624/10786049 (77.4%)]	Loss: 0.088587
Train Epoch: 2 [8397824/10786049 (77.9%)]	Loss: 0.072377
Train Epoch: 2 [8449024/10786049 (78.3%)]	Loss: 0.081981
Train Epoch: 2 [8500224/10786049 (78.8%)]	Loss: 0.096011
Train Epoch: 2 [8551424/10786049 (79.3%)]	Loss: 0.080744
Train Epoch: 2 [8602624/10786049 (79.8%)]	Loss: 0.087138
Train Epoch: 2 [8653824/10786049 (80.2%)]	Loss: 0.076264
Train Epoch: 2 [8705024/10786049 (80.7%)]	Loss: 0.093330
Train Epoch: 2 [8756224/10786049 (81.2%)]	Loss: 0.086696
Train Epoch: 2 [8807424/10786049 (81.7%)]	Loss: 0.090544
Train Epoch: 2 [8858624/10786049 (82.1%)]	Loss: 0.085943
Train Epoch: 2 [8909824/10786049 (82.6%)]	Loss: 0.087535
Train Epoch: 2 [8961024/10786049 (83.1%)]	Loss: 0.080519
Train Epoch: 2 [9012224/10786049 (83.6%)]	Loss: 0.090764
Train Epoch: 2 [9063424/10786049 (84.0%)]	Loss: 0.084036
Train Epoch: 2 [9114624/10786049 (84.5%)]	Loss: 0.070508
Train Epoch: 2 [9165824/10786049 (85.0%)]	Loss: 0.077389
Train Epoch: 2 [9217024/10786049 (85.5%)]	Loss: 0.092593
Train Epoch: 2 [9268224/10786049 (85.9%)]	Loss: 0.076720
Train Epoch: 2 [9319424/10786049 (86.4%)]	Loss: 0.086628
Train Epoch: 2 [9370624/10786049 (86.9%)]	Loss: 0.086011
Train Epoch: 2 [9421824/10786049 (87.4%)]	Loss: 0.099337
Train Epoch: 2 [9473024/10786049 (87.8%)]	Loss: 0.085010
Train Epoch: 2 [9524224/10786049 (88.3%)]	Loss: 0.083440
Train Epoch: 2 [9575424/10786049 (88.8%)]	Loss: 0.095199
Train Epoch: 2 [9626624/10786049 (89.3%)]	Loss: 0.096196
Train Epoch: 2 [9677824/10786049 (89.7%)]	Loss: 0.065539
Train Epoch: 2 [9729024/10786049 (90.2%)]	Loss: 0.092292
Train Epoch: 2 [9780224/10786049 (90.7%)]	Loss: 0.081285
Train Epoch: 2 [9831424/10786049 (91.1%)]	Loss: 0.084530
Train Epoch: 2 [9882624/10786049 (91.6%)]	Loss: 0.066477
Train Epoch: 2 [9933824/10786049 (92.1%)]	Loss: 0.091495
Train Epoch: 2 [9985024/10786049 (92.6%)]	Loss: 0.082909
Train Epoch: 2 [10036224/10786049 (93.0%)]	Loss: 0.085572
Train Epoch: 2 [10087424/10786049 (93.5%)]	Loss: 0.081087
Train Epoch: 2 [10138624/10786049 (94.0%)]	Loss: 0.090840
Train Epoch: 2 [10189824/10786049 (94.5%)]	Loss: 0.073533
Train Epoch: 2 [10241024/10786049 (94.9%)]	Loss: 0.076841
Train Epoch: 2 [10292224/10786049 (95.4%)]	Loss: 0.076209
Train Epoch: 2 [10343424/10786049 (95.9%)]	Loss: 0.076168
Train Epoch: 2 [10394624/10786049 (96.4%)]	Loss: 0.081197
Train Epoch: 2 [10445824/10786049 (96.8%)]	Loss: 0.088775
Train Epoch: 2 [10497024/10786049 (97.3%)]	Loss: 0.078582
Train Epoch: 2 [10548224/10786049 (97.8%)]	Loss: 0.071936
Train Epoch: 2 [10599424/10786049 (98.3%)]	Loss: 0.097758
Train Epoch: 2 [10650624/10786049 (98.7%)]	Loss: 0.079743
Train Epoch: 2 [10701824/10786049 (99.2%)]	Loss: 0.077575
Train Epoch: 2 [10753024/10786049 (99.7%)]	Loss: 0.085340

ACC in fold#3 was 0.887


Balanced ACC in fold#3 was 0.875


MCC in fold#3 was 0.725


Confusion Matrix in fold#3: 
           nonRipple   Ripple
nonRipple     616362   108503
Ripple        196850  1774797


Classification Report in fold#3: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.758        0.942  ...        0.850         0.893
recall            0.850        0.900  ...        0.875         0.887
f1-score          0.801        0.921  ...        0.861         0.889
sample size  724865.000  1971647.000  ...  2696512.000   2696512.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/10786049 (0.0%)]	Loss: 0.353357
Train Epoch: 1 [52224/10786049 (0.5%)]	Loss: 0.131164
Train Epoch: 1 [103424/10786049 (1.0%)]	Loss: 0.108369
Train Epoch: 1 [154624/10786049 (1.4%)]	Loss: 0.098609
Train Epoch: 1 [205824/10786049 (1.9%)]	Loss: 0.109372
Train Epoch: 1 [257024/10786049 (2.4%)]	Loss: 0.122214
Train Epoch: 1 [308224/10786049 (2.9%)]	Loss: 0.112304
Train Epoch: 1 [359424/10786049 (3.3%)]	Loss: 0.101366
Train Epoch: 1 [410624/10786049 (3.8%)]	Loss: 0.098720
Train Epoch: 1 [461824/10786049 (4.3%)]	Loss: 0.099329
Train Epoch: 1 [513024/10786049 (4.8%)]	Loss: 0.096957
Train Epoch: 1 [564224/10786049 (5.2%)]	Loss: 0.121523
Train Epoch: 1 [615424/10786049 (5.7%)]	Loss: 0.116188
Train Epoch: 1 [666624/10786049 (6.2%)]	Loss: 0.084684
Train Epoch: 1 [717824/10786049 (6.7%)]	Loss: 0.089612
Train Epoch: 1 [769024/10786049 (7.1%)]	Loss: 0.104838
Train Epoch: 1 [820224/10786049 (7.6%)]	Loss: 0.109314
Train Epoch: 1 [871424/10786049 (8.1%)]	Loss: 0.115700
Train Epoch: 1 [922624/10786049 (8.6%)]	Loss: 0.109062
Train Epoch: 1 [973824/10786049 (9.0%)]	Loss: 0.105699
Train Epoch: 1 [1025024/10786049 (9.5%)]	Loss: 0.104501
Train Epoch: 1 [1076224/10786049 (10.0%)]	Loss: 0.099595
Train Epoch: 1 [1127424/10786049 (10.5%)]	Loss: 0.105471
Train Epoch: 1 [1178624/10786049 (10.9%)]	Loss: 0.086049
Train Epoch: 1 [1229824/10786049 (11.4%)]	Loss: 0.093806
Train Epoch: 1 [1281024/10786049 (11.9%)]	Loss: 0.099561
Train Epoch: 1 [1332224/10786049 (12.4%)]	Loss: 0.090981
Train Epoch: 1 [1383424/10786049 (12.8%)]	Loss: 0.089646
Train Epoch: 1 [1434624/10786049 (13.3%)]	Loss: 0.097626
Train Epoch: 1 [1485824/10786049 (13.8%)]	Loss: 0.104159
Train Epoch: 1 [1537024/10786049 (14.3%)]	Loss: 0.105715
Train Epoch: 1 [1588224/10786049 (14.7%)]	Loss: 0.094958
Train Epoch: 1 [1639424/10786049 (15.2%)]	Loss: 0.115437
Train Epoch: 1 [1690624/10786049 (15.7%)]	Loss: 0.095950
Train Epoch: 1 [1741824/10786049 (16.1%)]	Loss: 0.101276
Train Epoch: 1 [1793024/10786049 (16.6%)]	Loss: 0.101776
Train Epoch: 1 [1844224/10786049 (17.1%)]	Loss: 0.087391
Train Epoch: 1 [1895424/10786049 (17.6%)]	Loss: 0.109628
Train Epoch: 1 [1946624/10786049 (18.0%)]	Loss: 0.088425
Train Epoch: 1 [1997824/10786049 (18.5%)]	Loss: 0.096644
Train Epoch: 1 [2049024/10786049 (19.0%)]	Loss: 0.101302
Train Epoch: 1 [2100224/10786049 (19.5%)]	Loss: 0.104212
Train Epoch: 1 [2151424/10786049 (19.9%)]	Loss: 0.106256
Train Epoch: 1 [2202624/10786049 (20.4%)]	Loss: 0.102271
Train Epoch: 1 [2253824/10786049 (20.9%)]	Loss: 0.102082
Train Epoch: 1 [2305024/10786049 (21.4%)]	Loss: 0.100881
Train Epoch: 1 [2356224/10786049 (21.8%)]	Loss: 0.090084
Train Epoch: 1 [2407424/10786049 (22.3%)]	Loss: 0.101154
Train Epoch: 1 [2458624/10786049 (22.8%)]	Loss: 0.105468
Train Epoch: 1 [2509824/10786049 (23.3%)]	Loss: 0.084550
Train Epoch: 1 [2561024/10786049 (23.7%)]	Loss: 0.095465
Train Epoch: 1 [2612224/10786049 (24.2%)]	Loss: 0.104970
Train Epoch: 1 [2663424/10786049 (24.7%)]	Loss: 0.101188
Train Epoch: 1 [2714624/10786049 (25.2%)]	Loss: 0.092618
Train Epoch: 1 [2765824/10786049 (25.6%)]	Loss: 0.105231
Train Epoch: 1 [2817024/10786049 (26.1%)]	Loss: 0.100541
Train Epoch: 1 [2868224/10786049 (26.6%)]	Loss: 0.100857
Train Epoch: 1 [2919424/10786049 (27.1%)]	Loss: 0.091874
Train Epoch: 1 [2970624/10786049 (27.5%)]	Loss: 0.091650
Train Epoch: 1 [3021824/10786049 (28.0%)]	Loss: 0.106927
Train Epoch: 1 [3073024/10786049 (28.5%)]	Loss: 0.107355
Train Epoch: 1 [3124224/10786049 (29.0%)]	Loss: 0.087269
Train Epoch: 1 [3175424/10786049 (29.4%)]	Loss: 0.104135
Train Epoch: 1 [3226624/10786049 (29.9%)]	Loss: 0.097943
Train Epoch: 1 [3277824/10786049 (30.4%)]	Loss: 0.100548
Train Epoch: 1 [3329024/10786049 (30.9%)]	Loss: 0.110047
Train Epoch: 1 [3380224/10786049 (31.3%)]	Loss: 0.081129
Train Epoch: 1 [3431424/10786049 (31.8%)]	Loss: 0.105206
Train Epoch: 1 [3482624/10786049 (32.3%)]	Loss: 0.115719
Train Epoch: 1 [3533824/10786049 (32.8%)]	Loss: 0.095431
Train Epoch: 1 [3585024/10786049 (33.2%)]	Loss: 0.085902
Train Epoch: 1 [3636224/10786049 (33.7%)]	Loss: 0.085207
Train Epoch: 1 [3687424/10786049 (34.2%)]	Loss: 0.112600
Train Epoch: 1 [3738624/10786049 (34.7%)]	Loss: 0.101254
Train Epoch: 1 [3789824/10786049 (35.1%)]	Loss: 0.095507
Train Epoch: 1 [3841024/10786049 (35.6%)]	Loss: 0.090453
Train Epoch: 1 [3892224/10786049 (36.1%)]	Loss: 0.107528
Train Epoch: 1 [3943424/10786049 (36.6%)]	Loss: 0.086331
Train Epoch: 1 [3994624/10786049 (37.0%)]	Loss: 0.106135
Train Epoch: 1 [4045824/10786049 (37.5%)]	Loss: 0.092558
Train Epoch: 1 [4097024/10786049 (38.0%)]	Loss: 0.094953
Train Epoch: 1 [4148224/10786049 (38.5%)]	Loss: 0.100774
Train Epoch: 1 [4199424/10786049 (38.9%)]	Loss: 0.099534
Train Epoch: 1 [4250624/10786049 (39.4%)]	Loss: 0.094766
Train Epoch: 1 [4301824/10786049 (39.9%)]	Loss: 0.098803
Train Epoch: 1 [4353024/10786049 (40.4%)]	Loss: 0.095467
Train Epoch: 1 [4404224/10786049 (40.8%)]	Loss: 0.094281
Train Epoch: 1 [4455424/10786049 (41.3%)]	Loss: 0.082851
Train Epoch: 1 [4506624/10786049 (41.8%)]	Loss: 0.113050
Train Epoch: 1 [4557824/10786049 (42.3%)]	Loss: 0.082921
Train Epoch: 1 [4609024/10786049 (42.7%)]	Loss: 0.105071
Train Epoch: 1 [4660224/10786049 (43.2%)]	Loss: 0.091670
Train Epoch: 1 [4711424/10786049 (43.7%)]	Loss: 0.102344
Train Epoch: 1 [4762624/10786049 (44.2%)]	Loss: 0.091847
Train Epoch: 1 [4813824/10786049 (44.6%)]	Loss: 0.113764
Train Epoch: 1 [4865024/10786049 (45.1%)]	Loss: 0.099477
Train Epoch: 1 [4916224/10786049 (45.6%)]	Loss: 0.091773
Train Epoch: 1 [4967424/10786049 (46.1%)]	Loss: 0.095358
Train Epoch: 1 [5018624/10786049 (46.5%)]	Loss: 0.096137
Train Epoch: 1 [5069824/10786049 (47.0%)]	Loss: 0.095981
Train Epoch: 1 [5121024/10786049 (47.5%)]	Loss: 0.100031
Train Epoch: 1 [5172224/10786049 (48.0%)]	Loss: 0.107191
Train Epoch: 1 [5223424/10786049 (48.4%)]	Loss: 0.082141
Train Epoch: 1 [5274624/10786049 (48.9%)]	Loss: 0.099051
Train Epoch: 1 [5325824/10786049 (49.4%)]	Loss: 0.097573
Train Epoch: 1 [5377024/10786049 (49.9%)]	Loss: 0.088158
Train Epoch: 1 [5428224/10786049 (50.3%)]	Loss: 0.089608
Train Epoch: 1 [5479424/10786049 (50.8%)]	Loss: 0.103340
Train Epoch: 1 [5530624/10786049 (51.3%)]	Loss: 0.114874
Train Epoch: 1 [5581824/10786049 (51.8%)]	Loss: 0.106832
Train Epoch: 1 [5633024/10786049 (52.2%)]	Loss: 0.089517
Train Epoch: 1 [5684224/10786049 (52.7%)]	Loss: 0.083140
Train Epoch: 1 [5735424/10786049 (53.2%)]	Loss: 0.084211
Train Epoch: 1 [5786624/10786049 (53.6%)]	Loss: 0.089686
Train Epoch: 1 [5837824/10786049 (54.1%)]	Loss: 0.108028
Train Epoch: 1 [5889024/10786049 (54.6%)]	Loss: 0.088379
Train Epoch: 1 [5940224/10786049 (55.1%)]	Loss: 0.087455
Train Epoch: 1 [5991424/10786049 (55.5%)]	Loss: 0.088109
Train Epoch: 1 [6042624/10786049 (56.0%)]	Loss: 0.102289
Train Epoch: 1 [6093824/10786049 (56.5%)]	Loss: 0.111631
Train Epoch: 1 [6145024/10786049 (57.0%)]	Loss: 0.096248
Train Epoch: 1 [6196224/10786049 (57.4%)]	Loss: 0.097180
Train Epoch: 1 [6247424/10786049 (57.9%)]	Loss: 0.103934
Train Epoch: 1 [6298624/10786049 (58.4%)]	Loss: 0.093451
Train Epoch: 1 [6349824/10786049 (58.9%)]	Loss: 0.089235
Train Epoch: 1 [6401024/10786049 (59.3%)]	Loss: 0.099683
Train Epoch: 1 [6452224/10786049 (59.8%)]	Loss: 0.091925
Train Epoch: 1 [6503424/10786049 (60.3%)]	Loss: 0.098991
Train Epoch: 1 [6554624/10786049 (60.8%)]	Loss: 0.114464
Train Epoch: 1 [6605824/10786049 (61.2%)]	Loss: 0.093254
Train Epoch: 1 [6657024/10786049 (61.7%)]	Loss: 0.107148
Train Epoch: 1 [6708224/10786049 (62.2%)]	Loss: 0.095745
Train Epoch: 1 [6759424/10786049 (62.7%)]	Loss: 0.099347
Train Epoch: 1 [6810624/10786049 (63.1%)]	Loss: 0.092958
Train Epoch: 1 [6861824/10786049 (63.6%)]	Loss: 0.092035
Train Epoch: 1 [6913024/10786049 (64.1%)]	Loss: 0.093064
Train Epoch: 1 [6964224/10786049 (64.6%)]	Loss: 0.081738
Train Epoch: 1 [7015424/10786049 (65.0%)]	Loss: 0.081046
Train Epoch: 1 [7066624/10786049 (65.5%)]	Loss: 0.096710
Train Epoch: 1 [7117824/10786049 (66.0%)]	Loss: 0.082654
Train Epoch: 1 [7169024/10786049 (66.5%)]	Loss: 0.088918
Train Epoch: 1 [7220224/10786049 (66.9%)]	Loss: 0.094825
Train Epoch: 1 [7271424/10786049 (67.4%)]	Loss: 0.089133
Train Epoch: 1 [7322624/10786049 (67.9%)]	Loss: 0.089157
Train Epoch: 1 [7373824/10786049 (68.4%)]	Loss: 0.088367
Train Epoch: 1 [7425024/10786049 (68.8%)]	Loss: 0.089537
Train Epoch: 1 [7476224/10786049 (69.3%)]	Loss: 0.122155
Train Epoch: 1 [7527424/10786049 (69.8%)]	Loss: 0.095000
Train Epoch: 1 [7578624/10786049 (70.3%)]	Loss: 0.084503
Train Epoch: 1 [7629824/10786049 (70.7%)]	Loss: 0.110390
Train Epoch: 1 [7681024/10786049 (71.2%)]	Loss: 0.077845
Train Epoch: 1 [7732224/10786049 (71.7%)]	Loss: 0.095769
Train Epoch: 1 [7783424/10786049 (72.2%)]	Loss: 0.082937
Train Epoch: 1 [7834624/10786049 (72.6%)]	Loss: 0.113448
Train Epoch: 1 [7885824/10786049 (73.1%)]	Loss: 0.081985
Train Epoch: 1 [7937024/10786049 (73.6%)]	Loss: 0.089145
Train Epoch: 1 [7988224/10786049 (74.1%)]	Loss: 0.075104
Train Epoch: 1 [8039424/10786049 (74.5%)]	Loss: 0.089578
Train Epoch: 1 [8090624/10786049 (75.0%)]	Loss: 0.099090
Train Epoch: 1 [8141824/10786049 (75.5%)]	Loss: 0.090888
Train Epoch: 1 [8193024/10786049 (76.0%)]	Loss: 0.095229
Train Epoch: 1 [8244224/10786049 (76.4%)]	Loss: 0.100048
Train Epoch: 1 [8295424/10786049 (76.9%)]	Loss: 0.089433
Train Epoch: 1 [8346624/10786049 (77.4%)]	Loss: 0.076183
Train Epoch: 1 [8397824/10786049 (77.9%)]	Loss: 0.104393
Train Epoch: 1 [8449024/10786049 (78.3%)]	Loss: 0.092050
Train Epoch: 1 [8500224/10786049 (78.8%)]	Loss: 0.092946
Train Epoch: 1 [8551424/10786049 (79.3%)]	Loss: 0.106840
Train Epoch: 1 [8602624/10786049 (79.8%)]	Loss: 0.100500
Train Epoch: 1 [8653824/10786049 (80.2%)]	Loss: 0.095608
Train Epoch: 1 [8705024/10786049 (80.7%)]	Loss: 0.081667
Train Epoch: 1 [8756224/10786049 (81.2%)]	Loss: 0.097075
Train Epoch: 1 [8807424/10786049 (81.7%)]	Loss: 0.098373
Train Epoch: 1 [8858624/10786049 (82.1%)]	Loss: 0.093047
Train Epoch: 1 [8909824/10786049 (82.6%)]	Loss: 0.091709
Train Epoch: 1 [8961024/10786049 (83.1%)]	Loss: 0.088442
Train Epoch: 1 [9012224/10786049 (83.6%)]	Loss: 0.096321
Train Epoch: 1 [9063424/10786049 (84.0%)]	Loss: 0.084071
Train Epoch: 1 [9114624/10786049 (84.5%)]	Loss: 0.111492
Train Epoch: 1 [9165824/10786049 (85.0%)]	Loss: 0.083232
Train Epoch: 1 [9217024/10786049 (85.5%)]	Loss: 0.103424
Train Epoch: 1 [9268224/10786049 (85.9%)]	Loss: 0.094415
Train Epoch: 1 [9319424/10786049 (86.4%)]	Loss: 0.108290
Train Epoch: 1 [9370624/10786049 (86.9%)]	Loss: 0.082848
Train Epoch: 1 [9421824/10786049 (87.4%)]	Loss: 0.095796
Train Epoch: 1 [9473024/10786049 (87.8%)]	Loss: 0.089379
Train Epoch: 1 [9524224/10786049 (88.3%)]	Loss: 0.080321
Train Epoch: 1 [9575424/10786049 (88.8%)]	Loss: 0.093440
Train Epoch: 1 [9626624/10786049 (89.3%)]	Loss: 0.111282
Train Epoch: 1 [9677824/10786049 (89.7%)]	Loss: 0.092303
Train Epoch: 1 [9729024/10786049 (90.2%)]	Loss: 0.093388
Train Epoch: 1 [9780224/10786049 (90.7%)]	Loss: 0.093761
Train Epoch: 1 [9831424/10786049 (91.1%)]	Loss: 0.100493
Train Epoch: 1 [9882624/10786049 (91.6%)]	Loss: 0.089073
Train Epoch: 1 [9933824/10786049 (92.1%)]	Loss: 0.087594
Train Epoch: 1 [9985024/10786049 (92.6%)]	Loss: 0.103308
Train Epoch: 1 [10036224/10786049 (93.0%)]	Loss: 0.100960
Train Epoch: 1 [10087424/10786049 (93.5%)]	Loss: 0.085189
Train Epoch: 1 [10138624/10786049 (94.0%)]	Loss: 0.093751
Train Epoch: 1 [10189824/10786049 (94.5%)]	Loss: 0.078886
Train Epoch: 1 [10241024/10786049 (94.9%)]	Loss: 0.079252
Train Epoch: 1 [10292224/10786049 (95.4%)]	Loss: 0.078526
Train Epoch: 1 [10343424/10786049 (95.9%)]	Loss: 0.100645
Train Epoch: 1 [10394624/10786049 (96.4%)]	Loss: 0.082065
Train Epoch: 1 [10445824/10786049 (96.8%)]	Loss: 0.088369
Train Epoch: 1 [10497024/10786049 (97.3%)]	Loss: 0.082991
Train Epoch: 1 [10548224/10786049 (97.8%)]	Loss: 0.089519
Train Epoch: 1 [10599424/10786049 (98.3%)]	Loss: 0.081984
Train Epoch: 1 [10650624/10786049 (98.7%)]	Loss: 0.092030
Train Epoch: 1 [10701824/10786049 (99.2%)]	Loss: 0.095300
Train Epoch: 1 [10753024/10786049 (99.7%)]	Loss: 0.088969
Train Epoch: 2 [1024/10786049 (0.0%)]	Loss: 0.081474
Train Epoch: 2 [52224/10786049 (0.5%)]	Loss: 0.091216
Train Epoch: 2 [103424/10786049 (1.0%)]	Loss: 0.095944
Train Epoch: 2 [154624/10786049 (1.4%)]	Loss: 0.083664
Train Epoch: 2 [205824/10786049 (1.9%)]	Loss: 0.102737
Train Epoch: 2 [257024/10786049 (2.4%)]	Loss: 0.089435
Train Epoch: 2 [308224/10786049 (2.9%)]	Loss: 0.083851
Train Epoch: 2 [359424/10786049 (3.3%)]	Loss: 0.086029
Train Epoch: 2 [410624/10786049 (3.8%)]	Loss: 0.093668
Train Epoch: 2 [461824/10786049 (4.3%)]	Loss: 0.093355
Train Epoch: 2 [513024/10786049 (4.8%)]	Loss: 0.095726
Train Epoch: 2 [564224/10786049 (5.2%)]	Loss: 0.094063
Train Epoch: 2 [615424/10786049 (5.7%)]	Loss: 0.111341
Train Epoch: 2 [666624/10786049 (6.2%)]	Loss: 0.092650
Train Epoch: 2 [717824/10786049 (6.7%)]	Loss: 0.088378
Train Epoch: 2 [769024/10786049 (7.1%)]	Loss: 0.100265
Train Epoch: 2 [820224/10786049 (7.6%)]	Loss: 0.082852
Train Epoch: 2 [871424/10786049 (8.1%)]	Loss: 0.091870
Train Epoch: 2 [922624/10786049 (8.6%)]	Loss: 0.089082
Train Epoch: 2 [973824/10786049 (9.0%)]	Loss: 0.088400
Train Epoch: 2 [1025024/10786049 (9.5%)]	Loss: 0.097685
Train Epoch: 2 [1076224/10786049 (10.0%)]	Loss: 0.087153
Train Epoch: 2 [1127424/10786049 (10.5%)]	Loss: 0.109088
Train Epoch: 2 [1178624/10786049 (10.9%)]	Loss: 0.088540
Train Epoch: 2 [1229824/10786049 (11.4%)]	Loss: 0.096975
Train Epoch: 2 [1281024/10786049 (11.9%)]	Loss: 0.089731
Train Epoch: 2 [1332224/10786049 (12.4%)]	Loss: 0.107632
Train Epoch: 2 [1383424/10786049 (12.8%)]	Loss: 0.085904
Train Epoch: 2 [1434624/10786049 (13.3%)]	Loss: 0.078212
Train Epoch: 2 [1485824/10786049 (13.8%)]	Loss: 0.090782
Train Epoch: 2 [1537024/10786049 (14.3%)]	Loss: 0.088140
Train Epoch: 2 [1588224/10786049 (14.7%)]	Loss: 0.085873
Train Epoch: 2 [1639424/10786049 (15.2%)]	Loss: 0.089867
Train Epoch: 2 [1690624/10786049 (15.7%)]	Loss: 0.100053
Train Epoch: 2 [1741824/10786049 (16.1%)]	Loss: 0.079687
Train Epoch: 2 [1793024/10786049 (16.6%)]	Loss: 0.086021
Train Epoch: 2 [1844224/10786049 (17.1%)]	Loss: 0.103446
Train Epoch: 2 [1895424/10786049 (17.6%)]	Loss: 0.087884
Train Epoch: 2 [1946624/10786049 (18.0%)]	Loss: 0.099599
Train Epoch: 2 [1997824/10786049 (18.5%)]	Loss: 0.085503
Train Epoch: 2 [2049024/10786049 (19.0%)]	Loss: 0.106423
Train Epoch: 2 [2100224/10786049 (19.5%)]	Loss: 0.084186
Train Epoch: 2 [2151424/10786049 (19.9%)]	Loss: 0.084469
Train Epoch: 2 [2202624/10786049 (20.4%)]	Loss: 0.088609
Train Epoch: 2 [2253824/10786049 (20.9%)]	Loss: 0.087488
Train Epoch: 2 [2305024/10786049 (21.4%)]	Loss: 0.102861
Train Epoch: 2 [2356224/10786049 (21.8%)]	Loss: 0.087873
Train Epoch: 2 [2407424/10786049 (22.3%)]	Loss: 0.091490
Train Epoch: 2 [2458624/10786049 (22.8%)]	Loss: 0.092272
Train Epoch: 2 [2509824/10786049 (23.3%)]	Loss: 0.080007
Train Epoch: 2 [2561024/10786049 (23.7%)]	Loss: 0.074652
Train Epoch: 2 [2612224/10786049 (24.2%)]	Loss: 0.107032
Train Epoch: 2 [2663424/10786049 (24.7%)]	Loss: 0.081100
Train Epoch: 2 [2714624/10786049 (25.2%)]	Loss: 0.082075
Train Epoch: 2 [2765824/10786049 (25.6%)]	Loss: 0.096134
Train Epoch: 2 [2817024/10786049 (26.1%)]	Loss: 0.108078
Train Epoch: 2 [2868224/10786049 (26.6%)]	Loss: 0.101152
Train Epoch: 2 [2919424/10786049 (27.1%)]	Loss: 0.086374
Train Epoch: 2 [2970624/10786049 (27.5%)]	Loss: 0.096056
Train Epoch: 2 [3021824/10786049 (28.0%)]	Loss: 0.084596
Train Epoch: 2 [3073024/10786049 (28.5%)]	Loss: 0.082517
Train Epoch: 2 [3124224/10786049 (29.0%)]	Loss: 0.096889
Train Epoch: 2 [3175424/10786049 (29.4%)]	Loss: 0.100186
Train Epoch: 2 [3226624/10786049 (29.9%)]	Loss: 0.090879
Train Epoch: 2 [3277824/10786049 (30.4%)]	Loss: 0.102073
Train Epoch: 2 [3329024/10786049 (30.9%)]	Loss: 0.086930
Train Epoch: 2 [3380224/10786049 (31.3%)]	Loss: 0.094481
Train Epoch: 2 [3431424/10786049 (31.8%)]	Loss: 0.085936
Train Epoch: 2 [3482624/10786049 (32.3%)]	Loss: 0.080100
Train Epoch: 2 [3533824/10786049 (32.8%)]	Loss: 0.092849
Train Epoch: 2 [3585024/10786049 (33.2%)]	Loss: 0.089978
Train Epoch: 2 [3636224/10786049 (33.7%)]	Loss: 0.093876
Train Epoch: 2 [3687424/10786049 (34.2%)]	Loss: 0.078247
Train Epoch: 2 [3738624/10786049 (34.7%)]	Loss: 0.097048
Train Epoch: 2 [3789824/10786049 (35.1%)]	Loss: 0.088714
Train Epoch: 2 [3841024/10786049 (35.6%)]	Loss: 0.088446
Train Epoch: 2 [3892224/10786049 (36.1%)]	Loss: 0.093652
Train Epoch: 2 [3943424/10786049 (36.6%)]	Loss: 0.083817
Train Epoch: 2 [3994624/10786049 (37.0%)]	Loss: 0.079427
Train Epoch: 2 [4045824/10786049 (37.5%)]	Loss: 0.091115
Train Epoch: 2 [4097024/10786049 (38.0%)]	Loss: 0.089745
Train Epoch: 2 [4148224/10786049 (38.5%)]	Loss: 0.096349
Train Epoch: 2 [4199424/10786049 (38.9%)]	Loss: 0.089808
Train Epoch: 2 [4250624/10786049 (39.4%)]	Loss: 0.084177
Train Epoch: 2 [4301824/10786049 (39.9%)]	Loss: 0.104819
Train Epoch: 2 [4353024/10786049 (40.4%)]	Loss: 0.080697
Train Epoch: 2 [4404224/10786049 (40.8%)]	Loss: 0.089156
Train Epoch: 2 [4455424/10786049 (41.3%)]	Loss: 0.088354
Train Epoch: 2 [4506624/10786049 (41.8%)]	Loss: 0.093606
Train Epoch: 2 [4557824/10786049 (42.3%)]	Loss: 0.087691
Train Epoch: 2 [4609024/10786049 (42.7%)]	Loss: 0.083962
Train Epoch: 2 [4660224/10786049 (43.2%)]	Loss: 0.095333
Train Epoch: 2 [4711424/10786049 (43.7%)]	Loss: 0.074959
Train Epoch: 2 [4762624/10786049 (44.2%)]	Loss: 0.101606
Train Epoch: 2 [4813824/10786049 (44.6%)]	Loss: 0.083630
Train Epoch: 2 [4865024/10786049 (45.1%)]	Loss: 0.089966
Train Epoch: 2 [4916224/10786049 (45.6%)]	Loss: 0.090553
Train Epoch: 2 [4967424/10786049 (46.1%)]	Loss: 0.091277
Train Epoch: 2 [5018624/10786049 (46.5%)]	Loss: 0.089296
Train Epoch: 2 [5069824/10786049 (47.0%)]	Loss: 0.089975
Train Epoch: 2 [5121024/10786049 (47.5%)]	Loss: 0.087528
Train Epoch: 2 [5172224/10786049 (48.0%)]	Loss: 0.085033
Train Epoch: 2 [5223424/10786049 (48.4%)]	Loss: 0.085404
Train Epoch: 2 [5274624/10786049 (48.9%)]	Loss: 0.093787
Train Epoch: 2 [5325824/10786049 (49.4%)]	Loss: 0.079559
Train Epoch: 2 [5377024/10786049 (49.9%)]	Loss: 0.095668
Train Epoch: 2 [5428224/10786049 (50.3%)]	Loss: 0.090686
Train Epoch: 2 [5479424/10786049 (50.8%)]	Loss: 0.096033
Train Epoch: 2 [5530624/10786049 (51.3%)]	Loss: 0.089055
Train Epoch: 2 [5581824/10786049 (51.8%)]	Loss: 0.092239
Train Epoch: 2 [5633024/10786049 (52.2%)]	Loss: 0.094993
Train Epoch: 2 [5684224/10786049 (52.7%)]	Loss: 0.089856
Train Epoch: 2 [5735424/10786049 (53.2%)]	Loss: 0.102617
Train Epoch: 2 [5786624/10786049 (53.6%)]	Loss: 0.085000
Train Epoch: 2 [5837824/10786049 (54.1%)]	Loss: 0.088245
Train Epoch: 2 [5889024/10786049 (54.6%)]	Loss: 0.085071
Train Epoch: 2 [5940224/10786049 (55.1%)]	Loss: 0.086470
Train Epoch: 2 [5991424/10786049 (55.5%)]	Loss: 0.097673
Train Epoch: 2 [6042624/10786049 (56.0%)]	Loss: 0.095936
Train Epoch: 2 [6093824/10786049 (56.5%)]	Loss: 0.109291
Train Epoch: 2 [6145024/10786049 (57.0%)]	Loss: 0.089126
Train Epoch: 2 [6196224/10786049 (57.4%)]	Loss: 0.085307
Train Epoch: 2 [6247424/10786049 (57.9%)]	Loss: 0.085612
Train Epoch: 2 [6298624/10786049 (58.4%)]	Loss: 0.079930
Train Epoch: 2 [6349824/10786049 (58.9%)]	Loss: 0.082549
Train Epoch: 2 [6401024/10786049 (59.3%)]	Loss: 0.098020
Train Epoch: 2 [6452224/10786049 (59.8%)]	Loss: 0.094451
Train Epoch: 2 [6503424/10786049 (60.3%)]	Loss: 0.103984
Train Epoch: 2 [6554624/10786049 (60.8%)]	Loss: 0.094096
Train Epoch: 2 [6605824/10786049 (61.2%)]	Loss: 0.094349
Train Epoch: 2 [6657024/10786049 (61.7%)]	Loss: 0.099022
Train Epoch: 2 [6708224/10786049 (62.2%)]	Loss: 0.079507
Train Epoch: 2 [6759424/10786049 (62.7%)]	Loss: 0.079157
Train Epoch: 2 [6810624/10786049 (63.1%)]	Loss: 0.101575
Train Epoch: 2 [6861824/10786049 (63.6%)]	Loss: 0.087365
Train Epoch: 2 [6913024/10786049 (64.1%)]	Loss: 0.099815
Train Epoch: 2 [6964224/10786049 (64.6%)]	Loss: 0.087396
Train Epoch: 2 [7015424/10786049 (65.0%)]	Loss: 0.086718
Train Epoch: 2 [7066624/10786049 (65.5%)]	Loss: 0.086908
Train Epoch: 2 [7117824/10786049 (66.0%)]	Loss: 0.096770
Train Epoch: 2 [7169024/10786049 (66.5%)]	Loss: 0.094369
Train Epoch: 2 [7220224/10786049 (66.9%)]	Loss: 0.095013
Train Epoch: 2 [7271424/10786049 (67.4%)]	Loss: 0.093316
Train Epoch: 2 [7322624/10786049 (67.9%)]	Loss: 0.096818
Train Epoch: 2 [7373824/10786049 (68.4%)]	Loss: 0.093509
Train Epoch: 2 [7425024/10786049 (68.8%)]	Loss: 0.071688
Train Epoch: 2 [7476224/10786049 (69.3%)]	Loss: 0.090008
Train Epoch: 2 [7527424/10786049 (69.8%)]	Loss: 0.083936
Train Epoch: 2 [7578624/10786049 (70.3%)]	Loss: 0.095882
Train Epoch: 2 [7629824/10786049 (70.7%)]	Loss: 0.082677
Train Epoch: 2 [7681024/10786049 (71.2%)]	Loss: 0.092337
Train Epoch: 2 [7732224/10786049 (71.7%)]	Loss: 0.098750
Train Epoch: 2 [7783424/10786049 (72.2%)]	Loss: 0.095425
Train Epoch: 2 [7834624/10786049 (72.6%)]	Loss: 0.085044
Train Epoch: 2 [7885824/10786049 (73.1%)]	Loss: 0.089279
Train Epoch: 2 [7937024/10786049 (73.6%)]	Loss: 0.094476
Train Epoch: 2 [7988224/10786049 (74.1%)]	Loss: 0.092629
Train Epoch: 2 [8039424/10786049 (74.5%)]	Loss: 0.105115
Train Epoch: 2 [8090624/10786049 (75.0%)]	Loss: 0.087532
Train Epoch: 2 [8141824/10786049 (75.5%)]	Loss: 0.085672
Train Epoch: 2 [8193024/10786049 (76.0%)]	Loss: 0.082501
Train Epoch: 2 [8244224/10786049 (76.4%)]	Loss: 0.099973
Train Epoch: 2 [8295424/10786049 (76.9%)]	Loss: 0.085108
Train Epoch: 2 [8346624/10786049 (77.4%)]	Loss: 0.094048
Train Epoch: 2 [8397824/10786049 (77.9%)]	Loss: 0.091120
Train Epoch: 2 [8449024/10786049 (78.3%)]	Loss: 0.083008
Train Epoch: 2 [8500224/10786049 (78.8%)]	Loss: 0.081630
Train Epoch: 2 [8551424/10786049 (79.3%)]	Loss: 0.095580
Train Epoch: 2 [8602624/10786049 (79.8%)]	Loss: 0.092864
Train Epoch: 2 [8653824/10786049 (80.2%)]	Loss: 0.083576
Train Epoch: 2 [8705024/10786049 (80.7%)]	Loss: 0.075900
Train Epoch: 2 [8756224/10786049 (81.2%)]	Loss: 0.094658
Train Epoch: 2 [8807424/10786049 (81.7%)]	Loss: 0.102367
Train Epoch: 2 [8858624/10786049 (82.1%)]	Loss: 0.080536
Train Epoch: 2 [8909824/10786049 (82.6%)]	Loss: 0.088731
Train Epoch: 2 [8961024/10786049 (83.1%)]	Loss: 0.082477
Train Epoch: 2 [9012224/10786049 (83.6%)]	Loss: 0.086957
Train Epoch: 2 [9063424/10786049 (84.0%)]	Loss: 0.093046
Train Epoch: 2 [9114624/10786049 (84.5%)]	Loss: 0.093009
Train Epoch: 2 [9165824/10786049 (85.0%)]	Loss: 0.095499
Train Epoch: 2 [9217024/10786049 (85.5%)]	Loss: 0.080093
Train Epoch: 2 [9268224/10786049 (85.9%)]	Loss: 0.104799
Train Epoch: 2 [9319424/10786049 (86.4%)]	Loss: 0.106447
Train Epoch: 2 [9370624/10786049 (86.9%)]	Loss: 0.088082
Train Epoch: 2 [9421824/10786049 (87.4%)]	Loss: 0.089099
Train Epoch: 2 [9473024/10786049 (87.8%)]	Loss: 0.094092
Train Epoch: 2 [9524224/10786049 (88.3%)]	Loss: 0.095133
Train Epoch: 2 [9575424/10786049 (88.8%)]	Loss: 0.083067
Train Epoch: 2 [9626624/10786049 (89.3%)]	Loss: 0.104279
Train Epoch: 2 [9677824/10786049 (89.7%)]	Loss: 0.080695
Train Epoch: 2 [9729024/10786049 (90.2%)]	Loss: 0.094117
Train Epoch: 2 [9780224/10786049 (90.7%)]	Loss: 0.081231
Train Epoch: 2 [9831424/10786049 (91.1%)]	Loss: 0.091524
Train Epoch: 2 [9882624/10786049 (91.6%)]	Loss: 0.086768
Train Epoch: 2 [9933824/10786049 (92.1%)]	Loss: 0.073221
Train Epoch: 2 [9985024/10786049 (92.6%)]	Loss: 0.068485
Train Epoch: 2 [10036224/10786049 (93.0%)]	Loss: 0.093865
Train Epoch: 2 [10087424/10786049 (93.5%)]	Loss: 0.087525
Train Epoch: 2 [10138624/10786049 (94.0%)]	Loss: 0.088387
Train Epoch: 2 [10189824/10786049 (94.5%)]	Loss: 0.104240
Train Epoch: 2 [10241024/10786049 (94.9%)]	Loss: 0.094878
Train Epoch: 2 [10292224/10786049 (95.4%)]	Loss: 0.081420
Train Epoch: 2 [10343424/10786049 (95.9%)]	Loss: 0.083016
Train Epoch: 2 [10394624/10786049 (96.4%)]	Loss: 0.090062
Train Epoch: 2 [10445824/10786049 (96.8%)]	Loss: 0.078533
Train Epoch: 2 [10497024/10786049 (97.3%)]	Loss: 0.095190
Train Epoch: 2 [10548224/10786049 (97.8%)]	Loss: 0.081806
Train Epoch: 2 [10599424/10786049 (98.3%)]	Loss: 0.083019
Train Epoch: 2 [10650624/10786049 (98.7%)]	Loss: 0.080161
Train Epoch: 2 [10701824/10786049 (99.2%)]	Loss: 0.095522
Train Epoch: 2 [10753024/10786049 (99.7%)]	Loss: 0.085251

ACC in fold#4 was 0.908


Balanced ACC in fold#4 was 0.832


MCC in fold#4 was 0.763


Confusion Matrix in fold#4: 
           nonRipple   Ripple
nonRipple     484376   240489
Ripple          7256  1964391


Classification Report in fold#4: 
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.985        0.891  ...        0.938         0.916
recall            0.668        0.996  ...        0.832         0.908
f1-score          0.796        0.941  ...        0.869         0.902
sample size  724865.000  1971647.000  ...  2696512.000   2696512.000

[4 rows x 5 columns]


Label Errors Rate:
0.027


 --- 5-fold CV overall metrics --- 


The Mattews correlation coefficient: 0.783 +/- 0.039 (mean +/- std.; n=5)


Balanced Accuracy Score: 0.876 +/- 0.03 (mean +/- std.; n=5)


Confusion Matrix (Test; sum; num. folds=5)
           nonRipple   Ripple
nonRipple    2875901   748425
Ripple        409390  9448845


Classification Report (Test; mean; num. folds=5)
              nonRipple       Ripple  ...    macro avg  weighted avg
precision         0.890        0.928  ...        0.909         0.918
recall            0.793        0.958  ...        0.876         0.914
f1-score          0.832        0.942  ...        0.887         0.913
sample size  724865.200  1971647.000  ...  2696512.200   2696512.200

[4 rows x 5 columns]


Classification Report (Test; std; num. folds=5)
             nonRipple  Ripple  balanced accuracy  macro avg  weighted avg
precision        0.081   0.025               0.03      0.033         0.015
recall           0.081   0.035               0.03      0.030         0.017
f1-score         0.033   0.012               0.03      0.021         0.016
sample size      0.400   0.000               0.03      0.400         0.400


ROC AUC micro Score: 0.973 +/- 0.009 (mean +/- std.; n=5)


ROC AUC macro Score: 0.968 +/- 0.009 (mean +/- std.; n=5)


Precision-Recall AUC micro Score: 0.972 +/- 0.009 (mean +/- std.; n=5)


Precision-Recall AUC macro Score: 0.962 +/- 0.01 (mean +/- std.; n=5)


Saved to: ./data/okada/cleanlab_results/D01-/are_errors.npy


Saved to: ./data/okada/cleanlab_results/D01-/are_ripple_GMM.npy


Saved to: ./data/okada/cleanlab_results/D01-/psx_ripple.npy


Saved to: ./data/okada/cleanlab_results/D01-/mccs.csv


Saved to: ./data/okada/cleanlab_results/D01-/balanced_accs.csv


Saved to: ./data/okada/cleanlab_results/D01-/conf_mat/fold#0.png


Saved to: ./data/okada/cleanlab_results/D01-/conf_mat/fold#1.png


Saved to: ./data/okada/cleanlab_results/D01-/conf_mat/fold#2.png


Saved to: ./data/okada/cleanlab_results/D01-/conf_mat/fold#3.png


Saved to: ./data/okada/cleanlab_results/D01-/conf_mat/fold#4.png


Saved to: ./data/okada/cleanlab_results/D01-/conf_mat/conf_mats.csv


Saved to: ./data/okada/cleanlab_results/D01-/conf_mat/overall_sum.png


Saved to: ./data/okada/cleanlab_results/D01-/clf_reports.csv


Saved to: ./data/okada/cleanlab_results/D01-/aucs.csv


Saved to: ./data/okada/cleanlab_results/D01-/roc_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D01-/roc_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D01-/roc_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D01-/roc_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D01-/roc_curves/fold#4.png


Saved to: ./data/okada/cleanlab_results/D01-/pr_curves/fold#0.png


Saved to: ./data/okada/cleanlab_results/D01-/pr_curves/fold#1.png


Saved to: ./data/okada/cleanlab_results/D01-/pr_curves/fold#2.png


Saved to: ./data/okada/cleanlab_results/D01-/pr_curves/fold#3.png


Saved to: ./data/okada/cleanlab_results/D01-/pr_curves/fold#4.png


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/02/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/03/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-1_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-2_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-3_fp16.pkl


Saved to: ./data/okada/04/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt7-4_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-1_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-2_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-3_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt2-4_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day1/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day2/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day3/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day4/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-1_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-2_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-3_fp16.pkl


Saved to: ./data/okada/05/day5/split/ripples_1kHz_pkl/CNN_labeled/D01-/tt6-4_fp16.pkl


Random seeds have been fixed as 42


Random seeds have been fixed as 42


Standard Output/Error are going to be logged in the followings: 
  - ./ripples/define_ripples/using_CNN/makes_labels/log/stdout.log
  - ./ripples/define_ripples/using_CNN/makes_labels/log/stderr.log


Random seeds have been fixed as 42


dataset_key: D01-

['./data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/02/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/03/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day1/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day2/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day3/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-1_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-2_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-3_fp16.npy',
 './data/okada/04/day4/split/LFP_MEP_1kHz_npy/orig/tt7-4_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-1_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-2_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-3_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt2-4_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day1/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day2/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day3/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day4/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-1_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-2_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-3_fp16.npy',
 './data/okada/05/day5/split/LFP_MEP_1kHz_npy/orig/tt6-4_fp16.npy']

2021-0807-0959

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/4420896 (0.0%)]	Loss: 0.353186
Train Epoch: 1 [52224/4420896 (1.2%)]	Loss: 0.225450
Train Epoch: 1 [103424/4420896 (2.3%)]	Loss: 0.178494
Train Epoch: 1 [154624/4420896 (3.5%)]	Loss: 0.170799
Train Epoch: 1 [205824/4420896 (4.7%)]	Loss: 0.177115
Train Epoch: 1 [257024/4420896 (5.8%)]	Loss: 0.153713
Train Epoch: 1 [308224/4420896 (7.0%)]	Loss: 0.192918
Train Epoch: 1 [359424/4420896 (8.1%)]	Loss: 0.153576
Train Epoch: 1 [410624/4420896 (9.3%)]	Loss: 0.163924
Train Epoch: 1 [461824/4420896 (10.4%)]	Loss: 0.156176
Train Epoch: 1 [513024/4420896 (11.6%)]	Loss: 0.175435
Train Epoch: 1 [564224/4420896 (12.8%)]	Loss: 0.163553
Train Epoch: 1 [615424/4420896 (13.9%)]	Loss: 0.166845
Train Epoch: 1 [666624/4420896 (15.1%)]	Loss: 0.163424
Train Epoch: 1 [717824/4420896 (16.2%)]	Loss: 0.163901
Train Epoch: 1 [769024/4420896 (17.4%)]	Loss: 0.154172
Train Epoch: 1 [820224/4420896 (18.6%)]	Loss: 0.160915
Train Epoch: 1 [871424/4420896 (19.7%)]	Loss: 0.156937
Train Epoch: 1 [922624/4420896 (20.9%)]	Loss: 0.141453
Train Epoch: 1 [973824/4420896 (22.0%)]	Loss: 0.152983
Train Epoch: 1 [1025024/4420896 (23.2%)]	Loss: 0.149409
Train Epoch: 1 [1076224/4420896 (24.3%)]	Loss: 0.158830
Train Epoch: 1 [1127424/4420896 (25.5%)]	Loss: 0.157956
Train Epoch: 1 [1178624/4420896 (26.7%)]	Loss: 0.144133
Train Epoch: 1 [1229824/4420896 (27.8%)]	Loss: 0.164271
Train Epoch: 1 [1281024/4420896 (29.0%)]	Loss: 0.151832
Train Epoch: 1 [1332224/4420896 (30.1%)]	Loss: 0.167499
Train Epoch: 1 [1383424/4420896 (31.3%)]	Loss: 0.157271
Train Epoch: 1 [1434624/4420896 (32.5%)]	Loss: 0.161068
Train Epoch: 1 [1485824/4420896 (33.6%)]	Loss: 0.166252
Train Epoch: 1 [1537024/4420896 (34.8%)]	Loss: 0.168864
Train Epoch: 1 [1588224/4420896 (35.9%)]	Loss: 0.158157
Train Epoch: 1 [1639424/4420896 (37.1%)]	Loss: 0.165116
Train Epoch: 1 [1690624/4420896 (38.2%)]	Loss: 0.154731
Train Epoch: 1 [1741824/4420896 (39.4%)]	Loss: 0.160180
Train Epoch: 1 [1793024/4420896 (40.6%)]	Loss: 0.168293
Train Epoch: 1 [1844224/4420896 (41.7%)]	Loss: 0.148559
Train Epoch: 1 [1895424/4420896 (42.9%)]	Loss: 0.154311
Train Epoch: 1 [1946624/4420896 (44.0%)]	Loss: 0.157739
Train Epoch: 1 [1997824/4420896 (45.2%)]	Loss: 0.163094
Train Epoch: 1 [2049024/4420896 (46.3%)]	Loss: 0.149046
Train Epoch: 1 [2100224/4420896 (47.5%)]	Loss: 0.147202
Train Epoch: 1 [2151424/4420896 (48.7%)]	Loss: 0.156923
Train Epoch: 1 [2202624/4420896 (49.8%)]	Loss: 0.153209
Train Epoch: 1 [2253824/4420896 (51.0%)]	Loss: 0.167650
Train Epoch: 1 [2305024/4420896 (52.1%)]	Loss: 0.168062
Train Epoch: 1 [2356224/4420896 (53.3%)]	Loss: 0.152372
Train Epoch: 1 [2407424/4420896 (54.5%)]	Loss: 0.157897
Train Epoch: 1 [2458624/4420896 (55.6%)]	Loss: 0.154471
Train Epoch: 1 [2509824/4420896 (56.8%)]	Loss: 0.155084
Train Epoch: 1 [2561024/4420896 (57.9%)]	Loss: 0.159295
Train Epoch: 1 [2612224/4420896 (59.1%)]	Loss: 0.151797
Train Epoch: 1 [2663424/4420896 (60.2%)]	Loss: 0.156585
Train Epoch: 1 [2714624/4420896 (61.4%)]	Loss: 0.156817
Train Epoch: 1 [2765824/4420896 (62.6%)]	Loss: 0.146241
Train Epoch: 1 [2817024/4420896 (63.7%)]	Loss: 0.142311
Train Epoch: 1 [2868224/4420896 (64.9%)]	Loss: 0.159500
Train Epoch: 1 [2919424/4420896 (66.0%)]	Loss: 0.156532
Train Epoch: 1 [2970624/4420896 (67.2%)]	Loss: 0.148475
Train Epoch: 1 [3021824/4420896 (68.4%)]	Loss: 0.168448
Train Epoch: 1 [3073024/4420896 (69.5%)]	Loss: 0.147946
Train Epoch: 1 [3124224/4420896 (70.7%)]	Loss: 0.148018
Train Epoch: 1 [3175424/4420896 (71.8%)]	Loss: 0.149814
Train Epoch: 1 [3226624/4420896 (73.0%)]	Loss: 0.150355
Train Epoch: 1 [3277824/4420896 (74.1%)]	Loss: 0.149512
Train Epoch: 1 [3329024/4420896 (75.3%)]	Loss: 0.152149
Train Epoch: 1 [3380224/4420896 (76.5%)]	Loss: 0.142018
Train Epoch: 1 [3431424/4420896 (77.6%)]	Loss: 0.141727
Train Epoch: 1 [3482624/4420896 (78.8%)]	Loss: 0.150128
Train Epoch: 1 [3533824/4420896 (79.9%)]	Loss: 0.147932
Train Epoch: 1 [3585024/4420896 (81.1%)]	Loss: 0.163637
Train Epoch: 1 [3636224/4420896 (82.3%)]	Loss: 0.162662
Train Epoch: 1 [3687424/4420896 (83.4%)]	Loss: 0.172246
Train Epoch: 1 [3738624/4420896 (84.6%)]	Loss: 0.150823
Train Epoch: 1 [3789824/4420896 (85.7%)]	Loss: 0.142030
Train Epoch: 1 [3841024/4420896 (86.9%)]	Loss: 0.163727
Train Epoch: 1 [3892224/4420896 (88.0%)]	Loss: 0.147904
Train Epoch: 1 [3943424/4420896 (89.2%)]	Loss: 0.146754
Train Epoch: 1 [3994624/4420896 (90.4%)]	Loss: 0.144974
Train Epoch: 1 [4045824/4420896 (91.5%)]	Loss: 0.158023
Train Epoch: 1 [4097024/4420896 (92.7%)]	Loss: 0.149833
Train Epoch: 1 [4148224/4420896 (93.8%)]	Loss: 0.148939
Train Epoch: 1 [4199424/4420896 (95.0%)]	Loss: 0.145031
Train Epoch: 1 [4250624/4420896 (96.1%)]	Loss: 0.145273
Train Epoch: 1 [4301824/4420896 (97.3%)]	Loss: 0.141339
Train Epoch: 1 [4353024/4420896 (98.5%)]	Loss: 0.145228
Train Epoch: 1 [4404224/4420896 (99.6%)]	Loss: 0.157063
Train Epoch: 2 [1024/4420896 (0.0%)]	Loss: 0.150004
Train Epoch: 2 [52224/4420896 (1.2%)]	Loss: 0.154310
Train Epoch: 2 [103424/4420896 (2.3%)]	Loss: 0.132963
Train Epoch: 2 [154624/4420896 (3.5%)]	Loss: 0.156101
Train Epoch: 2 [205824/4420896 (4.7%)]	Loss: 0.142860
Train Epoch: 2 [257024/4420896 (5.8%)]	Loss: 0.164894
Train Epoch: 2 [308224/4420896 (7.0%)]	Loss: 0.154654
Train Epoch: 2 [359424/4420896 (8.1%)]	Loss: 0.154927
Train Epoch: 2 [410624/4420896 (9.3%)]	Loss: 0.137750
Train Epoch: 2 [461824/4420896 (10.4%)]	Loss: 0.154113
Train Epoch: 2 [513024/4420896 (11.6%)]	Loss: 0.159977
Train Epoch: 2 [564224/4420896 (12.8%)]	Loss: 0.158269
Train Epoch: 2 [615424/4420896 (13.9%)]	Loss: 0.143838
Train Epoch: 2 [666624/4420896 (15.1%)]	Loss: 0.151125
Train Epoch: 2 [717824/4420896 (16.2%)]	Loss: 0.165926
Train Epoch: 2 [769024/4420896 (17.4%)]	Loss: 0.147952
Train Epoch: 2 [820224/4420896 (18.6%)]	Loss: 0.157107
Train Epoch: 2 [871424/4420896 (19.7%)]	Loss: 0.165520
Train Epoch: 2 [922624/4420896 (20.9%)]	Loss: 0.156473
Train Epoch: 2 [973824/4420896 (22.0%)]	Loss: 0.158196
Train Epoch: 2 [1025024/4420896 (23.2%)]	Loss: 0.150407
Train Epoch: 2 [1076224/4420896 (24.3%)]	Loss: 0.153586
Train Epoch: 2 [1127424/4420896 (25.5%)]	Loss: 0.157503
Train Epoch: 2 [1178624/4420896 (26.7%)]	Loss: 0.161827
Train Epoch: 2 [1229824/4420896 (27.8%)]	Loss: 0.152735
Train Epoch: 2 [1281024/4420896 (29.0%)]	Loss: 0.160839
Train Epoch: 2 [1332224/4420896 (30.1%)]	Loss: 0.155979
Train Epoch: 2 [1383424/4420896 (31.3%)]	Loss: 0.146513
Train Epoch: 2 [1434624/4420896 (32.5%)]	Loss: 0.153961
Train Epoch: 2 [1485824/4420896 (33.6%)]	Loss: 0.168613
Train Epoch: 2 [1537024/4420896 (34.8%)]	Loss: 0.159239
Train Epoch: 2 [1588224/4420896 (35.9%)]	Loss: 0.133539
Train Epoch: 2 [1639424/4420896 (37.1%)]	Loss: 0.148727
Train Epoch: 2 [1690624/4420896 (38.2%)]	Loss: 0.152123
Train Epoch: 2 [1741824/4420896 (39.4%)]	Loss: 0.146846
Train Epoch: 2 [1793024/4420896 (40.6%)]	Loss: 0.161556
Train Epoch: 2 [1844224/4420896 (41.7%)]	Loss: 0.143509
Train Epoch: 2 [1895424/4420896 (42.9%)]	Loss: 0.155897
Train Epoch: 2 [1946624/4420896 (44.0%)]	Loss: 0.152298
Train Epoch: 2 [1997824/4420896 (45.2%)]	Loss: 0.169617
Train Epoch: 2 [2049024/4420896 (46.3%)]	Loss: 0.154046
Train Epoch: 2 [2100224/4420896 (47.5%)]	Loss: 0.138731
Train Epoch: 2 [2151424/4420896 (48.7%)]	Loss: 0.140798
Train Epoch: 2 [2202624/4420896 (49.8%)]	Loss: 0.141477
Train Epoch: 2 [2253824/4420896 (51.0%)]	Loss: 0.137466
Train Epoch: 2 [2305024/4420896 (52.1%)]	Loss: 0.139771
Train Epoch: 2 [2356224/4420896 (53.3%)]	Loss: 0.149610
Train Epoch: 2 [2407424/4420896 (54.5%)]	Loss: 0.140989
Train Epoch: 2 [2458624/4420896 (55.6%)]	Loss: 0.155827
Train Epoch: 2 [2509824/4420896 (56.8%)]	Loss: 0.143266
Train Epoch: 2 [2561024/4420896 (57.9%)]	Loss: 0.144656
Train Epoch: 2 [2612224/4420896 (59.1%)]	Loss: 0.164250
Train Epoch: 2 [2663424/4420896 (60.2%)]	Loss: 0.138237
Train Epoch: 2 [2714624/4420896 (61.4%)]	Loss: 0.150477
Train Epoch: 2 [2765824/4420896 (62.6%)]	Loss: 0.156302
Train Epoch: 2 [2817024/4420896 (63.7%)]	Loss: 0.155562
Train Epoch: 2 [2868224/4420896 (64.9%)]	Loss: 0.126261
Train Epoch: 2 [2919424/4420896 (66.0%)]	Loss: 0.153268
Train Epoch: 2 [2970624/4420896 (67.2%)]	Loss: 0.140958
Train Epoch: 2 [3021824/4420896 (68.4%)]	Loss: 0.155823
Train Epoch: 2 [3073024/4420896 (69.5%)]	Loss: 0.144920
Train Epoch: 2 [3124224/4420896 (70.7%)]	Loss: 0.145862
Train Epoch: 2 [3175424/4420896 (71.8%)]	Loss: 0.149542
Train Epoch: 2 [3226624/4420896 (73.0%)]	Loss: 0.151198
Train Epoch: 2 [3277824/4420896 (74.1%)]	Loss: 0.159220
Train Epoch: 2 [3329024/4420896 (75.3%)]	Loss: 0.147029
Train Epoch: 2 [3380224/4420896 (76.5%)]	Loss: 0.151993
Train Epoch: 2 [3431424/4420896 (77.6%)]	Loss: 0.151069
Train Epoch: 2 [3482624/4420896 (78.8%)]	Loss: 0.146975
Train Epoch: 2 [3533824/4420896 (79.9%)]	Loss: 0.156268
Train Epoch: 2 [3585024/4420896 (81.1%)]	Loss: 0.158629
Train Epoch: 2 [3636224/4420896 (82.3%)]	Loss: 0.154832
Train Epoch: 2 [3687424/4420896 (83.4%)]	Loss: 0.157015
Train Epoch: 2 [3738624/4420896 (84.6%)]	Loss: 0.148085
Train Epoch: 2 [3789824/4420896 (85.7%)]	Loss: 0.142271
Train Epoch: 2 [3841024/4420896 (86.9%)]	Loss: 0.149958
Train Epoch: 2 [3892224/4420896 (88.0%)]	Loss: 0.158865
Train Epoch: 2 [3943424/4420896 (89.2%)]	Loss: 0.147857
Train Epoch: 2 [3994624/4420896 (90.4%)]	Loss: 0.145272
Train Epoch: 2 [4045824/4420896 (91.5%)]	Loss: 0.151139
Train Epoch: 2 [4097024/4420896 (92.7%)]	Loss: 0.148198
Train Epoch: 2 [4148224/4420896 (93.8%)]	Loss: 0.139121
Train Epoch: 2 [4199424/4420896 (95.0%)]	Loss: 0.139794
Train Epoch: 2 [4250624/4420896 (96.1%)]	Loss: 0.162215
Train Epoch: 2 [4301824/4420896 (97.3%)]	Loss: 0.158790
Train Epoch: 2 [4353024/4420896 (98.5%)]	Loss: 0.158695
Train Epoch: 2 [4404224/4420896 (99.6%)]	Loss: 0.149166

ACC in fold#0 was 0.849


Balanced ACC in fold#0 was 0.846


MCC in fold#0 was 0.693


Confusion Matrix in fold#0: 
           nonRipple   Ripple
nonRipple     954774   210313
Ripple        195879  1335547


Classification Report in fold#0: 
               nonRipple       Ripple  ...    macro avg  weighted avg
precision          0.830        0.864  ...        0.847         0.849
recall             0.819        0.872  ...        0.846         0.849
f1-score           0.825        0.868  ...        0.846         0.849
sample size  1165087.000  1531426.000  ...  2696513.000   2696513.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/4469480 (0.0%)]	Loss: 0.336312
Train Epoch: 1 [52224/4469480 (1.2%)]	Loss: 0.201339
Train Epoch: 1 [103424/4469480 (2.3%)]	Loss: 0.194138
Train Epoch: 1 [154624/4469480 (3.5%)]	Loss: 0.182095
Train Epoch: 1 [205824/4469480 (4.6%)]	Loss: 0.173913
Train Epoch: 1 [257024/4469480 (5.8%)]	Loss: 0.162809
Train Epoch: 1 [308224/4469480 (6.9%)]	Loss: 0.148204
Train Epoch: 1 [359424/4469480 (8.0%)]	Loss: 0.159586
Train Epoch: 1 [410624/4469480 (9.2%)]	Loss: 0.171508
Train Epoch: 1 [461824/4469480 (10.3%)]	Loss: 0.142802
Train Epoch: 1 [513024/4469480 (11.5%)]	Loss: 0.160606
Train Epoch: 1 [564224/4469480 (12.6%)]	Loss: 0.175019
Train Epoch: 1 [615424/4469480 (13.8%)]	Loss: 0.160723
Train Epoch: 1 [666624/4469480 (14.9%)]	Loss: 0.151222
Train Epoch: 1 [717824/4469480 (16.1%)]	Loss: 0.162609
Train Epoch: 1 [769024/4469480 (17.2%)]	Loss: 0.166819
Train Epoch: 1 [820224/4469480 (18.4%)]	Loss: 0.168612
Train Epoch: 1 [871424/4469480 (19.5%)]	Loss: 0.151095
Train Epoch: 1 [922624/4469480 (20.6%)]	Loss: 0.155264
Train Epoch: 1 [973824/4469480 (21.8%)]	Loss: 0.152377
Train Epoch: 1 [1025024/4469480 (22.9%)]	Loss: 0.177083
Train Epoch: 1 [1076224/4469480 (24.1%)]	Loss: 0.158970
Train Epoch: 1 [1127424/4469480 (25.2%)]	Loss: 0.157655
Train Epoch: 1 [1178624/4469480 (26.4%)]	Loss: 0.161269
Train Epoch: 1 [1229824/4469480 (27.5%)]	Loss: 0.157173
Train Epoch: 1 [1281024/4469480 (28.7%)]	Loss: 0.151313
Train Epoch: 1 [1332224/4469480 (29.8%)]	Loss: 0.149599
Train Epoch: 1 [1383424/4469480 (31.0%)]	Loss: 0.168396
Train Epoch: 1 [1434624/4469480 (32.1%)]	Loss: 0.155618
Train Epoch: 1 [1485824/4469480 (33.2%)]	Loss: 0.143146
Train Epoch: 1 [1537024/4469480 (34.4%)]	Loss: 0.144675
Train Epoch: 1 [1588224/4469480 (35.5%)]	Loss: 0.157806
Train Epoch: 1 [1639424/4469480 (36.7%)]	Loss: 0.147022
Train Epoch: 1 [1690624/4469480 (37.8%)]	Loss: 0.161565
Train Epoch: 1 [1741824/4469480 (39.0%)]	Loss: 0.156551
Train Epoch: 1 [1793024/4469480 (40.1%)]	Loss: 0.164397
Train Epoch: 1 [1844224/4469480 (41.3%)]	Loss: 0.144726
Train Epoch: 1 [1895424/4469480 (42.4%)]	Loss: 0.156554
Train Epoch: 1 [1946624/4469480 (43.6%)]	Loss: 0.148593
Train Epoch: 1 [1997824/4469480 (44.7%)]	Loss: 0.146830
Train Epoch: 1 [2049024/4469480 (45.8%)]	Loss: 0.163617
Train Epoch: 1 [2100224/4469480 (47.0%)]	Loss: 0.156993
Train Epoch: 1 [2151424/4469480 (48.1%)]	Loss: 0.152877
Train Epoch: 1 [2202624/4469480 (49.3%)]	Loss: 0.159591
Train Epoch: 1 [2253824/4469480 (50.4%)]	Loss: 0.151248
Train Epoch: 1 [2305024/4469480 (51.6%)]	Loss: 0.156695
Train Epoch: 1 [2356224/4469480 (52.7%)]	Loss: 0.164245
Train Epoch: 1 [2407424/4469480 (53.9%)]	Loss: 0.166167
Train Epoch: 1 [2458624/4469480 (55.0%)]	Loss: 0.145859
Train Epoch: 1 [2509824/4469480 (56.2%)]	Loss: 0.162262
Train Epoch: 1 [2561024/4469480 (57.3%)]	Loss: 0.180527
Train Epoch: 1 [2612224/4469480 (58.4%)]	Loss: 0.149238
Train Epoch: 1 [2663424/4469480 (59.6%)]	Loss: 0.162800
Train Epoch: 1 [2714624/4469480 (60.7%)]	Loss: 0.159584
Train Epoch: 1 [2765824/4469480 (61.9%)]	Loss: 0.155648
Train Epoch: 1 [2817024/4469480 (63.0%)]	Loss: 0.159038
Train Epoch: 1 [2868224/4469480 (64.2%)]	Loss: 0.165275
Train Epoch: 1 [2919424/4469480 (65.3%)]	Loss: 0.157046
Train Epoch: 1 [2970624/4469480 (66.5%)]	Loss: 0.147946
Train Epoch: 1 [3021824/4469480 (67.6%)]	Loss: 0.151097
Train Epoch: 1 [3073024/4469480 (68.8%)]	Loss: 0.149857
Train Epoch: 1 [3124224/4469480 (69.9%)]	Loss: 0.174308
Train Epoch: 1 [3175424/4469480 (71.0%)]	Loss: 0.158781
Train Epoch: 1 [3226624/4469480 (72.2%)]	Loss: 0.155532
Train Epoch: 1 [3277824/4469480 (73.3%)]	Loss: 0.172279
Train Epoch: 1 [3329024/4469480 (74.5%)]	Loss: 0.159199
Train Epoch: 1 [3380224/4469480 (75.6%)]	Loss: 0.149755
Train Epoch: 1 [3431424/4469480 (76.8%)]	Loss: 0.157105
Train Epoch: 1 [3482624/4469480 (77.9%)]	Loss: 0.150133
Train Epoch: 1 [3533824/4469480 (79.1%)]	Loss: 0.151908
Train Epoch: 1 [3585024/4469480 (80.2%)]	Loss: 0.151739
Train Epoch: 1 [3636224/4469480 (81.4%)]	Loss: 0.148690
Train Epoch: 1 [3687424/4469480 (82.5%)]	Loss: 0.165336
Train Epoch: 1 [3738624/4469480 (83.6%)]	Loss: 0.153314
Train Epoch: 1 [3789824/4469480 (84.8%)]	Loss: 0.155695
Train Epoch: 1 [3841024/4469480 (85.9%)]	Loss: 0.128397
Train Epoch: 1 [3892224/4469480 (87.1%)]	Loss: 0.141224
Train Epoch: 1 [3943424/4469480 (88.2%)]	Loss: 0.162324
Train Epoch: 1 [3994624/4469480 (89.4%)]	Loss: 0.142097
Train Epoch: 1 [4045824/4469480 (90.5%)]	Loss: 0.153533
Train Epoch: 1 [4097024/4469480 (91.7%)]	Loss: 0.158377
Train Epoch: 1 [4148224/4469480 (92.8%)]	Loss: 0.150906
Train Epoch: 1 [4199424/4469480 (94.0%)]	Loss: 0.141450
Train Epoch: 1 [4250624/4469480 (95.1%)]	Loss: 0.165066
Train Epoch: 1 [4301824/4469480 (96.2%)]	Loss: 0.157992
Train Epoch: 1 [4353024/4469480 (97.4%)]	Loss: 0.145330
Train Epoch: 1 [4404224/4469480 (98.5%)]	Loss: 0.151392
Train Epoch: 1 [4455424/4469480 (99.7%)]	Loss: 0.144551
Train Epoch: 2 [1024/4469480 (0.0%)]	Loss: 0.149095
Train Epoch: 2 [52224/4469480 (1.2%)]	Loss: 0.141720
Train Epoch: 2 [103424/4469480 (2.3%)]	Loss: 0.151458
Train Epoch: 2 [154624/4469480 (3.5%)]	Loss: 0.158298
Train Epoch: 2 [205824/4469480 (4.6%)]	Loss: 0.160687
Train Epoch: 2 [257024/4469480 (5.8%)]	Loss: 0.144852
Train Epoch: 2 [308224/4469480 (6.9%)]	Loss: 0.145979
Train Epoch: 2 [359424/4469480 (8.0%)]	Loss: 0.145335
Train Epoch: 2 [410624/4469480 (9.2%)]	Loss: 0.145589
Train Epoch: 2 [461824/4469480 (10.3%)]	Loss: 0.146649
Train Epoch: 2 [513024/4469480 (11.5%)]	Loss: 0.141562
Train Epoch: 2 [564224/4469480 (12.6%)]	Loss: 0.148622
Train Epoch: 2 [615424/4469480 (13.8%)]	Loss: 0.145137
Train Epoch: 2 [666624/4469480 (14.9%)]	Loss: 0.157595
Train Epoch: 2 [717824/4469480 (16.1%)]	Loss: 0.154369
Train Epoch: 2 [769024/4469480 (17.2%)]	Loss: 0.151618
Train Epoch: 2 [820224/4469480 (18.4%)]	Loss: 0.152995
Train Epoch: 2 [871424/4469480 (19.5%)]	Loss: 0.145458
Train Epoch: 2 [922624/4469480 (20.6%)]	Loss: 0.171261
Train Epoch: 2 [973824/4469480 (21.8%)]	Loss: 0.151309
Train Epoch: 2 [1025024/4469480 (22.9%)]	Loss: 0.148502
Train Epoch: 2 [1076224/4469480 (24.1%)]	Loss: 0.152813
Train Epoch: 2 [1127424/4469480 (25.2%)]	Loss: 0.153510
Train Epoch: 2 [1178624/4469480 (26.4%)]	Loss: 0.146814
Train Epoch: 2 [1229824/4469480 (27.5%)]	Loss: 0.146345
Train Epoch: 2 [1281024/4469480 (28.7%)]	Loss: 0.166511
Train Epoch: 2 [1332224/4469480 (29.8%)]	Loss: 0.154204
Train Epoch: 2 [1383424/4469480 (31.0%)]	Loss: 0.148556
Train Epoch: 2 [1434624/4469480 (32.1%)]	Loss: 0.156780
Train Epoch: 2 [1485824/4469480 (33.2%)]	Loss: 0.153552
Train Epoch: 2 [1537024/4469480 (34.4%)]	Loss: 0.136473
Train Epoch: 2 [1588224/4469480 (35.5%)]	Loss: 0.156975
Train Epoch: 2 [1639424/4469480 (36.7%)]	Loss: 0.135574
Train Epoch: 2 [1690624/4469480 (37.8%)]	Loss: 0.155870
Train Epoch: 2 [1741824/4469480 (39.0%)]	Loss: 0.155371
Train Epoch: 2 [1793024/4469480 (40.1%)]	Loss: 0.146994
Train Epoch: 2 [1844224/4469480 (41.3%)]	Loss: 0.143952
Train Epoch: 2 [1895424/4469480 (42.4%)]	Loss: 0.153611
Train Epoch: 2 [1946624/4469480 (43.6%)]	Loss: 0.153804
Train Epoch: 2 [1997824/4469480 (44.7%)]	Loss: 0.140285
Train Epoch: 2 [2049024/4469480 (45.8%)]	Loss: 0.126055
Train Epoch: 2 [2100224/4469480 (47.0%)]	Loss: 0.156643
Train Epoch: 2 [2151424/4469480 (48.1%)]	Loss: 0.146530
Train Epoch: 2 [2202624/4469480 (49.3%)]	Loss: 0.143868
Train Epoch: 2 [2253824/4469480 (50.4%)]	Loss: 0.146473
Train Epoch: 2 [2305024/4469480 (51.6%)]	Loss: 0.154093
Train Epoch: 2 [2356224/4469480 (52.7%)]	Loss: 0.157956
Train Epoch: 2 [2407424/4469480 (53.9%)]	Loss: 0.154750
Train Epoch: 2 [2458624/4469480 (55.0%)]	Loss: 0.145225
Train Epoch: 2 [2509824/4469480 (56.2%)]	Loss: 0.155435
Train Epoch: 2 [2561024/4469480 (57.3%)]	Loss: 0.145408
Train Epoch: 2 [2612224/4469480 (58.4%)]	Loss: 0.147888
Train Epoch: 2 [2663424/4469480 (59.6%)]	Loss: 0.154931
Train Epoch: 2 [2714624/4469480 (60.7%)]	Loss: 0.129456
Train Epoch: 2 [2765824/4469480 (61.9%)]	Loss: 0.139145
Train Epoch: 2 [2817024/4469480 (63.0%)]	Loss: 0.149316
Train Epoch: 2 [2868224/4469480 (64.2%)]	Loss: 0.154267
Train Epoch: 2 [2919424/4469480 (65.3%)]	Loss: 0.139520
Train Epoch: 2 [2970624/4469480 (66.5%)]	Loss: 0.166400
Train Epoch: 2 [3021824/4469480 (67.6%)]	Loss: 0.147980
Train Epoch: 2 [3073024/4469480 (68.8%)]	Loss: 0.162631
Train Epoch: 2 [3124224/4469480 (69.9%)]	Loss: 0.145650
Train Epoch: 2 [3175424/4469480 (71.0%)]	Loss: 0.149990
Train Epoch: 2 [3226624/4469480 (72.2%)]	Loss: 0.155945
Train Epoch: 2 [3277824/4469480 (73.3%)]	Loss: 0.144772
Train Epoch: 2 [3329024/4469480 (74.5%)]	Loss: 0.152805
Train Epoch: 2 [3380224/4469480 (75.6%)]	Loss: 0.138033
Train Epoch: 2 [3431424/4469480 (76.8%)]	Loss: 0.167003
Train Epoch: 2 [3482624/4469480 (77.9%)]	Loss: 0.165881
Train Epoch: 2 [3533824/4469480 (79.1%)]	Loss: 0.136219
Train Epoch: 2 [3585024/4469480 (80.2%)]	Loss: 0.156508
Train Epoch: 2 [3636224/4469480 (81.4%)]	Loss: 0.151144
Train Epoch: 2 [3687424/4469480 (82.5%)]	Loss: 0.161682
Train Epoch: 2 [3738624/4469480 (83.6%)]	Loss: 0.160383
Train Epoch: 2 [3789824/4469480 (84.8%)]	Loss: 0.176702
Train Epoch: 2 [3841024/4469480 (85.9%)]	Loss: 0.147369
Train Epoch: 2 [3892224/4469480 (87.1%)]	Loss: 0.153168
Train Epoch: 2 [3943424/4469480 (88.2%)]	Loss: 0.142470
Train Epoch: 2 [3994624/4469480 (89.4%)]	Loss: 0.155550
Train Epoch: 2 [4045824/4469480 (90.5%)]	Loss: 0.153542
Train Epoch: 2 [4097024/4469480 (91.7%)]	Loss: 0.141267
Train Epoch: 2 [4148224/4469480 (92.8%)]	Loss: 0.156444
Train Epoch: 2 [4199424/4469480 (94.0%)]	Loss: 0.147215
Train Epoch: 2 [4250624/4469480 (95.1%)]	Loss: 0.146658
Train Epoch: 2 [4301824/4469480 (96.2%)]	Loss: 0.151586
Train Epoch: 2 [4353024/4469480 (97.4%)]	Loss: 0.160597
Train Epoch: 2 [4404224/4469480 (98.5%)]	Loss: 0.149905
Train Epoch: 2 [4455424/4469480 (99.7%)]	Loss: 0.151139

ACC in fold#1 was 0.865


Balanced ACC in fold#1 was 0.858


MCC in fold#1 was 0.720


Confusion Matrix in fold#1: 
           nonRipple   Ripple
nonRipple     917886   206207
Ripple        158864  1413555


Classification Report in fold#1: 
               nonRipple       Ripple  ...    macro avg  weighted avg
precision          0.852        0.873  ...        0.863         0.864
recall             0.817        0.899  ...        0.858         0.865
f1-score           0.834        0.886  ...        0.860         0.864
sample size  1124093.000  1572419.000  ...  2696512.000   2696512.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/4851824 (0.0%)]	Loss: 0.339256
Train Epoch: 1 [52224/4851824 (1.1%)]	Loss: 0.212614
Train Epoch: 1 [103424/4851824 (2.1%)]	Loss: 0.181219
Train Epoch: 1 [154624/4851824 (3.2%)]	Loss: 0.166471
Train Epoch: 1 [205824/4851824 (4.2%)]	Loss: 0.180066
Train Epoch: 1 [257024/4851824 (5.3%)]	Loss: 0.159954
Train Epoch: 1 [308224/4851824 (6.4%)]	Loss: 0.161713
Train Epoch: 1 [359424/4851824 (7.4%)]	Loss: 0.146720
Train Epoch: 1 [410624/4851824 (8.5%)]	Loss: 0.160028
Train Epoch: 1 [461824/4851824 (9.5%)]	Loss: 0.163059
Train Epoch: 1 [513024/4851824 (10.6%)]	Loss: 0.161604
Train Epoch: 1 [564224/4851824 (11.6%)]	Loss: 0.183075
Train Epoch: 1 [615424/4851824 (12.7%)]	Loss: 0.171798
Train Epoch: 1 [666624/4851824 (13.7%)]	Loss: 0.148519
Train Epoch: 1 [717824/4851824 (14.8%)]	Loss: 0.166310
Train Epoch: 1 [769024/4851824 (15.9%)]	Loss: 0.157811
Train Epoch: 1 [820224/4851824 (16.9%)]	Loss: 0.171271
Train Epoch: 1 [871424/4851824 (18.0%)]	Loss: 0.159261
Train Epoch: 1 [922624/4851824 (19.0%)]	Loss: 0.156701
Train Epoch: 1 [973824/4851824 (20.1%)]	Loss: 0.153470
Train Epoch: 1 [1025024/4851824 (21.1%)]	Loss: 0.160441
Train Epoch: 1 [1076224/4851824 (22.2%)]	Loss: 0.153803
Train Epoch: 1 [1127424/4851824 (23.2%)]	Loss: 0.155776
Train Epoch: 1 [1178624/4851824 (24.3%)]	Loss: 0.152135
Train Epoch: 1 [1229824/4851824 (25.3%)]	Loss: 0.157362
Train Epoch: 1 [1281024/4851824 (26.4%)]	Loss: 0.154982
Train Epoch: 1 [1332224/4851824 (27.5%)]	Loss: 0.158069
Train Epoch: 1 [1383424/4851824 (28.5%)]	Loss: 0.157555
Train Epoch: 1 [1434624/4851824 (29.6%)]	Loss: 0.159354
Train Epoch: 1 [1485824/4851824 (30.6%)]	Loss: 0.163993
Train Epoch: 1 [1537024/4851824 (31.7%)]	Loss: 0.141634
Train Epoch: 1 [1588224/4851824 (32.7%)]	Loss: 0.162675
Train Epoch: 1 [1639424/4851824 (33.8%)]	Loss: 0.156538
Train Epoch: 1 [1690624/4851824 (34.8%)]	Loss: 0.156757
Train Epoch: 1 [1741824/4851824 (35.9%)]	Loss: 0.169585
Train Epoch: 1 [1793024/4851824 (37.0%)]	Loss: 0.147841
Train Epoch: 1 [1844224/4851824 (38.0%)]	Loss: 0.149583
Train Epoch: 1 [1895424/4851824 (39.1%)]	Loss: 0.148520
Train Epoch: 1 [1946624/4851824 (40.1%)]	Loss: 0.158319
Train Epoch: 1 [1997824/4851824 (41.2%)]	Loss: 0.163522
Train Epoch: 1 [2049024/4851824 (42.2%)]	Loss: 0.158229
Train Epoch: 1 [2100224/4851824 (43.3%)]	Loss: 0.166403
Train Epoch: 1 [2151424/4851824 (44.3%)]	Loss: 0.162825
Train Epoch: 1 [2202624/4851824 (45.4%)]	Loss: 0.156221
Train Epoch: 1 [2253824/4851824 (46.5%)]	Loss: 0.160752
Train Epoch: 1 [2305024/4851824 (47.5%)]	Loss: 0.158121
Train Epoch: 1 [2356224/4851824 (48.6%)]	Loss: 0.155770
Train Epoch: 1 [2407424/4851824 (49.6%)]	Loss: 0.139638
Train Epoch: 1 [2458624/4851824 (50.7%)]	Loss: 0.160161
Train Epoch: 1 [2509824/4851824 (51.7%)]	Loss: 0.147670
Train Epoch: 1 [2561024/4851824 (52.8%)]	Loss: 0.152863
Train Epoch: 1 [2612224/4851824 (53.8%)]	Loss: 0.165660
Train Epoch: 1 [2663424/4851824 (54.9%)]	Loss: 0.138774
Train Epoch: 1 [2714624/4851824 (56.0%)]	Loss: 0.142242
Train Epoch: 1 [2765824/4851824 (57.0%)]	Loss: 0.139720
Train Epoch: 1 [2817024/4851824 (58.1%)]	Loss: 0.154823
Train Epoch: 1 [2868224/4851824 (59.1%)]	Loss: 0.161447
Train Epoch: 1 [2919424/4851824 (60.2%)]	Loss: 0.160941
Train Epoch: 1 [2970624/4851824 (61.2%)]	Loss: 0.164889
Train Epoch: 1 [3021824/4851824 (62.3%)]	Loss: 0.151287
Train Epoch: 1 [3073024/4851824 (63.3%)]	Loss: 0.156426
Train Epoch: 1 [3124224/4851824 (64.4%)]	Loss: 0.163931
Train Epoch: 1 [3175424/4851824 (65.4%)]	Loss: 0.145633
Train Epoch: 1 [3226624/4851824 (66.5%)]	Loss: 0.137410
Train Epoch: 1 [3277824/4851824 (67.6%)]	Loss: 0.161369
Train Epoch: 1 [3329024/4851824 (68.6%)]	Loss: 0.146315
Train Epoch: 1 [3380224/4851824 (69.7%)]	Loss: 0.144567
Train Epoch: 1 [3431424/4851824 (70.7%)]	Loss: 0.146154
Train Epoch: 1 [3482624/4851824 (71.8%)]	Loss: 0.136685
Train Epoch: 1 [3533824/4851824 (72.8%)]	Loss: 0.157495
Train Epoch: 1 [3585024/4851824 (73.9%)]	Loss: 0.142857
Train Epoch: 1 [3636224/4851824 (74.9%)]	Loss: 0.146552
Train Epoch: 1 [3687424/4851824 (76.0%)]	Loss: 0.165238
Train Epoch: 1 [3738624/4851824 (77.1%)]	Loss: 0.147278
Train Epoch: 1 [3789824/4851824 (78.1%)]	Loss: 0.155784
Train Epoch: 1 [3841024/4851824 (79.2%)]	Loss: 0.173577
Train Epoch: 1 [3892224/4851824 (80.2%)]	Loss: 0.143209
Train Epoch: 1 [3943424/4851824 (81.3%)]	Loss: 0.154546
Train Epoch: 1 [3994624/4851824 (82.3%)]	Loss: 0.161516
Train Epoch: 1 [4045824/4851824 (83.4%)]	Loss: 0.150448
Train Epoch: 1 [4097024/4851824 (84.4%)]	Loss: 0.160082
Train Epoch: 1 [4148224/4851824 (85.5%)]	Loss: 0.158752
Train Epoch: 1 [4199424/4851824 (86.6%)]	Loss: 0.132416
Train Epoch: 1 [4250624/4851824 (87.6%)]	Loss: 0.161073
Train Epoch: 1 [4301824/4851824 (88.7%)]	Loss: 0.139494
Train Epoch: 1 [4353024/4851824 (89.7%)]	Loss: 0.140875
Train Epoch: 1 [4404224/4851824 (90.8%)]	Loss: 0.161289
Train Epoch: 1 [4455424/4851824 (91.8%)]	Loss: 0.144320
Train Epoch: 1 [4506624/4851824 (92.9%)]	Loss: 0.137196
Train Epoch: 1 [4557824/4851824 (93.9%)]	Loss: 0.149759
Train Epoch: 1 [4609024/4851824 (95.0%)]	Loss: 0.155126
Train Epoch: 1 [4660224/4851824 (96.1%)]	Loss: 0.140708
Train Epoch: 1 [4711424/4851824 (97.1%)]	Loss: 0.135276
Train Epoch: 1 [4762624/4851824 (98.2%)]	Loss: 0.144651
Train Epoch: 1 [4813824/4851824 (99.2%)]	Loss: 0.137060
Train Epoch: 2 [1024/4851824 (0.0%)]	Loss: 0.157203
Train Epoch: 2 [52224/4851824 (1.1%)]	Loss: 0.143957
Train Epoch: 2 [103424/4851824 (2.1%)]	Loss: 0.149175
Train Epoch: 2 [154624/4851824 (3.2%)]	Loss: 0.168003
Train Epoch: 2 [205824/4851824 (4.2%)]	Loss: 0.150475
Train Epoch: 2 [257024/4851824 (5.3%)]	Loss: 0.154794
Train Epoch: 2 [308224/4851824 (6.4%)]	Loss: 0.137839
Train Epoch: 2 [359424/4851824 (7.4%)]	Loss: 0.157975
Train Epoch: 2 [410624/4851824 (8.5%)]	Loss: 0.143883
Train Epoch: 2 [461824/4851824 (9.5%)]	Loss: 0.148778
Train Epoch: 2 [513024/4851824 (10.6%)]	Loss: 0.166588
Train Epoch: 2 [564224/4851824 (11.6%)]	Loss: 0.159165
Train Epoch: 2 [615424/4851824 (12.7%)]	Loss: 0.150342
Train Epoch: 2 [666624/4851824 (13.7%)]	Loss: 0.136292
Train Epoch: 2 [717824/4851824 (14.8%)]	Loss: 0.152256
Train Epoch: 2 [769024/4851824 (15.9%)]	Loss: 0.148396
Train Epoch: 2 [820224/4851824 (16.9%)]	Loss: 0.145881
Train Epoch: 2 [871424/4851824 (18.0%)]	Loss: 0.163819
Train Epoch: 2 [922624/4851824 (19.0%)]	Loss: 0.149987
Train Epoch: 2 [973824/4851824 (20.1%)]	Loss: 0.137165
Train Epoch: 2 [1025024/4851824 (21.1%)]	Loss: 0.138462
Train Epoch: 2 [1076224/4851824 (22.2%)]	Loss: 0.140335
Train Epoch: 2 [1127424/4851824 (23.2%)]	Loss: 0.149034
Train Epoch: 2 [1178624/4851824 (24.3%)]	Loss: 0.137078
Train Epoch: 2 [1229824/4851824 (25.3%)]	Loss: 0.149067
Train Epoch: 2 [1281024/4851824 (26.4%)]	Loss: 0.156381
Train Epoch: 2 [1332224/4851824 (27.5%)]	Loss: 0.124194
Train Epoch: 2 [1383424/4851824 (28.5%)]	Loss: 0.155533
Train Epoch: 2 [1434624/4851824 (29.6%)]	Loss: 0.146938
Train Epoch: 2 [1485824/4851824 (30.6%)]	Loss: 0.155528
Train Epoch: 2 [1537024/4851824 (31.7%)]	Loss: 0.143386
Train Epoch: 2 [1588224/4851824 (32.7%)]	Loss: 0.152914
Train Epoch: 2 [1639424/4851824 (33.8%)]	Loss: 0.148254
Train Epoch: 2 [1690624/4851824 (34.8%)]	Loss: 0.153778
Train Epoch: 2 [1741824/4851824 (35.9%)]	Loss: 0.134142
Train Epoch: 2 [1793024/4851824 (37.0%)]	Loss: 0.142916
Train Epoch: 2 [1844224/4851824 (38.0%)]	Loss: 0.144466
Train Epoch: 2 [1895424/4851824 (39.1%)]	Loss: 0.131345
Train Epoch: 2 [1946624/4851824 (40.1%)]	Loss: 0.142613
Train Epoch: 2 [1997824/4851824 (41.2%)]	Loss: 0.140145
Train Epoch: 2 [2049024/4851824 (42.2%)]	Loss: 0.154195
Train Epoch: 2 [2100224/4851824 (43.3%)]	Loss: 0.158047
Train Epoch: 2 [2151424/4851824 (44.3%)]	Loss: 0.146775
Train Epoch: 2 [2202624/4851824 (45.4%)]	Loss: 0.128473
Train Epoch: 2 [2253824/4851824 (46.5%)]	Loss: 0.135975
Train Epoch: 2 [2305024/4851824 (47.5%)]	Loss: 0.171948
Train Epoch: 2 [2356224/4851824 (48.6%)]	Loss: 0.151399
Train Epoch: 2 [2407424/4851824 (49.6%)]	Loss: 0.148979
Train Epoch: 2 [2458624/4851824 (50.7%)]	Loss: 0.157693
Train Epoch: 2 [2509824/4851824 (51.7%)]	Loss: 0.147686
Train Epoch: 2 [2561024/4851824 (52.8%)]	Loss: 0.156376
Train Epoch: 2 [2612224/4851824 (53.8%)]	Loss: 0.159170
Train Epoch: 2 [2663424/4851824 (54.9%)]	Loss: 0.133952
Train Epoch: 2 [2714624/4851824 (56.0%)]	Loss: 0.148908
Train Epoch: 2 [2765824/4851824 (57.0%)]	Loss: 0.143423
Train Epoch: 2 [2817024/4851824 (58.1%)]	Loss: 0.156820
Train Epoch: 2 [2868224/4851824 (59.1%)]	Loss: 0.142708
Train Epoch: 2 [2919424/4851824 (60.2%)]	Loss: 0.155680
Train Epoch: 2 [2970624/4851824 (61.2%)]	Loss: 0.143608
Train Epoch: 2 [3021824/4851824 (62.3%)]	Loss: 0.152685
Train Epoch: 2 [3073024/4851824 (63.3%)]	Loss: 0.146044
Train Epoch: 2 [3124224/4851824 (64.4%)]	Loss: 0.147751
Train Epoch: 2 [3175424/4851824 (65.4%)]	Loss: 0.128448
Train Epoch: 2 [3226624/4851824 (66.5%)]	Loss: 0.142365
Train Epoch: 2 [3277824/4851824 (67.6%)]	Loss: 0.141731
Train Epoch: 2 [3329024/4851824 (68.6%)]	Loss: 0.148541
Train Epoch: 2 [3380224/4851824 (69.7%)]	Loss: 0.143493
Train Epoch: 2 [3431424/4851824 (70.7%)]	Loss: 0.161531
Train Epoch: 2 [3482624/4851824 (71.8%)]	Loss: 0.144455
Train Epoch: 2 [3533824/4851824 (72.8%)]	Loss: 0.153148
Train Epoch: 2 [3585024/4851824 (73.9%)]	Loss: 0.145065
Train Epoch: 2 [3636224/4851824 (74.9%)]	Loss: 0.162262
Train Epoch: 2 [3687424/4851824 (76.0%)]	Loss: 0.139423
Train Epoch: 2 [3738624/4851824 (77.1%)]	Loss: 0.136069
Train Epoch: 2 [3789824/4851824 (78.1%)]	Loss: 0.137238
Train Epoch: 2 [3841024/4851824 (79.2%)]	Loss: 0.140908
Train Epoch: 2 [3892224/4851824 (80.2%)]	Loss: 0.151779
Train Epoch: 2 [3943424/4851824 (81.3%)]	Loss: 0.133211
Train Epoch: 2 [3994624/4851824 (82.3%)]	Loss: 0.147758
Train Epoch: 2 [4045824/4851824 (83.4%)]	Loss: 0.143104
Train Epoch: 2 [4097024/4851824 (84.4%)]	Loss: 0.155306
Train Epoch: 2 [4148224/4851824 (85.5%)]	Loss: 0.149976
Train Epoch: 2 [4199424/4851824 (86.6%)]	Loss: 0.158922
Train Epoch: 2 [4250624/4851824 (87.6%)]	Loss: 0.146150
Train Epoch: 2 [4301824/4851824 (88.7%)]	Loss: 0.145558
Train Epoch: 2 [4353024/4851824 (89.7%)]	Loss: 0.157410
Train Epoch: 2 [4404224/4851824 (90.8%)]	Loss: 0.148750
Train Epoch: 2 [4455424/4851824 (91.8%)]	Loss: 0.148679
Train Epoch: 2 [4506624/4851824 (92.9%)]	Loss: 0.158911
Train Epoch: 2 [4557824/4851824 (93.9%)]	Loss: 0.138237
Train Epoch: 2 [4609024/4851824 (95.0%)]	Loss: 0.154842
Train Epoch: 2 [4660224/4851824 (96.1%)]	Loss: 0.151721
Train Epoch: 2 [4711424/4851824 (97.1%)]	Loss: 0.150687
Train Epoch: 2 [4762624/4851824 (98.2%)]	Loss: 0.141694
Train Epoch: 2 [4813824/4851824 (99.2%)]	Loss: 0.148033

ACC in fold#2 was 0.849


Balanced ACC in fold#2 was 0.849


MCC in fold#2 was 0.691


Confusion Matrix in fold#2: 
           nonRipple   Ripple
nonRipple     934387   162739
Ripple        245505  1353881


Classification Report in fold#2: 
               nonRipple       Ripple  ...    macro avg  weighted avg
precision          0.792        0.893  ...        0.842         0.852
recall             0.852        0.847  ...        0.849         0.849
f1-score           0.821        0.869  ...        0.845         0.849
sample size  1097126.000  1599386.000  ...  2696512.000   2696512.000

[4 rows x 5 columns]

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
Train Epoch: 1 [1024/4449240 (0.0%)]	Loss: 0.341518
Train Epoch: 1 [52224/4449240 (1.2%)]	Loss: 0.194189
Train Epoch: 1 [103424/4449240 (2.3%)]	Loss: 0.177427
Train Epoch: 1 [154624/4449240 (3.5%)]	Loss: 0.168978
Train Epoch: 1 [205824/4449240 (4.6%)]	Loss: 0.175093
Train Epoch: 1 [257024/4449240 (5.8%)]	Loss: 0.173927
Train Epoch: 1 [308224/4449240 (6.9%)]	Loss: 0.176170
Train Epoch: 1 [359424/4449240 (8.1%)]	Loss: 0.166709
Train Epoch: 1 [410624/4449240 (9.2%)]	Loss: 0.168993
Train Epoch: 1 [461824/4449240 (10.4%)]	Loss: 0.157622
Train Epoch: 1 [513024/4449240 (11.5%)]	Loss: 0.162148
Train Epoch: 1 [564224/4449240 (12.7%)]	Loss: 0.186667
Train Epoch: 1 [615424/4449240 (13.8%)]	Loss: 0.168233
Train Epoch: 1 [666624/4449240 (15.0%)]	Loss: 0.166546
Train Epoch: 1 [717824/4449240 (16.1%)]	Loss: 0.170591
Train Epoch: 1 [769024/4449240 (17.3%)]	Loss: 0.164428
Train Epoch: 1 [820224/4449240 (18.4%)]	Loss: 0.148968
Train Epoch: 1 [871424/4449240 (19.6%)]	Loss: 0.169329
Train Epoch: 1 [922624/4449240 (20.7%)]	Loss: 0.163921
Train Epoch: 1 [973824/4449240 (21.9%)]	Loss: 0.154552
Train Epoch: 1 [1025024/4449240 (23.0%)]	Loss: 0.171367
Train Epoch: 1 [1076224/4449240 (24.2%)]	Loss: 0.155180
Train Epoch: 1 [1127424/4449240 (25.3%)]	Loss: 0.131851
Train Epoch: 1 [1178624/4449240 (26.5%)]	Loss: 0.142186
Train Epoch: 1 [1229824/4449240 (27.6%)]	Loss: 0.156886
Train Epoch: 1 [1281024/4449240 (28.8%)]	Loss: 0.147943
Train Epoch: 1 [1332224/4449240 (29.9%)]	Loss: 0.150854
Train Epoch: 1 [1383424/4449240 (31.1%)]	Loss: 0.165663
Train Epoch: 1 [1434624/4449240 (32.2%)]	Loss: 0.143215
Train Epoch: 1 [1485824/4449240 (33.4%)]	Loss: 0.171852
Train Epoch: 1 [1537024/4449240 (34.5%)]	Loss: 0.152965
Train Epoch: 1 [1588224/4449240 (35.7%)]	Loss: 0.165338
Train Epoch: 1 [1639424/4449240 (36.8%)]	Loss: 0.163224
Train Epoch: 1 [1690624/4449240 (38.0%)]	Loss: 0.143845
Train Epoch: 1 [1741824/4449240 (39.1%)]	Loss: 0.148638
Train Epoch: 1 [1793024/4449240 (40.3%)]	Loss: 0.163524
Train Epoch: 1 [1844224/4449240 (41.5%)]	Loss: 0.148180
Train Epoch: 1 [1895424/4449240 (42.6%)]	Loss: 0.167530
Train Epoch: 1 [1946624/4449240 (43.8%)]	Loss: 0.160037
Train Epoch: 1 [1997824/4449240 (44.9%)]	Loss: 0.150533
Train Epoch: 1 [2049024/4449240 (46.1%)]	Loss: 0.150236
Train Epoch: 1 [2100224/4449240 (47.2%)]	Loss: 0.155846
Train Epoch: 1 [2151424/4449240 (48.4%)]	Loss: 0.163587
Train Epoch: 1 [2202624/4449240 (49.5%)]	Loss: 0.155493
Train Epoch: 1 [2253824/4449240 (50.7%)]	Loss: 0.163378
Train Epoch: 1 [2305024/4449240 (51.8%)]	Loss: 0.153838
Train Epoch: 1 [2356224/4449240 (53.0%)]	Loss: 0.157751
Train Epoch: 1 [2407424/4449240 (54.1%)]	Loss: 0.140205
Train Epoch: 1 [2458624/4449240 (55.3%)]	Loss: 0.156895
Train Epoch: 1 [2509824/4449240 (56.4%)]	Loss: 0.152157
Train Epoch: 1 [2561024/4449240 (57.6%)]	Loss: 0.138973
Train Epoch: 1 [2612224/4449240 (58.7%)]	Loss: 0.158229
Train Epoch: 1 [2663424/4449240 (59.9%)]	Loss: 0.168414
Train Epoch: 1 [2714624/4449240 (61.0%)]	Loss: 0.142186
Train Epoch: 1 [2765824/4449240 (62.2%)]	Loss: 0.176680
Train Epoch: 1 [2817024/4449240 (63.3%)]	Loss: 0.156363
Train Epoch: 1 [2868224/4449240 (64.5%)]	Loss: 0.149666
Train Epoch: 1 [2919424/4449240 (65.6%)]	Loss: 0.143832
Train Epoch: 1 [2970624/4449240 (66.8%)]	Loss: 0.158544
Train Epoch: 1 [3021824/4449240 (67.9%)]	Loss: 0.157732
Train Epoch: 1 [3073024/4449240 (69.1%)]	Loss: 0.174341
Train Epoch: 1 [3124224/4449240 (70.2%)]	Loss: 0.138500
Train Epoch: 1 [3175424/4449240 (71.4%)]	Loss: 0.156162
Train Epoch: 1 [3226624/4449240 (72.5%)]	Loss: 0.141984
Train Epoch: 1 [3277824/4449240 (73.7%)]	Loss: 0.140930
Train Epoch: 1 [3329024/4449240 (74.8%)]	Loss: 0.143800
Train Epoch: 1 [3380224/4449240 (76.0%)]	Loss: 0.140384
Train Epoch: 1 [3431424/4449240 (77.1%)]	Loss: 0.137772
Train Epoch: 1 [3482624/4449240 (78.3%)]	Loss: 0.156005
Train Epoch: 1 [3533824/4449240 (79.4%)]	Loss: 0.153549
Train Epoch: 1 [3585024/4449240 (80.6%)]	Loss: 0.156586
Train Epoch: 1 [3636224/4449240 (81.7%)]	Loss: 0.150486
Train Epoch: 1 [3687424/4449240 (82.9%)]	Loss: 0.145580
Train Epoch: 1 [3738624/4449240 (84.0%)]	Loss: 0.139657
Train Epoch: 1 [3789824/4449240 (85.2%)]	Loss: 0.156601
Train Epoch: 1 [3841024/4449240 (86.3%)]	Loss: 0.152611
Train Epoch: 1 [3892224/4449240 (87.5%)]	Loss: 0.164767
Train Epoch: 1 [3943424/4449240 (88.6%)]	Loss: 0.145484
Train Epoch: 1 [3994624/4449240 (89.8%)]	Loss: 0.170257
Train Epoch: 1 [4045824/4449240 (90.9%)]	Loss: 0.153638
Train Epoch: 1 [4097024/4449240 (92.1%)]	Loss: 0.162416
Train Epoch: 1 [4148224/4449240 (93.2%)]	Loss: 0.155133
Train Epoch: 1 [4199424/4449240 (94.4%)]	Loss: 0.158516
Train Epoch: 1 [4250624/4449240 (95.5%)]	Loss: 0.156849
Train Epoch: 1 [4301824/4449240 (96.7%)]	Loss: 0.141613
Train Epoch: 1 [4353024/4449240 (97.8%)]	Loss: 0.148743
Train Epoch: 1 [4404224/4449240 (99.0%)]	Loss: 0.147460
